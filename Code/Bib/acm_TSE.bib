@ARTICLE{10292497,
author={Peldszus, Sven and Burger, Jens and Jurjens, Jan},
journal={ IEEE Transactions on Software Engineering },
title={{ UMLsecRT: Reactive Security Monitoring of Java Applications With Round-Trip Engineering }},
year={2024},
volume={50},
number={01},
ISSN={1939-3520},
pages={16-47},
abstract={ Today's software systems tend to be long-living and often process security-critical data, so keeping up with ever-changing security measures, attacks, and mitigations is critical to maintaining their security. While it has become common practice to consider security aspects during the design of a system, OWASP still identifies insecure design as one of the top 10 threats to security. Furthermore, even if the planned design is secure, verifying that the planned security assumptions hold at run-time and investigating any violations that may have occurred is cumbersome. In particular, the configuration of run-time monitors such as the Java Security Manager, which could enforce design-time security assumptions, is non-trivial and therefore used in practice rarely. To address these challenges, we present UMLsecRT for automatically supporting model-based security engineering with run-time monitoring of design-time security specifications and round-trip engineering for propagating run-time observations to the design level. Following the established security-by-design approach UMLsec, security experts annotate system models with security properties that UMLsecRT automatically synchronizes with corresponding source code annotations for the automatic configuration of UMLsecRT's run-time monitor. To this end, UMLecRT monitors these security properties at run-time without additional effort to specify monitoring policies. Developers can define mitigations for attacks detected at run-time in advance by adjusting the automatically synchronized annotations at implementation time. Triggered by a security violation, UMLsecRT can adapt the design-time models based on run-time findings to facilitate the investigation of security violations. We evaluated UMLsecRT concerning its effectiveness and applicability to security violations extracted from real-world attacks and the DaCapo benchmark, conducted user studies on the usability of the adapted models and the feasibility of UMLsecRT in practice, especially concerning countermeasures, and investigated the scalability of UMLsecRT. To study the applicability of the whole development process, we applied UMLsecRT in two case studies to the Eclipse Secure Storage and the electronic health record system iTrust. },
keywords={Security;Unified modeling language;Monitoring;Java;Adaptation models;Runtime;Source coding},
doi={10.1109/TSE.2023.3326366},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3326366},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jan}

@ARTICLE{10305545,
author={Gao, Zicong and Xiong, Hao and Dong, Weiyu and Chang, Rui and Yang, Rui and Zhou, Yajin and Jiang, Liehui},
journal={ IEEE Transactions on Software Engineering },
title={{ FA-Fuzz: A Novel Scheduling Scheme Using Firefly Algorithm for Mutation-Based Fuzzing }},
year={2024},
volume={50},
number={01},
ISSN={1939-3520},
pages={1-15},
abstract={ Mutation-based fuzzing has been widely used in both academia and industry. Recently, researchers observe that the mutation scheduling scheme affects the efficiency of fuzzing. Accordingly, they propose PSO algorithm or machine learning-based technique to optimize the scheduling process. However, these methods fail to consider the fact that the optimal operator distribution of different seeds is different, even for the same program. In this paper, we propose a novel general scheduling scheme, named FA-fuzz, to find the optimal selecting probability distribution of mutation operators, which is based on the observations that the effective mutation operators are different for different seeds. Specifically, our method is based on the firefly algorithm. The positions of fireflies are mapped to the selection probability distribution of different mutation operators. The brightness of fireflies is expressed as the efficiency of discovering unique testcases. We implement prototype systems on multiple state-of-art fuzzers, and perform evaluations on two datasets. Our proposed method improves both the number of unique paths and unique bugs on real-world datasets. In addition, we discover 30 zero-day vulnerabilities in eight real-world programs, which demonstrate the effectiveness of FA-fuzz. },
keywords={Fuzzing;Probability distribution;Optimization;Software algorithms;Job shop scheduling;Computer bugs;Research and development},
doi={10.1109/TSE.2023.3326144},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3326144},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jan}

@ARTICLE{10315708,
author={Babikian, Aren A. and Semerath, Oszkar and Varro, Daniel},
journal={ IEEE Transactions on Software Engineering },
title={{ Concretization of Abstract Traffic Scene Specifications Using Metaheuristic Search }},
year={2024},
volume={50},
number={01},
ISSN={1939-3520},
pages={48-68},
abstract={ Existing safety assurance approaches for autonomous vehicles (AVs) perform system-level safety evaluation by placing the AV-under-test in challenging traffic scenarios captured by abstract scenario specifications and investigated in realistic traffic simulators. As a first step towards scenario-based testing of AVs, the initial scene of a traffic scenario must be concretized. In this context, the scene concretization challenge takes as input a high-level specification of abstract traffic scenes and aims to map them to concrete scenes where exact numeric initial values are defined for each attribute of a vehicle (e.g. position or velocity). In this paper, we propose a traffic scene concretization approach that places vehicles on realistic road maps such that they satisfy an extensible set of abstract constraints defined by an expressive scene specification language which also supports static detection of inconsistencies. Then, abstract constraints are mapped to corresponding numeric constraints, which are solved by metaheuristic search with customizable objective functions and constraint aggregation strategies. We conduct a series of experiments over three realistic road maps to compare eight configurations of our approach with three variations of the state-of-the-art Scenic tool, and to evaluate its scalability. },
keywords={Safety;Metaheuristics;Behavioral sciences;Testing;Roads;Standards;Specification languages},
doi={10.1109/TSE.2023.3331254},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3331254},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jan}

@ARTICLE{10323231,
author={Shen, Yuchen and Breaux, Travis},
journal={ IEEE Transactions on Software Engineering },
title={{ Stakeholder Preference Extraction From Scenarios }},
year={2024},
volume={50},
number={01},
ISSN={1939-3520},
pages={69-84},
abstract={ Companies use personalization to tailor user experiences. Personalization appears in search engines and online stores, which include salutations and statistically learned correlations over search-, browsing- and purchase-histories. However, users have a wider variety of substantive, domain-specific preferences that affect their choices when they use directory services, and these have largely been overlooked or ignored. The contributions of this paper include: (1) a grounded theory describing how stakeholder preferences are expressed in text scenarios; (2) an app feature survey to assess whether elicited preferences represent missing requirements in existing systems; (3) an evaluation of three classifiers to label preference words in scenarios; and (4) a linker to build preference phrases by linking labeled preference words to each other based on word position. In this study, the authors analyzed 217 elicited directory service scenarios across 12 domain categories to yield a total of 7,661 stakeholder preferences labels. The app survey yielded 43 stakeholder preferences that were missed on average 49.7% by 15 directory service websites studied. The BERT-based transformer showed the best average overall 81.1% precision, 84.4% recall and 82.6% F1-score when tested on unseen domains. Finally, the preference linker correctly links preference phrases with 90.1% accuracy. Given these results, we believe directory service developers can use this approach to automatically identify user preferences to improve service designs. },
keywords={Stakeholders;Software;Surveys;Feature extraction;Transformers;Syntactics;Machine learning},
doi={10.1109/TSE.2023.3333265},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3333265},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jan}

@ARTICLE{10329992,
author={Schafer, Max and Nadi, Sarah and Eghbali, Aryaz and Tip, Frank},
journal={ IEEE Transactions on Software Engineering },
title={{ An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation }},
year={2024},
volume={50},
number={01},
ISSN={1939-3520},
pages={85-105},
abstract={ Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. Large Language Models (LLMs) have recently been applied to various aspects of software development, including their suggested use for automated generation of unit tests, but while requiring additional training or few-shot learning on examples of existing tests. This paper presents a large-scale empirical evaluation on the effectiveness of LLMs for automated unit test generation without requiring additional training or manual effort. Concretely, we consider an approach where the LLM is provided with prompts that include the signature and implementation of a function under test, along with usage examples extracted from documentation. Furthermore, if a generated test fails, our approach attempts to generate a new test that fixes the problem by re-prompting the model with the failing test and error message. We implement our approach in TestPilot, an adaptive LLM-based test generation tool for JavaScript that automatically generates unit tests for the methods in a given project's API. We evaluate TestPilot using OpenAI's gpt3.5-turbo LLM on 25 npm packages with a total of 1,684 API functions. The generated tests achieve a median statement coverage of 70.2% and branch coverage of 52.8%. In contrast, the state-of-the feedback-directed JavaScript test generation technique, Nessie, achieves only 51.3% statement coverage and 25.6% branch coverage. Furthermore, experiments with excluding parts of the information included in the prompts show that all components contribute towards the generation of effective test suites. We also find that 92.8% of TestPilot's generated tests have $\leq$ 50% similarity with existing tests (as measured by normalized edit distance), with none of them being exact copies. Finally, we run TestPilot with two additional LLMs, OpenAI's older code-cushman-002 LLM and StarCoder, an LLM for which the training process is publicly documented. Overall, we observed similar results with the former (68.2% median statement coverage), and somewhat worse results with the latter (54.0% median statement coverage), suggesting that the effectiveness of the approach is influenced by the size and training set of the LLM, but does not fundamentally depend on the specific model. },
keywords={Training;Test pattern generators;Documentation;Codes;Source coding;Software;Electronic mail},
doi={10.1109/TSE.2023.3334955},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3334955},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jan}

@ARTICLE{10341212,
author={Dolata, Mateusz and Crowston, Kevin},
journal={ IEEE Transactions on Software Engineering },
title={{ Making Sense of AI Systems Development }},
year={2024},
volume={50},
number={01},
ISSN={1939-3520},
pages={123-140},
abstract={ We identify and describe episodes of sensemaking around challenges in modern Artificial-Intelligence (AI)-based systems development that emerged in projects carried out by IBM and client companies. All projects used IBM Watson as the development platform for building tailored AI-based solutions to support workers or customers of the client companies. Yet, many of the projects turned out to be significantly more challenging than IBM and its clients had expected. The analysis reveals that project members struggled to establish reliable meanings about the technology, the project, context, and data to act upon. The project members report multiple aspects of the projects that they were not expecting to need to make sense of yet were problematic. Many issues bear upon the current-generation AI’s inherent characteristics, such as dependency on large data sets and continuous improvement as more data becomes available. Those characteristics increase the complexity of the projects and call for balanced mindfulness to avoid unexpected problems. },
keywords={Artificial intelligence;Software;Probabilistic logic;Companies;Training;Cognition;Task analysis},
doi={10.1109/TSE.2023.3338857},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3338857},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jan}

@ARTICLE{10348017,
author={Gao, Shanquan and Zhang, Liyuan and Liu, Huaxiao and Wang, Yihui},
journal={ IEEE Transactions on Software Engineering },
title={{ Which Animation API Should I Use Next? A Multimodal Real-Time Animation API Recommendation Model for Android Apps }},
year={2024},
volume={50},
number={01},
ISSN={1939-3520},
pages={106-122},
abstract={ UI animation is a widely adopted design element in the UI of Android apps. There are many animation APIs available for a variety of purposes, and developers can utilize them to realize the UI animations to avoid reinventing the wheel and thus improve the development efficiency. However, the number of animation APIs is as high as thousands and it is non-trivial for developers to systematically master their use. Facing such a problem, we construct a multi-modal real-time animation API recommendation model called U-A2A in this paper, which can provide the available animation API for developers of Android apps in real-time throughout the animation realization according to the multi-modal information, that is, the information of UI animation task and the animation API context of current program (i.e., the animation API sequence that has been used). The reason for considering the animation API context is that realizing a UI animation requires the use of multiple animation APIs and relevant animation APIs roughly follow a sequence. U-A2A consists of two important parts: feature extractor and predictor. The feature extractor, which is constructed based on 3D CNN and GRU, can gain the combined feature of UI animation task as well as animation API context. The predictor consists of a fully connected layer as well as a softmax layer, and it can predict and recommend the next available animation API according to the result from feature extractor. Furthermore, we use the development experience about animation APIs of existing app products as the basis to adjust the parameters of U-A2A, thereby completing the training work of recommendation model. The experimental result shows that when 1, 3, 5, and 10 animation APIs are considered, U-A2A can achieve 45.13%, 65.72%, 72.97% and 81.85% accuracy respectively, which is much higher than the baseline LUPE. },
keywords={Animation;Task analysis;Feature extraction;Real-time systems;Operating systems;Codes;Solid modeling},
doi={10.1109/TSE.2023.3338728},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3338728},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jan}

@ARTICLE{10350049,
author={Verwijs, Christiaan and Russo, Daniel},
journal={ IEEE Transactions on Software Engineering },
title={{ The Double-Edged Sword of Diversity: How Diversity, Conflict, and Psychological Safety Impact Software Teams }},
year={2024},
volume={50},
number={01},
ISSN={1939-3520},
pages={141-157},
abstract={ Team diversity can be seen as a double-edged sword. It brings additional cognitive resources to teams at the risk of increased conflict. Few studies have investigated how different types of diversity impact software teams. This study views diversity through the lens of the categorization-elaboration model (CEM). We investigated how diversity in gender, age, role, and cultural background impacts team effectiveness and conflict, and how these associations are moderated by psychological safety. Our sample consisted of 1,118 participants from 161 teams and was analyzed with Covariance-Based Structural Equation Modeling (CB-SEM). We found a positive effect of age diversity on team effectiveness and gender diversity on relational conflict. Psychological safety contributed directly to effective teamwork and less conflict but did not moderate the diversity-effectiveness link. While our results are consistent with the CEM theory for age and gender diversity, other types of diversity did not yield similar results. We discuss several reasons for this, including curvilinear effects, moderators such as task interdependence, or the presence of a diversity mindset. With this paper, we argue that a dichotomous nature of diversity is oversimplified. Indeed, it is a complex relationship where context plays a pivotal role. A more nuanced understanding of diversity through the lens of theories, such as the CEM, may lead to more effective teamwork. },
keywords={Task analysis;Software;Cultural differences;Psychology;Safety;Gender issues;Mathematical models},
doi={10.1109/TSE.2023.3339881},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3339881},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jan}

@ARTICLE{10387509,
author={Wang, Yang and Zhang, Peng and Sun, Maolin and Lu, Zeyu and Yang, Yibiao and Tang, Yutian and Qian, Junyan and Li, Zhi and Zhou, Yuming},
journal={ IEEE Transactions on Software Engineering },
title={{ Corrections to “Uncovering Bugs in Code Coverage Profilers via Control Flow Constraint Solving” }},
year={2024},
volume={50},
number={01},
ISSN={1939-3520},
pages={158-158},
abstract={ In [1, p. 4967], a figure citation is incorrect and “Fig. 3(c)” should be “Fig. 1(c)” in the left column, the fourth line from the bottom. It is corrected below. },
keywords={Computer bugs;Codes;Testing;Constraint handling},
doi={10.1109/TSE.2023.3339345},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3339345},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jan}


@ARTICLE{10321828,
author={Arya, Deeksha M. and Guo, Jin L. C. and Robillard, Martin P.},
journal={ IEEE Transactions on Software Engineering },
title={{ Properties and Styles of Software Technology Tutorials }},
year={2024},
volume={50},
number={02},
ISSN={1939-3520},
pages={159-172},
abstract={ A large number of tutorials for popular software development technologies are available online, and those about the same technology vary widely in their presentation. We studied the design of tutorials in the software documentation landscape for five popular programming languages: Java, C#, Python, Javascript, and Typescript. We investigated the extent to which tutorial pages, i.e. resources, differ and report statistics of variations in resource properties. We developed a framework for characterizing resources based on their distinguishing attributes, i.e. properties that vary widely for the resource, relative to other resources. Additionally, we propose that a resource can be represented by its resource style, i.e. the combination of its distinguishing attributes. We discuss three techniques for characterizing resources based on our framework, to capture notable and relevant content and presentation properties of tutorial pages. We apply these techniques on a data set of 2551 resources to validate that our framework identifies valid and interpretable styles. We contribute this framework for reasoning about the design of resources in the online software documentation landscape. },
keywords={Tutorials;Software;Documentation;Task analysis;Java;Codes;Python},
doi={10.1109/TSE.2023.3332568},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3332568},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=feb}

@ARTICLE{10338828,
author={Zhang, Mengxi and Liu, Huaxiao and Chen, Chunyang and Gao, Guangyong and Li, Han and Zhao, Jian},
journal={ IEEE Transactions on Software Engineering },
title={{ AccessFixer: Enhancing GUI Accessibility for Low Vision Users With R-GCN Model }},
year={2024},
volume={50},
number={02},
ISSN={1939-3520},
pages={173-189},
abstract={ The Graphical User Interface (GUI) plays a critical role in the interaction between users and mobile applications (apps), aiming at facilitating the operation process. However, due to the variety of functions and non-standardized design, GUIs might have many accessibility issues, like the size of components being too small or their intervals being narrow. These issues would hinder the operation of low vision users, preventing them from obtaining information accurately and conveniently. Although several technologies and methods have been proposed to address these issues, they are typically confined to issue identification, leaving the resolution in the hands of developers. Moreover, it can be challenging to ensure that the color, size, and interval of the fixed GUIs are appropriately compared to the original ones. In this work, we propose a novel approach named AccessFixer (Accessibility Issues Fixing Method), which utilizes the Relational-Graph Convolutional Neural Network (R-GCN) to simultaneously fix three kinds of accessibility issues, including small sizes, narrow intervals, and low color contrast in GUIs. With AccessFixer, the fixed GUIs would have a consistent color palette, uniform intervals, and adequate size changes achieved through coordinated adjustments to the attributes of related components. Our experiments demonstrate the effectiveness and usefulness of AccessFixer in fixing GUI accessibility issues. After fixing 30 real-world apps, our approach solves an average of 81.2% of their accessibility issues. Compared with the baseline tool that can only fix size-related issues, AccessFixer not only fixes both the interval and color contrast of components, but also ensures that no new issues arise in the fixed results. Also, we apply AccessFixer to 10 open-source apps by submitting the fixed results with pull requests (PRs) on GitHub. The results demonstrate that developers approve of our submitted fixed GUIs, with 8 PRs being merged or under fixing. A user study examines that low vision users host a positive attitude toward the GUIs fixed by our method. },
keywords={Graphical user interfaces;Color;Image color analysis;Internet;Guidelines;Electronic mail;Visualization},
doi={10.1109/TSE.2023.3337421},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3337421},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=feb}

@ARTICLE{10339688,
author={Zhu, Shunkai and Wang, Jingyi and Sun, Jun and Yang, Jie and Lin, Xingwei and Wang, Tianyi and Zhang, Liyi and Cheng, Peng},
journal={ IEEE Transactions on Software Engineering },
title={{ Better Pay Attention Whilst Fuzzing }},
year={2024},
volume={50},
number={02},
ISSN={1939-3520},
pages={190-208},
abstract={ Fuzzing is one of the prevailing methods for vulnerability detection. However, even state-of-the-art fuzzing methods become ineffective after some period of time, i.e., the coverage hardly improves as existing methods are ineffective to focus the attention of fuzzing on covering the hard-to-trigger program paths. In other words, they cannot generate inputs that can break the bottleneck due to the fundamental difficulty in capturing the complex relations between the test inputs and program coverage. In particular, existing fuzzers suffer from the following main limitations: 1) lacking an overall analysis of the program to identify the most “rewarding” seeds, and 2) lacking an effective mutation strategy which could continuously select and mutates the more relevant “bytes” of the seeds. In this work, we propose an approach called ATTuzz to address these two issues systematically. First, we propose a lightweight dynamic analysis technique that estimates the “reward” of covering each basic block and selects the most rewarding seeds accordingly. Second, we mutate the selected seeds according to a neural network model which predicts whether a certain “rewarding” block will be covered given certain mutations on certain bytes of a seed. The model is a deep learning model equipped with an attention mechanism which is learned and updated periodically whilst fuzzing. Our evaluation shows that ATTuzz significantly outperforms 5 state-of-the-art grey-box fuzzers on 6 popular real-world programs and MAGMA data sets at achieving higher edge coverage and finding new bugs. In particular, ATTuzz achieved 1.2X edge coverage and 1.8X bugs detected than AFL++ over 24-hour runs. In addition, ATTuzz also finds 4 new bugs in the latest version of some popular software including p7zip and openUSD. },
keywords={Fuzzing;Deep learning;Computer bugs;Codes;Image edge detection;Electronic mail;Recurrent neural networks},
doi={10.1109/TSE.2023.3338129},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3338129},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=feb}

@ARTICLE{10352439,
author={Long, Daniel and Drylie, Scott and Ritschel, Jonathan D. and Koschnick, Clay},
journal={ IEEE Transactions on Software Engineering },
title={{ An Assessment of Rules of Thumb for Software Phase Management, and the Relationship Between Phase Effort and Schedule Success }},
year={2024},
volume={50},
number={02},
ISSN={1939-3520},
pages={209-219},
abstract={ In the planning of a software development project, managers must estimate the amount of effort needed for distinct phases of activity. A number of rules of thumb exist in the literature to help the program manager in this task. However, very little work has been done to validate these rules of thumb. Applying least square models and Hotelling's $T^{2}$T2 test, we evaluate these rules of thumb against a large database of Department of Defense projects. We determine that variability limits the simple application of any such rule. However, there are some worthy of closer attention, and we recommend adjustments for improved application. We also determine that projects which give extra attention to early phases experience less schedule growth. These findings were robust across developmental process type, military service, and project size. },
keywords={Software;Encoding;Testing;Decision making;Schedules;Costs;Task analysis},
doi={10.1109/TSE.2023.3339383},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3339383},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=feb}

@ARTICLE{10354028,
author={Karmakar, Anjan and Robbes, Romain},
journal={ IEEE Transactions on Software Engineering },
title={{ INSPECT: Intrinsic and Systematic Probing Evaluation for Code Transformers }},
year={2024},
volume={50},
number={02},
ISSN={1939-3520},
pages={220-238},
abstract={ Pre-trained models of source code have recently been successfully applied to a wide variety of Software Engineering tasks; they have also seen some practical adoption in practice, e.g. for code completion. Yet, we still know very little about what these pre-trained models learn about source code. In this article, we use probing—simple diagnostic tasks that do not further train the models—to discover to what extent pre-trained models learn about specific aspects of source code. We use an extensible framework to define 15 probing tasks that exercise surface, syntactic, structural and semantic characteristics of source code. We probe 8 pre-trained source code models, as well as a natural language model (BERT) as our baseline. We find that models that incorporate some structural information (such as GraphCodeBERT) have a better representation of source code characteristics. Surprisingly, we find that for some probing tasks, BERT is competitive with the source code models, indicating that there are ample opportunities to improve source-code specific pre-training on the respective code characteristics. We encourage other researchers to evaluate their models with our probing task suite, so that they may peer into the hidden layers of the models and identify what intrinsic code characteristics are encoded. },
keywords={Task analysis;Source coding;Probes;Codes;Training;Natural languages;Data models},
doi={10.1109/TSE.2023.3341624},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3341624},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=feb}

@ARTICLE{10373775,
author={Cheng, Wei and Hu, Wei and Ma, Xiaoxing},
journal={ IEEE Transactions on Software Engineering },
title={{ Revisiting Knowledge-Based Inference of Python Runtime Environments: A Realistic and Adaptive Approach }},
year={2024},
volume={50},
number={02},
ISSN={1939-3520},
pages={258-279},
abstract={ The reuse and integration of existing code is a common practice for efficient software development. Constantly updated Python interpreters and third-party packages introduce many challenges to Python runtime environment inference. Existing knowledge-based approaches have achieved good performance but still suffer from several limitations in the real world, especially from incomplete domain knowledge. In this paper, we propose ReadPyE, a realistic and adaptive approach to Python runtime environment inference. To leverage the rich code information, we present an automated approach to the construction and maintenance of our designed Python ecosystem knowledge graph (KG). Moreover, we are the first to handle real-world challenges such as complex dependency specifications and incomplete domain knowledge. Specifically, we define a naming similarity measure to match candidate packages for unknown modules and set priorities for multiple candidate packages. ReadPyE solves the optimization problems of candidate package selection and generates compatible runtime environments step by step based on the current Python environment. The inferred environments are iteratively validated and adjusted by matched exception templates in the validation logs. The evaluation results on three real-world datasets show the superior effectiveness and good efficiency of our ReadPyE compared to the existing knowledge-based approaches. ReadPyE solves the environment-related exceptions for 79.75% single-file code snippets, 93% Python projects, and 63.34% program pairs for code integration. We believe ReadPyE can help programmers reduce the time spent on inferring Python runtime environments and facilitate automated software configuration management. },
keywords={Python;Codes;Runtime environment;Knowledge based systems;Task analysis;Syntactics;Feature extraction},
doi={10.1109/TSE.2023.3346474},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3346474},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=feb}

@ARTICLE{10374027,
author={Formica, Federico and Fan, Tony and Rajhans, Akshay and Pantelic, Vera and Lawford, Mark and Menghi, Claudio},
journal={ IEEE Transactions on Software Engineering },
title={{ Simulation-Based Testing of Simulink Models With Test Sequence and Test Assessment Blocks }},
year={2024},
volume={50},
number={02},
ISSN={1939-3520},
pages={239-257},
abstract={ Simulation-based software testing supports engineers in finding faults in Simulink® models. It typically relies on search algorithms that iteratively generate test inputs used to exercise models in simulation to detect design errors. While simulation-based software testing techniques are effective in many practical scenarios, they are typically not fully integrated within the Simulink environment and require additional manual effort. Many techniques require engineers to specify requirements using logical languages that are neither intuitive nor fully supported by Simulink, thereby limiting their adoption in industry. This work presents HECATE, a testing approach for Simulink models using Test Sequence and Test Assessment blocks from Simulink® Test™. Unlike existing testing techniques, HECATE uses information from Simulink models to guide the search-based exploration. Specifically, HECATE relies on information provided by the Test Sequence and Test Assessment blocks to guide the search procedure. Across a benchmark of $18$18 Simulink models from different domains and industries, our comparison of HECATE with the state-of-the-art testing tool S-Taliro indicates that HECATE is both more effective (more failure-revealing test cases) and efficient (less iterations and computational time) than S-Taliro for $\approx$≈94% and $\approx$≈83% of benchmark models respectively. Furthermore, HECATE successfully generated a failure-revealing test case for a representative case study from the automotive domain demonstrating its practical usefulness. },
keywords={Software packages;Sensors;Manuals;Biological system modeling;Industries;Benchmark testing;Pacemakers},
doi={10.1109/TSE.2023.3343753},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3343753},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=feb}

@ARTICLE{10374028,
author={Huang, Qing and Li, Zishuai and Xing, Zhenchang and Zuo, Zhengkang and Peng, Xin and Xu, Xiwei and Lu, Qinghua},
journal={ IEEE Transactions on Software Engineering },
title={{ Answering Uncertain, Under-Specified API Queries Assisted by Knowledge-Aware Human-AI Dialogue }},
year={2024},
volume={50},
number={02},
ISSN={1939-3520},
pages={280-295},
abstract={ Developers’ API needs should be more pragmatic, such as seeking suggestive, explainable, and extensible APIs rather than the so-called best result. Existing API search research cannot meet these pragmatic needs because they are solely concerned with query-API relevance. This necessitates a focus on enhancing the entire query process, from query definition to query refinement through intent clarification to query results promoting divergent thinking about results. This paper designs a novel Knowledge-Aware Human-AI Dialog agent (KAHAID) which guides the developer to clarify the uncertain, under-specified query through multi-round question answering and recommends APIs for the clarified query with relevance explanation and extended suggestions (e.g., alternative, collaborating or opposite-function APIs). We systematically evaluate KAHAID. In terms of human-AI dialogue process, it achieves a high diversity of question options (the average diversity between any two options is 74.9%) and the ability to guide developers to find APIs using fewer dialogue rounds (no more than 3 rounds on average). For API recommendation, KAHAID achieves an MRR and MAP of 0.769 and 0.794, outperforming state-of-the-art API search approaches BIKER and CLEAR by at least 47% in MRR and 226.7% in MAP. For knowledge extension, KAHAID obtains an MRR and MAP of 0.815 and 0.864, surpassing state-of-the-art query clarification approaches by at least 42% in MRR and 45.2% in MAP. As the first of its kind, KAHAID opens the door to integrating the immediate response capability of API research and the interaction, clarification, explanation, and extensibility capability of social-technical information seeking. },
keywords={Pragmatics;Behavioral sciences;Semantics;Decision trees;Knowledge graphs;Java;Extensibility},
doi={10.1109/TSE.2023.3346954},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3346954},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=feb}

@ARTICLE{10375892,
author={Coppola, Riccardo and Fulcini, Tommaso and Ardito, Luca and Torchiano, Marco and Alegroth, Emil},
journal={ IEEE Transactions on Software Engineering },
title={{ On Effectiveness and Efficiency of Gamified Exploratory GUI Testing }},
year={2024},
volume={50},
number={02},
ISSN={1939-3520},
pages={322-337},
abstract={ Context: Gamification appears to improve enjoyment and quality of execution of software engineering activities, including software testing. Though commonly employed in industry, manual exploratory testing of web application GUIs was proven to be mundane and expensive. Gamification applied to that kind of testing activity has the potential to overcome its limitations, though no empirical research has explored this area yet. Goal: Collect preliminary insights on how gamification, when performed by novice testers, affects the effectiveness, efficiency, test case realism, and user experience in exploratory testing of web applications. Method: Common gamification features augment an existing exploratory testing tool: Final Score with Leaderboard, Injected Bugs, Progress Bar, and Exploration Highlights. The original tool and the gamified version are then compared in an experiment involving 144 participants. User experience is elicited using the Technology Acceptance Model (TAM) questionnaire instrument. Results: Statistical analysis identified several significant differences for metrics that represent the effectiveness and efficiency of tests showing an improvement in coverage when they were developed with gamification. Additionally, user experience is improved with gamification. Conclusions: Gamification of exploratory testing has a tangible effect on how testers create test cases for web applications. While the results are mixed, the effects are most beneficial and interesting and warrant more research in the future. Further research shall be aimed at confirming the presented results in the context of state-of-the-art testing tools and real-world development environments. },
keywords={Software testing;Graphical user interfaces;Manuals;Software;Task analysis;Games;User experience},
doi={10.1109/TSE.2023.3348036},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3348036},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=feb}

@ARTICLE{10378848,
author={Tufano, Rosalia and Dabic, Ozren and Mastropaolo, Antonio and Ciniselli, Matteo and Bavota, Gabriele},
journal={ IEEE Transactions on Software Engineering },
title={{ Code Review Automation: Strengths and Weaknesses of the State of the Art }},
year={2024},
volume={50},
number={02},
ISSN={1939-3520},
pages={338-353},
abstract={ The automation of code review has been tackled by several researchers with the goal of reducing its cost. The adoption of deep learning in software engineering pushed the automation to new boundaries, with techniques imitating developers in generative tasks, such as commenting on a code change as a reviewer would do or addressing a reviewer's comment by modifying code. The performance of these techniques is usually assessed through quantitative metrics, e.g., the percentage of instances in the test set for which correct predictions are generated, leaving many open questions on the techniques’ capabilities. For example, knowing that an approach is able to correctly address a reviewer's comment in 10% of cases is of little value without knowing what was asked by the reviewer: What if in all successful cases the code change required to address the comment was just the removal of an empty line? In this paper we aim at characterizing the cases in which three code review automation techniques tend to succeed or fail in the two above-described tasks. The study has a strong qualitative focus, with $\sim$∼105 man-hours of manual inspection invested in manually analyzing correct and wrong predictions generated by the three techniques, for a total of 2,291 inspected predictions. The output of this analysis are two taxonomies reporting, for each of the two tasks, the types of code changes on which the experimented techniques tend to succeed or to fail, pointing to areas for future work. A result of our manual analysis was also the identification of several issues in the datasets used to train and test the experimented techniques. Finally, we assess the importance of researching in techniques specialized for code review automation by comparing their performance with ChatGPT, a general purpose large language model, finding that ChatGPT struggles in commenting code as a human reviewer would do. },
keywords={Codes;Task analysis;Automation;Software;Java;Transformers;Costs},
doi={10.1109/TSE.2023.3348172},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3348172},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=feb}

@ARTICLE{10379838,
author={Yang, Yanming and Hu, Xing and Gao, Zhipeng and Chen, Jinfu and Ni, Chao and Xia, Xin and Lo, David},
journal={ IEEE Transactions on Software Engineering },
title={{ Federated Learning for Software Engineering: A Case Study of Code Clone Detection and Defect Prediction }},
year={2024},
volume={50},
number={02},
ISSN={1939-3520},
pages={296-321},
abstract={ In various research domains, artificial intelligence (AI) has gained significant prominence, leading to the development of numerous learning-based models in research laboratories, which are evaluated using benchmark datasets. While the models proposed in previous studies may demonstrate satisfactory performance on benchmark datasets, translating academic findings into practical applications for industry practitioners presents challenges. This can entail either the direct adoption of trained academic models into industrial applications, leading to a performance decrease, or retraining models with industrial data, a task often hindered by insufficient data instances or skewed data distributions. Real-world industrial data is typically significantly more intricate than benchmark datasets, frequently exhibiting data-skewing issues, such as label distribution skews and quantity skews. Furthermore, accessing industrial data, particularly source code, can prove challenging for Software Engineering (SE) researchers due to privacy policies. This limitation hinders SE researchers’ ability to gain insights into industry developers’ concerns and subsequently enhance their proposed models. To bridge the divide between academic models and industrial applications, we introduce a federated learning (FL)-based framework called Almity. Our aim is to simplify the process of implementing research findings into practical use for both SE researchers and industry developers. Almity enhances model performance on sensitive skewed data distributions while ensuring data privacy and security. It introduces an innovative aggregation strategy that takes into account three key attributes: data scale, data balance, and minority class learnability. This strategy is employed to refine model parameters, thereby enhancing model performance on sensitive skewed datasets. In our evaluation, we employ two well-established SE tasks, i.e., code clone detection and defect prediction, as evaluation tasks. We compare the performance of Almity on both machine learning (ML) and deep learning (DL) models against two mainstream training methods, specifically the Centralized Training Method (CTM) and Vanilla Federated Learning (VFL), to validate the effectiveness and generalizability of Almity. Our experimental results demonstrate that our framework is not only feasible but also practical in real-world scenarios. Almity consistently enhances the performance of learning-based models, outperforming baseline training methods across all types of data distributions. },
keywords={Data models;Training;Codes;Cloning;Task analysis;Benchmark testing;Industries},
doi={10.1109/TSE.2023.3347898},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3347898},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=feb}

@ARTICLE{10433394,
author={},
journal={ IEEE Transactions on Software Engineering },
title={{ 2023 Reviewers List }},
year={2024},
volume={50},
number={02},
ISSN={1939-3520},
pages={354-358},
abstract={ },
keywords={},
doi={10.1109/TSE.2023.3348716},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3348716},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=feb}

@ARTICLE{10376026,
author={Wen, Xin-Cheng and Gao, Cuiyun and Ye, Jiaxin and Li, Yichen and Tian, Zhihong and Jia, Yan and Wang, Xuan},
journal={ IEEE Transactions on Software Engineering },
title={{ Meta-Path Based Attentional Graph Learning Model for Vulnerability Detection }},
year={2024},
volume={50},
number={03},
ISSN={1939-3520},
pages={360-375},
abstract={ In recent years, deep learning (DL)-based methods have been widely used in code vulnerability detection. The DL-based methods typically extract structural information from source code, e.g., code structure graph, and adopt neural networks such as Graph Neural Networks (GNNs) to learn the graph representations. However, these methods fail to consider the heterogeneous relations in the code structure graph, i.e., the heterogeneous relations mean that the different types of edges connect different types of nodes in the graph, which may obstruct the graph representation learning. Besides, these methods are limited in capturing long-range dependencies due to the deep levels in the code structure graph. In this paper, we propose a Meta-path based Attentional Graph learning model for code vulNErability deTection, called MAGNET. MAGNET constructs a multi-granularity meta-path graph for each code snippet, in which the heterogeneous relations are denoted as meta-paths to represent the structural information. A meta-path based hierarchical attentional graph neural network is also proposed to capture the relations between distant nodes in the graph. We evaluate MAGNET on three public datasets and the results show that MAGNET outperforms the best baseline method in terms of F1 score by 6.32%, 21.50%, and 25.40%, respectively. MAGNET also achieves the best performance among all the baseline methods in detecting Top-25 most dangerous Common Weakness Enumerations (CWEs), further demonstrating its effectiveness in vulnerability detection. },
keywords={Codes;Graph neural networks;Source coding;Image edge detection;Software;Syntactics;Semantics},
doi={10.1109/TSE.2023.3340267},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3340267},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=mar}

@ARTICLE{10378737,
author={Zhu, Jie and Wang, Leye and Han, Xiao and Liu, Anmin and Xie, Tao},
journal={ IEEE Transactions on Software Engineering },
title={{ Safety and Performance, Why Not Both? Bi-Objective Optimized Model Compression Against Heterogeneous Attacks Toward AI Software Deployment }},
year={2024},
volume={50},
number={03},
ISSN={1939-3520},
pages={376-390},
abstract={ The size of deep learning models in artificial intelligence (AI) software is increasing rapidly, hindering the large-scale deployment on resource-restricted devices (e.g., smartphones). To mitigate this issue, AI software compression plays a crucial role, which aims to compress model size while keeping high performance. However, the intrinsic defects in a big model may be inherited by the compressed one. Such defects may be easily leveraged by adversaries, since a compressed model is usually deployed in a large number of devices without adequate protection. In this article, we aim to address the safe model compression problem from the perspective of safety-performance co-optimization. Specifically, inspired by the test-driven development (TDD) paradigm in software engineering, we propose a test-driven sparse training framework called SafeCompress. By simulating the attack mechanism as safety testing, SafeCompress can automatically compress a big model to a small one following the dynamic sparse training paradigm. Then, considering two kinds of representative and heterogeneous attack mechanisms, i.e., black-box membership inference attack and white-box membership inference attack, we develop two concrete instances called BMIA-SafeCompress and WMIA-SafeCompress. Further, we implement another instance called MMIA-SafeCompress by extending SafeCompress to defend against the occasion when adversaries conduct black-box and white-box membership inference attacks simultaneously. We conduct extensive experiments on five datasets for both computer vision and natural language processing tasks. The results show the effectiveness and generalizability of our framework. We also discuss how to adapt SafeCompress to other attacks besides membership inference attack, demonstrating the flexibility of SafeCompress. },
keywords={Artificial intelligence;Training;Safety;Computational modeling;Task analysis;Glass box;Artificial neural networks},
doi={10.1109/TSE.2023.3348515},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3348515},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=mar}

@ARTICLE{10382258,
author={Dang, Xueqi and Li, Yinghua and Papadakis, Mike and Klein, Jacques and Bissyande, Tegawende F. and Traon, Yves Le},
journal={ IEEE Transactions on Software Engineering },
title={{ Test Input Prioritization for Machine Learning Classifiers }},
year={2024},
volume={50},
number={03},
ISSN={1939-3520},
pages={413-442},
abstract={ Machine learning has achieved remarkable success across diverse domains. Nevertheless, concerns about interpretability in black-box models, especially within Deep Neural Networks (DNNs), have become pronounced in safety-critical fields like healthcare and finance. Classical machine learning (ML) classifiers, known for their higher interpretability, are preferred in these domains. Similar to DNNs, classical ML classifiers can exhibit bugs that could lead to severe consequences in practice. Test input prioritization has emerged as a promising approach to ensure the quality of an ML system, which prioritizes potentially misclassified tests so that such tests can be identified earlier with limited manual labeling costs. However, when applying to classical ML classifiers, existing DNN test prioritization methods are constrained from three perspectives: 1) Coverage-based methods are inefficient and time-consuming; 2) Mutation-based methods cannot be adapted to classical ML models due to mismatched model mutation rules; 3) Confidence-based methods are restricted to a single dimension when applying to binary ML classifiers, solely depending on the model's prediction probability for one class. To overcome the challenges, we propose MLPrior, a test prioritization approach specifically tailored for classical ML models. MLPrior leverages the characteristics of classical ML classifiers (i.e., interpretable models and carefully engineered attribute features) to prioritize test inputs. The foundational principles are: 1) tests more sensitive to mutations are more likely to be misclassified, and 2) tests closer to the model's decision boundary are more likely to be misclassified. Building on the first concept, we design mutation rules to generate two types of mutation features (i.e., model mutation features and input mutation features) for each test. Drawing from the second notion, MLPrior generates attribute features of each test based on its attribute values, which can indirectly reveal the proximity between the test and the decision boundary. For each test, MLPrior combines all three types of features of it into a final vector. Subsequently, MLPrior employs a pre-trained ranking model to predict the misclassification probability of each test based on its final vector and ranks tests accordingly. We conducted an extensive study to evaluate MLPrior based on 185 subjects, encompassing natural datasets, mixed noisy datasets, and fairness datasets. The results demonstrate that MLPrior outperforms all the compared test prioritization approaches, with an average improvement of 14.74%$\sim$∼66.93% on natural datasets, 18.55%$\sim$∼67.73% on mixed noisy datasets, and 15.34%$\sim$∼62.72% on fairness datasets. },
keywords={Predictive models;Adaptation models;Labeling;Machine learning;Testing;Noise measurement;Manuals},
doi={10.1109/TSE.2024.3350019},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3350019},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=mar}

@ARTICLE{10384481,
author={Zheng, Wei and Lin, Lidan and Wu, Xiaoxue and Chen, Xiang},
journal={ IEEE Transactions on Software Engineering },
title={{ An Empirical Study on Correlations Between Deep Neural Network Fairness and Neuron Coverage Criteria }},
year={2024},
volume={50},
number={03},
ISSN={1939-3520},
pages={391-412},
abstract={ Recently, with the widespread use of deep neural networks (DNNs) in high-stakes decision-making systems (such as fraud detection and prison sentencing), concerns have arisen about the fairness of DNNs in terms of the potential negative impact they may have on individuals and society. Therefore, fairness testing has become an important research topic in DNN testing. At the same time, the neural network coverage criteria (such as criteria based on neuronal activation) is considered as an adequacy test for DNN white-box testing. It is implicitly assumed that improving the coverage can enhance the quality of test suites. Nevertheless, the correlation between DNN fairness (a test property) and coverage criteria (a test method) has not been adequately explored. To address this issue, we conducted a systematic empirical study on seven coverage criteria, six fairness metrics, three fairness testing techniques, and five bias mitigation methods on five DNN models and nine fairness datasets to assess the correlation between coverage criteria and DNN fairness. Our study achieved the following findings: 1) with the increase in the size of the test suite, some of the coverage and fairness metrics changed significantly, as the size of the test suite increased; 2) the statistical correlation between coverage criteria and DNN fairness is limited; and 3) after bias mitigation for improving the fairness of DNN, the change pattern in coverage criteria is different; 4) Models debiased by different bias mitigation methods have a lower correlation between coverage and fairness compared to the original models. Our findings cast doubt on the validity of coverage criteria concerning DNN fairness (i.e., increasing the coverage may even have a negative impact on the fairness of DNNs). Therefore, we warn DNN testers against blindly pursuing higher coverage of coverage criteria at the cost of test properties of DNNs (such as fairness). },
keywords={Testing;Neurons;Artificial neural networks;Correlation;Measurement;Software;Deep learning},
doi={10.1109/TSE.2023.3349001},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3349001},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=mar}

@ARTICLE{10398589,
author={Han, Junxiao and Zhang, Jiahao and Lo, David and Xia, Xin and Deng, Shuiguang and Wu, Minghui},
journal={ IEEE Transactions on Software Engineering },
title={{ Understanding Newcomers’ Onboarding Process in Deep Learning Projects }},
year={2024},
volume={50},
number={03},
ISSN={1939-3520},
pages={443-460},
abstract={ Attracting and retaining newcomers are critical for the sustainable development of Open Source Software (OSS) projects. Considerable efforts have been made to help newcomers identify and overcome barriers in the onboarding process. However, fewer studies focus on newcomers’ activities before their successful onboarding. Given the rising popularity of deep learning (DL) techniques, we wonder what the onboarding process of DL newcomers is, and if there exist commonalities or differences in the onboarding process for DL and non-DL newcomers. Therefore, we reported a study to understand the growth trends of DL and non-DL newcomers, mine DL and non-DL newcomers’ activities before their successful onboarding (i.e., past activities), and explore the relationships between newcomers’ past activities and their first commit patterns and retention rates. By analyzing 20 DL projects with 9,191 contributors and 20 non-DL projects with 9,839 contributors, and conducting email surveys with contributors, we derived the following findings: 1) DL projects have attracted and retained more newcomers than non-DL projects. 2) Compared to non-DL newcomers, DL newcomers encounter more deployment, documentation, and version issues before their successful onboarding. 3) DL newcomers statistically require more time to successfully onboard compared to non-DL newcomers, and DL newcomers with more past activities (e.g., issues, issue comments, and watch) are prone to submit an intensive first commit (i.e., a commit with many source code and documentation files being modified). Based on the findings, we shed light on the onboarding process for DL and non-DL newcomers, highlight future research directions, and provide practical suggestions to newcomers, researchers, and projects. },
keywords={Market research;Deep learning;Documentation;Software development management;Libraries;Open source software;Tutorials},
doi={10.1109/TSE.2024.3353297},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3353297},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=mar}

@ARTICLE{10401946,
author={Han, Ruidong and Ma, Siqi and Li, Juanru and Nepal, Surya and Lo, David and Ma, Zhuo and Ma, JianFeng},
journal={ IEEE Transactions on Software Engineering },
title={{ Range Specification Bug Detection in Flight Control System Through Fuzzing }},
year={2024},
volume={50},
number={03},
ISSN={1939-3520},
pages={461-473},
abstract={ Developers and manufacturers provide configurable control parameters for flight control programs to support various environments and missions, along with suggested ranges for these parameters to ensure flight safety. However, this flexible mechanism can also introduce a vulnerability known as range specification bugs. The vulnerability originates from the evidence that certain combinations of parameter values may affect the drone's physical stability even though its parameters are within the suggested range. The paper introduces a novel system called icsearcher, designed to identify incorrect configurations or unreasonable combinations of parameters and suggest more reasonable ranges for these parameters. icsearcher applies a metaheuristic search algorithm to find configurations with a high probability of driving the drone into unstable states. In particular, icsearcher adopts a machine learning-based predictor to assist the searcher in evaluating the fitness of configuration. Finally, leveraging searched incorrect configurations, icsearcher can summarize the feasible ranges through multi-objective optimization. icsearcher applies a predictor to guide the search, which eliminates the need for realistic/simulation executions when evaluating configurations and further promotes search efficiency. We have carried out experimental evaluations of icsearcher in different control programs. The evaluation results show that the system successfully reports potentially incorrect configurations, of which over $94\%$94% leads to unstable states. },
keywords={Drones;Aerospace control;Trajectory;Computer bugs;Fuzzing;Actuators;Codes},
doi={10.1109/TSE.2024.3354739},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3354739},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=mar}

@ARTICLE{10402034,
author={Wang, Yong and Cui, Wenzhong and Wang, Gai-Ge and Wang, Jian and Gong, Dunwei},
journal={ IEEE Transactions on Software Engineering },
title={{ Improving Test Data Generation for MPI Program Path Coverage With FERPSO-IMPR and Surrogate-Assisted Models }},
year={2024},
volume={50},
number={03},
ISSN={1939-3520},
pages={495-511},
abstract={ Message passing interface (MPI) is a powerful tool for parallel computing, originally designed for high-performance computing on massively parallel computers. In this paper, we combine FERPSO-IMPR (fitness Euclidean distance ratio particle swarm optimizer with information migration-based penalty and population reshaping) and surrogate-assisted models to generate test cases for MPI program path coverage testing. In our proposed method, FERPSO-IMPR employs a dual population strategy to initialize data and calculate fitness. Then, we create a sample set based on the initial data and its fitness. Subsequently, we train the master-slave surrogate models to predict individual fitness. Finally, a small number of elite individuals are selected to execute the program to decide whether to generate the required test data and guide the subsequent evolution process. We apply the proposed method to seven MPI programs and perform experimental comparisons from five directions. Experimental results show that compared with the comparative method, the time consumption of the proposed method is reduced by 33.2%, the number of evaluations is reduced by 38.8%, and the success rate is increased by 7.6%. These results prove that our method can effectively reduce the test data generation cost of MPI programs. },
keywords={Process control;Computational modeling;Software;Parallel processing;Statistics;Sociology;Data models},
doi={10.1109/TSE.2024.3354971},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3354971},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=mar}

@ARTICLE{10402095,
author={Zhang, Quanjun and Fang, Chunrong and Sun, Weisong and Liu, Yan and He, Tieke and Hao, Xiaodong and Chen, Zhenyu},
journal={ IEEE Transactions on Software Engineering },
title={{ APPT: Boosting Automated Patch Correctness Prediction via Fine-Tuning Pre-Trained Models }},
year={2024},
volume={50},
number={03},
ISSN={1939-3520},
pages={474-494},
abstract={ Automated program repair (APR) aims to fix software bugs automatically without human debugging efforts and plays a crucial role in software development and maintenance. Despite the recent significant progress in the number of fixed bugs, APR is still challenged by a long-standing overfitting problem (i.e., the generated patch is plausible but overfitting). Various techniques have thus been proposed to address the overfitting problem. Recently, researchers have employed BERT to extract code features, which are then used to train a classifier for patch correctness prediction, indicating the potential of such pre-trained models in reasoning about patch correctness. However, BERT is restricted to feature extraction for classifier training without benefiting from the training process, potentially generating sub-optimal vector representations for patched code snippets. In this paper, we propose APPT, a pre-trained model-based automated patch correctness assessment technique by both pre-training and fine-tuning. APPT adopts a pre-trained model as the encoder stack, followed by an LSTM stack and a deep learning classifier. More importantly, the pre-trained model is fine-tuned in conjunction with other components as a whole pipeline to fully adapt it specifically for reasoning about patch correctness. Although our idea is general and can be built on various existing pre-trained models, we have implemented APPT based on the BERT model. We conduct an extensive experiment on 1,183 Defects4J patches and the experimental results show that APPT achieves prediction accuracy of 79.7% and recall of 83.2%, outperforming the state-of-the-art technique CACHE by 4.3% and 6.7%. Our additional investigation on 49,694 real-world patches shows that APPT achieves the optimum performance (exceeding 99% in five common metrics for assessing patch classification techniques) compared with existing representation learning techniques. We further investigate the impact of each component and find that they all positively contribute to APPT, e.g., the fine-tuning process and the LSTM stack increase F1-score by 10.22% and 4.11%, respectively. We also prove that adopting advanced pre-trained models can further provide substantial advancement (e.g., GraphCodeBERT-based APPT improves BERT-based APPT by 2.8% and 3.3% in precision and AUC, respectively), highlighting the generalizability of APPT. Overall, our study highlights the promising future of fine-tuning pre-trained models to assess patch correctness and reduce the manual inspection effort of debugging experts when deploying APR tools in practice. },
keywords={Codes;Feature extraction;Task analysis;Predictive models;Maintenance engineering;Computer bugs;Adaptation models},
doi={10.1109/TSE.2024.3354969},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3354969},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=mar}

@ARTICLE{10413894,
author={Li, Junjie and Yang, Jinqiu},
journal={ IEEE Transactions on Software Engineering },
title={{ Tracking the Evolution of Static Code Warnings: The State-of-the-Art and a Better Approach }},
year={2024},
volume={50},
number={03},
ISSN={1939-3520},
pages={534-550},
abstract={ Static bug detection tools help developers detect problems in the code, including bad programming practices and potential defects. Recent efforts to integrate static bug detectors in modern software development workflows, such as in code review and continuous integration, are shown to better motivate developers to fix the reported warnings on the fly. A proper mechanism to track the evolution of the reported warnings can better support such integration. Moreover, tracking the static code warnings will benefit many downstream software engineering tasks, such as learning the fix patterns for automated program repair, and learning which warnings are of more interest, so they can be prioritized automatically. In addition, the utilization of tracking tools enables developers to concentrate on the most recent and actionable static warnings rather than being overwhelmed by the thousands of warnings from the entire project. This, in turn, enhances the utilization of static analysis tools. Hence, precisely tracking the warnings by static bug detectors is critical to improving the utilization of static bug detectors further. In this paper, we study the effectiveness of the state-of-the-art (SOTA) solution in tracking static code warnings and propose a better solution based on our analysis of the insufficiency of the SOTA solution. In particular, we examined over 2,000 commits in four large-scale open-source systems (i.e., JClouds, Kafka, Spring-boot, and Guava) and crafted a dataset of 3,451 static code warnings by two static bug detectors (i.e., Spotbugs and PMD). We manually uncovered the ground-truth evolution status of the static warnings: persistent, removedfix, removednon-fix and newly-introduced. Upon manual analysis, we identified the main reasons behind the insufficiency of the SOTA solution. Furthermore, we propose StaticTracker to track static warnings over software development history. Our evaluation shows that StaticTracker significantly improves the tracking precision, i.e., from 64.4% to 90.3% for the evolution statuses combined (removedfix, removednon-fix and newly-introduced). },
keywords={Codes;Computer bugs;Detectors;Software;History;Maintenance engineering;Task analysis},
doi={10.1109/TSE.2024.3358283},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3358283},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=mar}

@ARTICLE{10413895,
author={Wei, Changqing and Yao, Xiangjuan and Gong, Dunwei and Liu, Huai},
journal={ IEEE Transactions on Software Engineering },
title={{ Test Data Generation for Mutation Testing Based on Markov Chain Usage Model and Estimation of Distribution Algorithm }},
year={2024},
volume={50},
number={03},
ISSN={1939-3520},
pages={551-573},
abstract={ Mutation testing, a mainstream fault-based software testing technique, can mimic a wide variety of software faults by seeding them into the target program and resulting in the so-called mutants. Test data generated in mutation testing should be able to kill as many mutants as possible, hence guaranteeing a high fault-detection effectiveness of testing. Nevertheless, the test data generation can be very expensive, because mutation testing normally involves an extremely large number of mutants and some mutants are hard to kill. It is thus a critical yet challenging job to find an efficient way to generate a small set of test data that are able to kill multiple mutants at the same time as well as reveal those hard-to-detect faults. In this paper, we propose a new approach for test data generation in mutation testing, through the novel applications of the Markov chain usage model and the estimation of distribution algorithm. We first utilize the Markov chain usage model to reduce the so-called mutant branches in weak mutation testing and generate a minimal set of extended paths. Then, we regard the problem of generating test data as the problem of covering extended paths and use an estimation of distribution algorithm based on probability model to solve the problem. Finally, we develop a framework, TAMMEA, to implement the new approach of generating test data for mutation testing. The empirical studies based on fifteen object programs show that TAMMEA can kill more mutants using fewer test data compared with baseline techniques. In addition, the computation overhead of TAMMEA is lower than that of the baseline technique based on the traditional genetic algorithm, and comparable to that of the random method. It is clear that the new approach improves both the effectiveness and efficiency of mutation testing, thus promoting its practicability. },
keywords={Testing;Markov processes;Estimation;Software algorithms;Genetic algorithms;Data models;Costs},
doi={10.1109/TSE.2024.3358297},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3358297},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=mar}

@ARTICLE{10413900,
author={Yang, Haoran and Nong, Yu and Wang, Shaowei and Cai, Haipeng},
journal={ IEEE Transactions on Software Engineering },
title={{ Multi-Language Software Development: Issues, Challenges, and Solutions }},
year={2024},
volume={50},
number={03},
ISSN={1939-3520},
pages={512-533},
abstract={ Developing software projects that incorporate multiple languages has been a prevalent practice for many years. However, the issues encountered by developers during the development process, the underlying challenges causing these issues, and the solutions provided to developers remain unknown. In this paper, our objective is to provide answers to these questions by conducting a study on developer discussions on Stack Overflow (SO). Through a manual analysis of 586 highly relevant posts spanning 14 years, we revealed that multilingual development is a highly and sustainably active topic on SO, with older questions becoming inactive and newer ones getting first asked (and then mostly remaining active for more than one year). From these posts, we observed a diverse array of issues (11 categories), primarily centered around interfacing and data handling across different languages. Our analysis suggests that error/exception handling issues were the most difficult to resolve among those issue categories, while security related issues were most likely to receive an accepted answer. The primary challenge faced by developers was the complexity and diversity inherent in building multilingual code and ensuring interoperability. Additionally, developers often struggled due to a lack of technical expertise on the varied features of different programming languages (e.g., threading and memory management mechanisms). In addition, properly handling message passing across languages constituted a key challenge with using implicit language interfacing. Notably, Stack Overflow emerged as a crucial source of solutions to these challenges, with the majority (73%) of the posts receiving accepted answers, most within a week (36.5% within 24 hours and 25% in the following six days). Based on our analysis results, we have formulated actionable insights and recommendations that can be utilized by researchers and developers in this field. },
keywords={Computer languages;Message passing;Data handling;Memory management;Manuals;Software;Complexity theory},
doi={10.1109/TSE.2024.3358258},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3358258},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=mar}

@ARTICLE{10414288,
author={Turker, Uraz Cengiz and Hierons, Robert M. and El-Fakih, Khaled and Mousavi, Mohammad Reza and Tyukin, Ivan Y.},
journal={ IEEE Transactions on Software Engineering },
title={{ Accelerating Finite State Machine-Based Testing Using Reinforcement Learning }},
year={2024},
volume={50},
number={03},
ISSN={1939-3520},
pages={574-597},
abstract={ Testing is a crucial phase in the development of complex systems, and this has led to interest in automated test generation techniques based on state-based models. Many approaches use models that are types of finite state machine (FSM). Corresponding test generation algorithms typically require that certain test components, such as reset sequences (RSs) and preset distinguishing sequences (PDSs), have been produced for the FSM specification. Unfortunately, the generation of RSs and PDSs is computationally expensive, and this affects the scalability of such FSM-based test generation algorithms. This paper addresses this scalability problem by introducing a reinforcement learning framework: the $\mathcal{Q}$Q-Graph framework for MBT. We show how this framework can be used in the generation of RSs and PDSs and consider both (potentially partial) timed and untimed models. The proposed approach was evaluated using three types of FSMs: randomly generated FSMs, FSMs from a benchmark, and an FSM of an Engine Status Manager for a printer. In experiments, the proposed approach was much faster and used much less memory than the state-of-the-art methods in computing PDSs and RSs. },
keywords={Test pattern generators;Scalability;Graphics processing units;Automata;Software systems;Engines;Real-time systems},
doi={10.1109/TSE.2024.3358416},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3358416},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=mar}

@ARTICLE{10416264,
author={Xu, Zhengkang and Guo, Shikai and Wang, Yumiao and Chen, Rong and Li, Hui and Li, Xiaochen and Jiang, He},
journal={ IEEE Transactions on Software Engineering },
title={{ Code Comment Inconsistency Detection Based on Confidence Learning }},
year={2024},
volume={50},
number={03},
ISSN={1939-3520},
pages={598-617},
abstract={ Code comments are a crucial source of software documentation that captures various aspects of the code. Such comments play a vital role in understanding the source code and facilitating communication between developers. However, with the iterative release of software, software projects become larger and more complex, leading to a corresponding increase in issues such as mismatched, incomplete, or outdated code comments. These inconsistencies in code comments can misguide developers and result in potential bugs, and there has been a steady rise in reports of such inconsistencies over time. Despite numerous methods being proposed for detecting code comment inconsistencies, their learning effect remains limited due to a lack of consideration for issues such as characterization noise and labeling errors in datasets. To overcome these limitations, we propose a novel approach called MCCL that first removes noise from the dataset and then detects inconsistent code comments in a timely manner, thereby enhancing the model's learning ability. Our proposed model facilitates better matching between code and comments, leading to improved development of software engineering projects. MCCL comprises two components, namely method comment detection and confidence learning denoising. The method comment detection component captures the intricate relationships between code and comments by learning their syntactic and semantic structures. It correlates the code and comments through an attention mechanism to identify how changes in the code affect the comments. Furthermore, confidence learning denoising component of MCCL identifies and removes characterization noises and labeling errors to enhance the quality of the datasets. This is achieved by implementing principles such as pruning noisy data, counting with probabilistic thresholds to estimate noise, and ranking examples to train with confidence. By effectively eliminating noise from the dataset, our model is able to more accurately learn inconsistencies between comments and source code. Our experiments on 1,518 open-source projects demonstrate that MCCL can accurately detect inconsistencies, achieving an average F1-score of 82.6%. This result outperforms state-of-the-art methods by 2.4% to 28.0%. Therefore, MCCL is more effective in identifying inconsistent comments based on code changes compared to existing approaches. },
keywords={Codes;Software;Labeling;Source coding;Semantics;Syntactics;Noise reduction},
doi={10.1109/TSE.2024.3358489},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3358489},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=mar}

@ARTICLE{10416811,
author={Niu, Zifeng and Casale, Giuliano},
journal={ IEEE Transactions on Software Engineering },
title={{ Neural Density Estimation of Response Times in Layered Software Systems }},
year={2024},
volume={50},
number={03},
ISSN={1939-3520},
pages={636-650},
abstract={ Layered queueing networks (LQNs) are a class of performance models for software systems in which multiple distributed resources may be possessed simultaneously by a job. Estimating response times in a layered system is an essential but challenging analysis dimension in Quality of Service (QoS) assessment. Current analytic methods are capable of providing accurate estimates of mean response times. However, accurately approximating response time distributions used in service-level objective analysis is a demanding task. This paper proposes a novel hybrid framework that leverages phase-type (PH) distributions and neural networks to provide accurate density estimates of response times in layered queueing networks. The core step of this framework is to recursively obtain response time distributions in the submodels that are used to analyze the network by means of decomposition. We describe these response time distributions as a mixture of density functions for which we learn the parameters through a Mixture Density Network (MDN). The approach recursively propagates MDN predictions across software layers using PH distributions and performs repeated moment-matching based refitting to efficiently estimate end-to-end response time densities. Extensive numerical experiment results show that our scheme significantly improves density estimations compared to the state-of-the-art. },
keywords={Time factors;Task analysis;Servers;Queueing analysis;Analytical models;Probability distribution;Software systems},
doi={10.1109/TSE.2024.3360093},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3360093},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=mar}

@ARTICLE{10417068,
author={Xiao, Yuan-An and Yang, Chenyang and Wang, Bo and Xiong, Yingfei},
journal={ IEEE Transactions on Software Engineering },
title={{ Accelerating Patch Validation for Program Repair With Interception-Based Execution Scheduling }},
year={2024},
volume={50},
number={03},
ISSN={1939-3520},
pages={618-635},
abstract={ Long patch validation time is a limiting factor for automated program repair (APR). Though the duality between patch validation and mutation testing is recognized, so far there exists no study of systematically adapting mutation testing techniques to general-purpose patch validation. To address this gap, we investigate existing mutation testing techniques and identify five classes of acceleration techniques that are suitable for general-purpose patch validation. Among them, mutant schemata and mutant deduplication have not been adapted to general-purpose patch validation due to the arbitrary changes that third-party APR approaches may introduce. This presents two problems for adaption: 1) the difficulty of implementing the static equivalence analysis required by the state-of-the-art mutant deduplication approach; 2) the difficulty of capturing the changes of patches to the system state at runtime. To overcome these problems, we propose two novel approaches: 1) execution scheduling, which detects the equivalence between patches online, avoiding the static equivalence analysis and its imprecision; 2) interception-based instrumentation, which intercepts the changes of patches to the system state, avoiding a full interpreter and its overhead. Based on the contributions above, we implement ExpressAPR, a general-purpose patch validator for Java that integrates all recognized classes of techniques suitable for patch validation. Our large-scale evaluation with four APR approaches shows that ExpressAPR accelerates patch validation by 137.1x over plain validation or 8.8x over the state-of-the-art approach, making patch validation no longer the time bottleneck of APR. Patch validation time for a single bug can be reduced to within a few minutes on mainstream CPUs. },
keywords={Testing;Maintenance engineering;Life estimation;Computer bugs;Runtime;Instruments;Codes},
doi={10.1109/TSE.2024.3359969},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3359969},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=mar}

@ARTICLE{10418890,
author={Carvalho, Luiz and Colanzi, Thelma Elita and Assuncao, Wesley K. G. and Garcia, Alessandro and Pereira, Juliana Alves and Kalinowski, Marcos and de Mello, Rafael Maiani and de Lima, Maria Julia and Lucena, Carlos},
journal={ IEEE Transactions on Software Engineering },
title={{ On the Usefulness of Automatically Generated Microservice Architectures }},
year={2024},
volume={50},
number={03},
ISSN={1939-3520},
pages={651-667},
abstract={ The modernization of monolithic legacy systems with microservices has been a trend in recent years. As part of this modernization, identifying microservice candidates starting from legacy code is challenging, as maintainers may consider many criteria simultaneously. Multi-objective search-based approaches represent a promising state-of-the-art solution to support this decision-making process. However, the rationale to adopt each microservice candidate automatically identified by these approaches is poorly investigated in industrial cases. Furthermore, studies with these approaches have not carefully investigated how maintainers reason and make decisions when designing microservice architectures from legacy systems. To address this gap, we conducted an on-site case study with maintainers of an industrial legacy system to investigate the usefulness of automatically generated microservice architectures. We analyze design decisions pointed out by the maintainers when reasoning about microservice candidates using several criteria at the same time. Our study is the first to assess a search-based approach involving actual maintainers conceiving microservice architectures in an industrial setting. Therefore, firstly, we considered individual evaluation of microservice candidates to understand the rationale for identifying a service. Secondly, we conducted a focus group study with maintainers with the goal of investigating design decisions at an architectural level. The results show that: (i) the automated approach is able to identify useful microservices; (ii) the criteria observed by previous studies are, in fact, considered by maintainers; and (iii) the maintainer profiles, i.e., the preferred granularity for microservice, highly affect design decisions. Finally, we observed the maintainers needed little effort in adjusting the automatically identified microservices to make them adoptable. In addition to indicating a promising potential of search-based approaches to generate microservice architectures, our findings highlight the need for: (i) interactive and/or customizable approaches that enable maintainers to include their preferences during the search process, and (ii) flexible or automated selection of criteria that fits the scenario in which the modernization is taking place. },
keywords={Microservice architectures;Aging;Computer architecture;Decision making;Codes;Source coding;Interviews},
doi={10.1109/TSE.2024.3361209},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3361209},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=mar}

@ARTICLE{10473612,
author={Uchitel, Sebastian},
journal={ IEEE Transactions on Software Engineering },
title={{ Distinguished Reviewers 2023 }},
year={2024},
volume={50},
number={03},
ISSN={1939-3520},
pages={359-359},
abstract={ Lists the reviewers who contributed to this publication in 2023. },
keywords={},
doi={10.1109/TSE.2024.3373234},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3373234},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=mar}

@ARTICLE{10381511,
author={AlOmar, Eman Abdullah and Mkaouer, Mohamed Wiem and Ouni, Ali},
journal={ IEEE Transactions on Software Engineering },
title={{ Behind the Intent of Extract Method Refactoring: A Systematic Literature Review }},
year={2024},
volume={50},
number={04},
ISSN={1939-3520},
pages={668-694},
abstract={ Background: Code refactoring is widely recognized as an essential software engineering practice to improve the understandability and maintainability of the source code. The Extract Method refactoring is considered as “Swiss army knife” of refactorings, as developers often apply it to improve their code quality, e.g., decompose long code fragments, reduce code complexity, eliminate duplicated code, etc. In recent years, several studies attempted to recommend Extract Method refactorings allowing the collection, analysis, and revelation of actionable data-driven insights about refactoring practices within software projects. Aim: In this paper, we aim at reviewing the current body of knowledge on existing Extract Method refactoring research and explore their limitations and potential improvement opportunities for future research efforts. That is, Extract Method is considered one of the most widely-used refactorings, but difficult to apply in practice as it involves low-level code changes such as statements, variables, parameters, return types, etc. Hence, researchers and practitioners begin to be aware of the state-of-the-art and identify new research opportunities in this context. Method: We review the body of knowledge related to Extract Method refactoring in the form of a systematic literature review (SLR). After compiling an initial pool of 1,367 papers, we conducted a systematic selection and our final pool included 83 primary studies. We define three sets of research questions and systematically develop and refine a classification schema based on several criteria including their methodology, applicability, and degree of automation. Results: The results construct a catalog of 83 Extract Method approaches indicating that several techniques have been proposed in the literature. Our results show that: (i) 38.6% of Extract Method refactoring studies primarily focus on addressing code clones; (ii) Several of the Extract Method tools incorporate the developer's involvement in the decision-making process when applying the method extraction, and (iii) the existing benchmarks are heterogeneous and do not contain the same type of information, making standardizing them for the purpose of benchmarking difficult. Conclusions: Our study serves as an “index” to the body of knowledge in this area for researchers and practitioners in determining the Extract Method refactoring approach that is most appropriate for their needs. Our findings also empower the community with information to guide the future development of refactoring tools. },
keywords={Codes;Data mining;Systematics;Bibliographies;Software engineering;Behavioral sciences;Software},
doi={10.1109/TSE.2023.3345800},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3345800},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=apr}

@ARTICLE{10418899,
author={Veizaga, Alvaro and Shin, Seung Yeob and Briand, Lionel C.},
journal={ IEEE Transactions on Software Engineering },
title={{ Automated Smell Detection and Recommendation in Natural Language Requirements }},
year={2024},
volume={50},
number={04},
ISSN={1939-3520},
pages={695-720},
abstract={ Requirement specifications are typically written in natural language (NL) due to its usability across multiple domains and understandability by all stakeholders. However, unstructured NL is prone to quality problems (e.g., ambiguity) when writing requirements, which can result in project failures. To address this issue, we present a tool, named Paska, that takes as input any NL requirements, automatically detects quality problems as smells in the requirements, and offers recommendations to improve their quality. Our approach relies on natural language processing (NLP) techniques and a state-of-the-art controlled natural language (CNL) for requirements (Rimay), to detect smells and suggest recommendations using patterns defined in Rimay to improve requirement quality. We evaluated Paska through an industrial case study in the financial domain involving 13 systems and 2725 annotated requirements. The results show that our tool is accurate in detecting smells (89% precision and recall) and suggesting appropriate Rimay pattern recommendations (96% precision and 94% recall). },
keywords={Syntactics;Grammar;Software;Information systems;Stakeholders;Ear;Usability},
doi={10.1109/TSE.2024.3361033},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3361033},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=apr}

@ARTICLE{10423390,
author={Prestat, Dimitri and Moha, Naouel and Villemaire, Roger and Avellaneda, Florent},
journal={ IEEE Transactions on Software Engineering },
title={{ DynAMICS: A Tool-Based Method for the Specification and Dynamic Detection of Android Behavioral Code Smells }},
year={2024},
volume={50},
number={04},
ISSN={1939-3520},
pages={765-784},
abstract={ Code smells are the result of poor design choices within software systems that complexify source code and impede evolution and performance. Therefore, detecting code smells within software systems is an important priority to decrease technical debt. Furthermore, the emergence of mobile applications (apps) has brought new types of Android-specific code smells, which relate to limitations and constraints on resources like memory, performance and energy consumption. Among these Android-specific smells are those that describe inappropriate behaviour during the execution that may negatively impact software quality. Static analysis tools, however, show limitations for detecting these behavioural code smells and properly detecting behavioural code smells requires considering the dynamic behaviour of the apps. To dynamically detect behavioural code smells, we hence propose three contributions: (1) A method, the Dynamics method, a step-by-step method for the specification and dynamic detection of Android behavioural code smells; (2) A tool, the Dynamics tool, implementing this method on seven code smells; and (3) A validation of our approach on 538 apps from F-Droid with a comparison with the static analysis detection tools, aDoctor and Paprika, from the literature. Our method consists of four steps: (1) the specification of the code smells, (2) the instrumentation of the app, (3) the execution of the apps, and (4) the detection of the behavioural code smells. Our results show that many instances of code smells that cannot be detected with static detection tools are indeed detected with our dynamic approach with an average precision of $92.8\%$92.8% and an average recall of $53.4\%$53.4%. },
keywords={Codes;Mobile applications;Source coding;Memory management;Static analysis;Software systems;Security},
doi={10.1109/TSE.2024.3363223},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3363223},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=apr}

@ARTICLE{10431665,
author={Yang, Zhou and Xu, Bowen and Zhang, Jie M. and Kang, Hong Jin and Shi, Jieke and He, Junda and Lo, David},
journal={ IEEE Transactions on Software Engineering },
title={{ Stealthy Backdoor Attack for Code Models }},
year={2024},
volume={50},
number={04},
ISSN={1939-3520},
pages={721-741},
abstract={ Code models, such as CodeBERT and CodeT5, offer general-purpose representations of code and play a vital role in supporting downstream automated software engineering tasks. Most recently, code models were revealed to be vulnerable to backdoor attacks. A code model that is backdoor-attacked can behave normally on clean examples but will produce pre-defined malicious outputs on examples injected with triggers that activate the backdoors. Existing backdoor attacks on code models use unstealthy and easy-to-detect triggers. This paper aims to investigate the vulnerability of code models with stealthy backdoor attacks. To this end, we propose Afraidoor (Adversarial Feature as Adaptive Backdoor). Afraidoor achieves stealthiness by leveraging adversarial perturbations to inject adaptive triggers into different inputs. We apply Afraidoor to three widely adopted code models (CodeBERT, PLBART, and CodeT5) and two downstream tasks (code summarization and method name prediction). We evaluate three widely used defense methods and find that Afraidoor is more unlikely to be detected by the defense methods than by baseline methods. More specifically, when using spectral signature as defense, around 85% of adaptive triggers in Afraidoor bypass the detection in the defense process. By contrast, only less than 12% of the triggers from previous work bypass the defense. When the defense method is not applied, both Afraidoor and baselines have almost perfect attack success rates. However, once a defense is applied, the attack success rates of baselines decrease dramatically, while the success rate of Afraidoor remains high. Our finding exposes security weaknesses in code models under stealthy backdoor attacks and shows that state-of-the-art defense methods cannot provide sufficient protection. We call for more research efforts in understanding security threats to code models and developing more effective countermeasures. },
keywords={Codes;Adaptation models;Data models;Task analysis;Security;Predictive models;Grammar},
doi={10.1109/TSE.2024.3361661},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3361661},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=apr}

@ARTICLE{10433002,
author={Zhang, Yuxia and Qiu, Zhiqing and Stol, Klaas-Jan and Zhu, Wenhui and Zhu, Jiaxin and Tian, Yingchen and Liu, Hui},
journal={ IEEE Transactions on Software Engineering },
title={{ Automatic Commit Message Generation: A Critical Review and Directions for Future Work }},
year={2024},
volume={50},
number={04},
ISSN={1939-3520},
pages={816-835},
abstract={ Commit messages are critical for code comprehension and software maintenance. Writing a high-quality message requires skill and effort. To support developers and reduce their effort on this task, several approaches have been proposed to automatically generate commit messages. Despite the promising performance reported, we have identified three significant and prevalent threats in these automated approaches: 1) the datasets used to train and evaluate these approaches contain a considerable amount of ‘noise’; 2) current approaches only consider commits of a limited diff size; and 3) current approaches can only generate the subject of a commit message, not the message body. The first limitation may let the models ‘learn’ inappropriate messages in the training stage, and also lead to inflated performance results in their evaluation. The other two threats can considerably weaken the practical usability of these approaches. Further, with the rapid emergence of large language models (LLMs) that show superior performance in many software engineering tasks, it is worth asking: can LLMs address the challenge of long diffs and whole message generation? This article first reports the results of an empirical study to assess the impact of these three threats on the performance of the state-of-the-art auto generators of commit messages. We collected commit data of the Top 1,000 most-starred Java projects in GitHub and systematically removed noisy commits with bot-submitted and meaningless messages. We then compared the performance of four approaches representative of the state-of-the-art before and after the removal of noisy messages, or with different lengths of commit diffs. We also conducted a qualitative survey with developers to investigate their perspectives on simply generating message subjects. Finally, we evaluate the performance of two representative LLMs, namely UniXcoder and ChatGPT, in generating more practical commit messages. The results demonstrate that generating commit messages is of great practical value, considerable work is needed to mature the current state-of-the-art, and LLMs can be an avenue worth trying to address the current limitations. Our analyses provide insights for future work to achieve better performance in practice. },
keywords={Codes;Chatbots;Task analysis;Noise measurement;Machine translation;Information retrieval;Software maintenance},
doi={10.1109/TSE.2024.3364675},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3364675},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=apr}

@ARTICLE{10433067,
author={Minani, Jean Baptiste and Sabir, Fatima and Moha, Naouel and Gueheneuc, Yann-Gael},
journal={ IEEE Transactions on Software Engineering },
title={{ A Systematic Review of IoT Systems Testing: Objectives, Approaches, Tools, and Challenges }},
year={2024},
volume={50},
number={04},
ISSN={1939-3520},
pages={785-815},
abstract={ Internet of Things (IoT) systems are becoming prevalent in various domains, from healthcare to smart homes. Testing IoT systems is critical in ensuring their reliability. Previous papers studied separately the objectives, approaches, tools, and challenges of IoT systems testing. However, despite the rapid evolution of the IoT domain, no review has been undertaken to investigate all four aspects collectively. This paper presents a systematic literature review that aggregates, synthesizes, and discusses the results of 83 primary studies (PSs) concerning IoT testing objectives, approaches, tools, and challenges. We followed the Preferred Reporting Items for Systematic Review and Meta-Analysis (PRISMA) protocol to report our findings and answer research questions (RQs). To select PSs, we applied inclusion and exclusion criteria to relevant studies published between 2012 and 2022. We extracted and analyzed the data from PSs to understand IoT systems testing. The results reveal that IoT systems testing embraces traditional software quality attributes but also introduces new ones like connectivity, energy efficiency, device lifespan, distributivity, and dynamicity. They also show that existing IoT systems testing approaches are limited to specific aspects and should be expanded for more comprehensive testing. They also show 19 testing tools and 15 testbeds for testing IoT systems with their limitations, necessitating the development or enhancement for wider coverage. The large number of heterogeneous devices generating data in different formats, along with the need for testing in real-world scenarios, poses a challenge. Thus, our study offers insights into the testing objectives, approaches, tools, and challenges associated with IoT systems. Based on the results, we also provide practical guidance for IoT practitioners by cataloging existing tools and approaches, while also identifying new research opportunities for interested researchers. },
keywords={Testing;Internet of Things;System testing;Systematics;Software systems;Computer science;Software engineering},
doi={10.1109/TSE.2024.3363611},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3363611},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=apr}

@ARTICLE{10433080,
author={Barzolevskaia, Anna and Branca, Enrico and Stakhanova, Natalia},
journal={ IEEE Transactions on Software Engineering },
title={{ Measuring and Characterizing (Mis)compliance of the Android Permission System }},
year={2024},
volume={50},
number={04},
ISSN={1939-3520},
pages={742-764},
abstract={ Within the Android mobile operating system, Android permissions act as a system of safeguards designed to restrict access to potentially sensitive data and privileged components. Multiple research studies indicate flaws and limitations of the Android permission system, prompting Google to implement a more regulated and fine-grained permission model. This newly-introduced complexity creates confusion for developers leading to incorrect permissions and a significant risk to users security and privacy. We present a systematic study of theoretical and practical misuse of permissions. For this analysis we derive the unified permissions and call mappings that represent theoretical requirements of permissions and calls. We develop PChecker, an approach that identifies the discrepancies between the official Android permissions documentation and permission implementation in the Android platform source code based on these mappings. We evaluate four versions of the Android Open Source Project code (major versions 10–13) and shed light on the prevalence of discrepancies between the official Android guidelines for permissions and their implementation in the Android platform source code. We further show that these discrepancies result in miscompliance in third-party Android apps. },
keywords={Operating systems;Smart phones;Documentation;Source coding;Codes;Guidelines;Runtime},
doi={10.1109/TSE.2024.3362921},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3362921},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=apr}

@ARTICLE{10438900,
author={Clerissi, Diego and Denaro, Giovanni and Mobilio, Marco and Mariani, Leonardo},
journal={ IEEE Transactions on Software Engineering },
title={{ Guess the State: Exploiting Determinism to Improve GUI Exploration Efficiency }},
year={2024},
volume={50},
number={04},
ISSN={1939-3520},
pages={836-853},
abstract={ Many automatic Web testing techniques generate test cases by analyzing the GUI of the Web applications under test, aiming to exercise sequences of actions that are similar to the ones that testers could manually execute. However, the efficiency of the test generation process is severely limited by the cost of analyzing the content of the GUI screens after executing each action. In this paper, we introduce an inference component, Sibilla, which accumulates knowledge about the behavior of the GUI after each action. Sibilla enables the test generators to reuse the results computed for GUI screens that recur multiple times during the test generation process, thus improving the efficiency of Web testing techniques. We experimented Sibilla with Web testing techniques based on three different GUI exploration strategies (Random, Depth-first, and Q-learning) and nine target systems, observing reductions from 22% to 96% of the test generation time. },
keywords={Graphical user interfaces;Testing;Test pattern generators;Generators;Behavioral sciences;Knowledge based systems;Q-learning},
doi={10.1109/TSE.2024.3366586},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3366586},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=apr}

@ARTICLE{10438901,
author={Zhou, Zhichao and Zhou, Yuming and Fang, Chunrong and Chen, Zhenyu and Luo, Xiapu and He, Jingzhu and Tang, Yutian},
journal={ IEEE Transactions on Software Engineering },
title={{ Coverage Goal Selector for Combining Multiple Criteria in Search-Based Unit Test Generation }},
year={2024},
volume={50},
number={04},
ISSN={1939-3520},
pages={854-883},
abstract={ Unit testing is critical to the software development process, ensuring the correctness of basic programming units in a program (e.g., a method). Search-based software testing (SBST) is an automated approach to generating test cases. SBST generates test cases with genetic algorithms by specifying the coverage criterion (e.g., branch coverage). However, a good test suite must have different properties, which cannot be captured using an individual coverage criterion. Therefore, the state-of-the-art approach combines multiple criteria to generate test cases. Since combining multiple coverage criteria brings multiple objectives for optimization, it hurts the test suites’ coverage for certain criteria compared with using the single criterion. To cope with this problem, we propose a novel approach named smart selection. Based on the coverage correlations among criteria and the subsumption relationships among coverage goals, smart selection selects a subset of coverage goals to reduce the number of optimization objectives and avoid missing any properties of all criteria. We conduct experiments to evaluate smart selection on $400$400 Java classes with three state-of-the-art genetic algorithms under the $2$2-minute budget. On average, smart selection outperforms combining all goals on $65.1\%$65.1% of the classes having significant differences between the two approaches. Secondly, we conduct experiments to verify our assumptions about coverage criteria relationships. Furthermore, we assess the coverage performance of smart selection under varying budgets of $5$5, $8$8, and $10$10 minutes and explore its effect on bug detection, confirming the advantage of smart selection over combining all goals. },
keywords={Genetic algorithms;Test pattern generators;Correlation;Optimization;Java;Software testing;Diversity reception},
doi={10.1109/TSE.2024.3366613},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3366613},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=apr}

@ARTICLE{10439257,
author={Doncevic, Juraj and Fertalj, Kresimir and Brcic, Mario and Kovac, Mihael},
journal={ IEEE Transactions on Software Engineering },
title={{ Mask–Mediator–Wrapper Architecture as a Data Mesh Driver }},
year={2024},
volume={50},
number={04},
ISSN={1939-3520},
pages={900-910},
abstract={ The data mesh is a novel data management concept that emphasizes the importance of a domain before technology. The concept is still in the early stages of development and many efforts to implement and use it are expected to have negative consequences for organizations due to a lack of technological guidelines and best practices. To mitigate the risk of negative outcomes this paper proposes the use of the mask–mediator–wrapper architecture as a driver for a data mesh implementation. The mask–mediator–wrapper architecture provides a set of prefabricated configurable components that provide basic functionalities that a data mesh requires. This paper shows how the two concepts are compatible in terms of functionality, data modeling, evolvability, and aligned capabilities. A mask–mediator–wrapper-driven data mesh facilitates low-risk adoption trials, rapid prototyping, standardization, and a guarantee of evolvability. We demonstrate a mask–mediator–wrapper-driven data mesh by using our open-source Janus system to experimentally drive an exemplified data mesh. },
keywords={Computer architecture;Soft sensors;Organizations;Standards organizations;Metadata;Mediation;Big Data applications},
doi={10.1109/TSE.2024.3367126},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3367126},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=apr}

@ARTICLE{10440574,
author={Wang, Junjie and Huang, Yuchao and Chen, Chunyang and Liu, Zhe and Wang, Song and Wang, Qing},
journal={ IEEE Transactions on Software Engineering },
title={{ Software Testing With Large Language Models: Survey, Landscape, and Vision }},
year={2024},
volume={50},
number={04},
ISSN={1939-3520},
pages={911-936},
abstract={ Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs. This paper provides a comprehensive review of the utilization of LLMs in software testing. It analyzes 102 relevant studies that have used LLMs for software testing, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative. It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs. It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in software testing. },
keywords={Software testing;Task analysis;Computational modeling;Codes;Software systems;Natural language processing;Reviews},
doi={10.1109/TSE.2024.3368208},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3368208},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=apr}

@ARTICLE{10443714,
author={Jiang, He and Wang, Zun and Zhou, Zhide and Li, Xiaochen and Guo, Shikai and Sun, Weifeng and Zhang, Tao},
journal={ IEEE Transactions on Software Engineering },
title={{ A Testing Program and Pragma Combination Selection Based Framework for High-Level Synthesis Tool Pragma-Related Bug Detection }},
year={2024},
volume={50},
number={04},
ISSN={1939-3520},
pages={937-955},
abstract={ High-Level Synthesis (HLS) tools convert C/C++ design code into Hardware Description Language (HDL) code automatically, which are often used for Field Programmable Gate Array (FPGA) design. HLS tools provide many pragmas, which are a kind of directive to be inserted into C/C++ code, for designers to efficiently control the synthesis of code components (e.g., arrays and loops) to generate FPGA implementations with varying performances and costs. However, the use of some pragmas may trigger HLS tool bugs (e.g., tool crashes). Although many formal methods have been proposed to verify the correctness of various HLS phases, no relevant work addresses the problem on detecting HLS tool pragma-related bugs. To resolve this problem, two challenges need to be addressed, namely the selection of testing programs and the acquisition of pragma combinations, due to the enormous number of testing programs and pragma combinations. In this paper, we propose TEPACS, a TEsting Program and prAgma Combination Selection-based framework, to construct diverse testing programs with pragmas for effectively detecting HLS tool pragma-related bugs. TEPACS follows the idea of fuzzing, which is a widely used technique in software testing. First, TEPACS selects the representative testing program according to the cosine distance between the code component vectors of testing programs. Then, for a selected program, TEPACS generates its golden output and uses the pragma combination selection method based on combinatorial testing to generate a set of programs with different pragmas. TEPACS uses the HLS tool under test to convert these testing programs into HDL codes and obtains the simulation results of the HDL code. Finally, based on differential testing, TEPACS identifies HLS tool bugs triggered if the simulation result and golden output are inconsistent. We evaluate TEPACS and its five variants on Vitis HLS, a widely used FPGA HLS tool. Experimental results show that TEPACS outperforms the baselines by at least 11.17% in terms of the bug-finding capability. In one month, TEPACS detected 34 bugs on the latest version of Vitis HLS, of which 9 bugs have been confirmed. },
keywords={Testing;Computer bugs;Codes;Field programmable gate arrays;Hardware design languages;Optimization;Pipeline processing},
doi={10.1109/TSE.2024.3368553},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3368553},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=apr}

@ARTICLE{10444097,
author={Hajari, Fahimeh and Malmir, Samaneh and Mirsaeedi, Ehsan and Rigby, Peter C.},
journal={ IEEE Transactions on Software Engineering },
title={{ Factoring Expertise, Workload, and Turnover Into Code Review Recommendation }},
year={2024},
volume={50},
number={04},
ISSN={1939-3520},
pages={884-899},
abstract={ Developer turnover is inevitable on software projects and leads to knowledge loss, a reduction in productivity, and an increase in defects. Mitigation strategies to deal with turnover tend to disrupt and increase workloads for developers. In this work, we suggest that through code review recommendation we can distribute knowledge and mitigate turnover while more evenly distributing review workload. We conduct historical analyses to understand the natural concentration of review workload and the degree of knowledge spreading that is inherent in code review. Even though review workload is highly concentrated, we show that code review natural spreads knowledge thereby reducing the files at risk to turnover. Using simulation, we evaluate existing code review recommenders and develop novel recommenders to understand their impact on the level of expertise during review, the workload of reviewers, and the files at risk to turnover. Our simulations use seeded random replacement of reviewers to allow us to compare the reviewer recommenders without the confounding variation of different reviewers being replaced for each recommender. We find that prior work that assigns reviewers based on file ownership concentrates knowledge on a small group of core developers increasing the risk of knowledge loss from turnover. Recent work, WhoDo, that considers developer workload, assigns developers that are not sufficiently committed to the project and we see an increase in files at risk to turnover. We propose learning and retention aware review recommenders that when combined are effective at reducing the risk of turnover, but they unacceptably reduce the overall expertise during reviews. Combining recommenders, we develop the SofiaWL recommender that suggests experts with low active review workload when none of the files under review are known by only one developer. In contrast, when knowledge is concentrated on one developer, it sends the review to other reviewers to spread knowledge. For the projects we study, we are able to globally increase expertise during reviews, $+3$+3%, reduce workload concentration, $-12$−12%, and reduce the files at risk, $-28$−28%. We make our scripts and data available in our replication package [1]. Developers can optimize for a particular outcome measure based on the needs of their project, or use our GitHub bot to automatically balance the outcomes [2]. },
keywords={Reviews;Codes;Software engineering;Software;Task analysis;Productivity;Birds},
doi={10.1109/TSE.2024.3366753},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3366753},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=apr}

@ARTICLE{10444932,
author={Moran, Jesus and Bertolino, Antonia and de la Riva, Claudio and Tuya, Javier},
journal={ IEEE Transactions on Software Engineering },
title={{ Automatic Debugging of Design Faults in MapReduce Applications }},
year={2024},
volume={50},
number={04},
ISSN={1939-3520},
pages={956-978},
abstract={ Among the current technologies to analyse large data, the MapReduce processing model stands out in Big Data. MapReduce is implemented in frameworks such as Hadoop, Spark or Flink that are able to manage the program executions according to the resources available at runtime. The developer should design the program in order to support all possible non-deterministic executions. However, the program may fail due to a design fault. Debugging these kinds of faults is difficult because the data are executed non-deterministically in parallel and the fault is not caused directly by the code, but by its design. This paper presents a framework called MRDebug which includes two debugging techniques focused on the MapReduce design faults. A spectrum-based fault localization technique locates the root cause of these faults analysing several executions of the test case, and a Delta Debugging technique isolates the data relevant to trigger the failure. An empirical evaluation with 13 programs shows that MRDebug is effective in debugging the faults, especially when the localization is done with the reduced data. In summary, MRDebug automatically provides valuable information to understand MapReduce design faults as it helps locate their root cause and obtains a minimal data that triggers the failure. },
keywords={Debugging;Location awareness;Big Data;Testing;Data models;Codes;Sparks},
doi={10.1109/TSE.2024.3369766},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3369766},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=apr}

@ARTICLE{10460136,
author={Hwang, Sungjae and Lee, Sungho and Ryu, Sukyoung},
journal={ IEEE Transactions on Software Engineering },
title={{ An Empirical Study of JVMs’ Behaviors on Erroneous JNI Interoperations }},
year={2024},
volume={50},
number={04},
ISSN={1939-3520},
pages={979-994},
abstract={ Java Native Interface (JNI) allows Java applications to access native libraries, but it is challenging to develop correct JNI programs. By leveraging native code, the JNI enables Java developers to implement efficient applications and reuse code written in other programming languages such as C and C++. The core Java libraries use the JNI to provide system features like graphical user interfaces, and mainstream Java Virtual Machines (JVMs) support the JNI. However, implementing correct JNI programs is not trivial due to the complex interoperation semantics between different programming languages. While JVMs do not validate JNI interoperations by default because of the performance overhead, they provide two methods. First, JVMs report the interoperation failures defined in the JNI specification at runtime. Second, they support a debug option, which validates JNI interoperations, degrading the runtime performance. To the best of our knowledge, literature has not thoroughly studied the quality of JVMs’ methods, even though erroneous JNI interoperations may result in incorrect behaviors. In this paper, we empirically study the behaviors of JVMs on erroneous JNI interoperations. For a systematic study, we propose JUSTGen, a semi-automatic tool that generates JNI test programs incurring erroneous interoperations from the JNI specification. JUSTGen receives the JNI specification written in our domain-specific language (DSL) and automatically discovers cases that may lead to runtime errors on interoperations using an SMT solver. It then generates test programs that trigger the behaviors on the erroneous cases. Using the generated tests, we empirically evaluate JVM's failure handling mechanisms and the debug option capabilities on erroneous JNI interoperations. Our experiment results show that there exist erroneous cases in which JVMs do not handle failures or handle them differently from the specification. We also found that the JNI debug option does not validate thousands of erroneous cases, which can cause critical runtime errors such as memory corruption and violation of the Java type system. We reported 18 erroneous cases of which JVMs do not handle failures correctly to their respective vendors. Among them, 16 cases have been resolved. },
keywords={Java;Codes;Behavioral sciences;Semantics;Runtime;Virtual machining;DSL},
doi={10.1109/TSE.2024.3373239},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3373239},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=apr}

@ARTICLE{10466433,
author={Bombarda, Andrea and Bonfanti, Silvia and De Sanctis, Martina and Gargantini, Angelo and Pelliccione, Patrizio and Riccobene, Elvinia and Scandurra, Patrizia},
journal={ IEEE Transactions on Software Engineering },
title={{ Evaluation Framework for Autonomous Systems: The Case of Programmable Electronic Medical Systems }},
year={2024},
volume={50},
number={04},
ISSN={1939-3520},
pages={995-1014},
abstract={ This paper proposes an evaluation framework for autonomous systems, called LENS. It is an instrument to make an assessment of a system through the lens of abilities related to adaptation and smartness. The assessment can then help engineers understand in which direction it is worth investing to make their system smarter. It also helps to identify possible improvement directions and to plan for concrete activities. Finally, it helps to make a re-assessment when the improvement has been performed in order to check whether the activity plan has been accomplished. Given the high variability in the various domains in which autonomous systems are and can be used, LENS is defined in abstract terms and instantiated to a specific and important class of medical devices, i.e., Programmable Electronic Medical Systems (PEMS). The instantiation, called LENS${}_{\textit{PEMS}}$PEMS, is validated in terms of applicability, i.e., how it is applicable to real PEMS, generalizability, i.e., to what extent LENS${}_{\textit{PEMS}}$PEMS is generalizable to the PEMS class of systems, and usefulness, i.e., how it is useful in making an assessment and identifying possible directions of improvement towards smartness. },
keywords={Lenses;Robots;Autonomous systems;Instruments;Robot sensing systems;Ventilators;Adaptive systems},
doi={10.1109/TSE.2024.3374382},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3374382},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=apr}


@ARTICLE{10440483,
author={Warnett, Stephen John and Zdun, Uwe},
journal={ IEEE Transactions on Software Engineering },
title={{ On the Understandability of MLOps System Architectures }},
year={2024},
volume={50},
number={05},
ISSN={1939-3520},
pages={1015-1039},
abstract={ Machine Learning Operations (MLOps) is the practice of streamlining and optimising the machine learning (ML) workflow, from development to deployment, using DevOps (software development and IT operations) principles and ML-specific activities. Architectural descriptions of MLOps systems often consist of informal textual descriptions and informal graphical system diagrams that vary considerably in consistency, quality, detail, and content. Such descriptions only sometimes follow standards or schemata and may be hard to understand. We aimed to investigate informal textual descriptions and informal graphical MLOps system architecture representations and compare them with semi-formal MLOps system diagrams for those systems. We report on a controlled experiment with sixty-three participants investigating the understandability of MLOps system architecture descriptions based on informal and semi-formal representations. The results indicate that the understandability (quantified by task correctness) of MLOps system descriptions is significantly greater using supplementary semi-formal MLOps system diagrams, that using semi-formal MLOps system diagrams does not significantly increase task duration (and thus hinder understanding), and that task correctness is only significantly correlated with task duration when semi-formal MLOps system diagrams are provided. },
keywords={Pipelines;Unified modeling language;Task analysis;Software architecture;Computer architecture;Systems architecture;Machine learning},
doi={10.1109/TSE.2024.3367488},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3367488},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=may}

@ARTICLE{10461558,
author={Prudjinski, Micha and Hadar, Irit and Luria, Gil},
journal={ IEEE Transactions on Software Engineering },
title={{ Exploring the Role of Team Security Climate in the Implementation of Security by Design: A Case Study in the Defense Sector }},
year={2024},
volume={50},
number={05},
ISSN={1939-3520},
pages={1065-1079},
abstract={ The rapid diffusion of software systems into all aspects of human life has exacerbated security threats and thus amplified the requirement for proactive approaches for designing security as a default. Following evidence from previous studies, indicating organizational climate as a key influencer on developers’ security mindsets and behaviors, this study was focused on examining the relationship between team security climate level and developers’ actual practices when addressing security threats during software development. The empirical study was conducted in a defense software development organization and included a survey questionnaire completed by 212 developers from 50 software teams. The results were compared to managers’ evaluations regarding the implementation level of security mechanisms in the teams’ development. The findings indicate a positive relationship between team security climate level and the implementation level of security mechanisms in the teams' software development and that team productivity climate moderates this relationship. The results also reveal that team security climate mediates the association between manager–developer relationships and the implementation level of security mechanisms in software development. The study provides support to organizational climate theory and to the specific scale of organizational security climate, demonstrating the predictive validity of this scale, and sheds light on the influence of leadership and competitive facets on security engineering. },
keywords={Security;Meteorology;Software;Organizations;Behavioral sciences;Information security;Privacy},
doi={10.1109/TSE.2024.3374114},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3374114},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=may}

@ARTICLE{10462634,
author={Yuan, Yuanyuan and Pang, Qi and Wang, Shuai},
journal={ IEEE Transactions on Software Engineering },
title={{ Provably Valid and Diverse Mutations of Real-World Media Data for DNN Testing }},
year={2024},
volume={50},
number={05},
ISSN={1939-3520},
pages={1040-1064},
abstract={ Deep neural networks (DNNs) often accept high-dimensional media data (e.g., photos, text, and audio) and understand their perceptual content (e.g., a cat). To test DNNs, diverse inputs are needed to trigger mis-predictions. Some preliminary works use byte-level mutations or domain-specific filters (e.g., foggy), whose enabled mutations may be limited and likely error-prone. State-of-the-art (SOTA) works employ deep generative models to generate (infinite) inputs. Also, to keep the mutated inputs perceptually valid (e.g., a cat remains a “cat” after mutation), existing efforts rely on imprecise and less generalizable heuristics. This study revisits two key objectives in media input mutation — perception diversity (Div) and validity (Val) — in a rigorous manner based on manifold, a well-developed theory capturing perceptions of high-dimensional media data in a low-dimensional space. We show important results that Div and Val inextricably bound each other, and prove that SOTA generative model-based methods fundamentally fail to mutate real-world media data (either sacrificing Div or Val). In contrast, we discuss the feasibility of mutating real-world media data with provably high Div and Val based on manifold. Following, we concretize the technical solution of mutating media data of various formats (images, audios, text) via a unified manner based on manifold. Specifically, when media data are projected into a low-dimensional manifold, the data can be mutated by walking on the manifold with certain directions and step sizes. When contrasted with the input data, the mutated data exhibit encouraging Div in the perceptual traits (e.g., lying vs. standing dog) while retaining reasonably high Val (i.e., a dog remains a dog). We implement our techniques in DeepWalk for testing DNNs. DeepWalk constructs manifolds for media data offline. In online testing, DeepWalk walks on manifolds to generate mutated media data with provably high Div and Val. Our evaluation tests DNNs executing various tasks (e.g., classification, self-driving, machine translation) and media data of different types (image, audio, text). DeepWalk outperforms prior methods in terms of the testing comprehensiveness and can find more error-triggering inputs with higher quality. The tested DNNs, after repaired using DeepWalk's findings, exhibit better accuracy. },
keywords={Media;Testing;Manifolds;Dogs;Legged locomotion;Convolutional neural networks;Training},
doi={10.1109/TSE.2024.3370807},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3370807},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=may}

@ARTICLE{10471607,
author={Blanco, Alison Fernandez and Cordova, Araceli Queirolo and Bergel, Alexandre and Alcocer, Juan Pablo Sandoval},
journal={ IEEE Transactions on Software Engineering },
title={{ Asking and Answering Questions During Memory Profiling }},
year={2024},
volume={50},
number={05},
ISSN={1939-3520},
pages={1096-1117},
abstract={ The software engineering community has produced numerous tools, techniques, and methodologies for practitioners to analyze and optimize memory usage during software execution. However, little is known about the actual needs of programmers when analyzing memory behavior and how they use tools to address those needs. We conducted an exploratory study (i) to understand what a programmer needs to know when analyzing memory behavior and (ii) how a programmer finds that information with current tools. From our observations, we provide a catalog of 34 questions programmers ask themselves when analyzing memory behavior. We also report a detailed analysis of how some tools are used to answer these questions and the difficulties participants face during the process. Finally, we present four recommendations to guide researchers and developers in designing, evaluating, and improving memory behavior analysis tools. },
keywords={Memory management;Software;Resource management;Codes;Python;Libraries;Data mining},
doi={10.1109/TSE.2024.3377127},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3377127},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=may}

@ARTICLE{10471610,
author={Hu, Qiang and Guo, Yuejun and Xie, Xiaofei and Cordy, Maxime and Ma, Lei and Papadakis, Mike and Traon, Yves Le},
journal={ IEEE Transactions on Software Engineering },
title={{ Active Code Learning: Benchmarking Sample-Efficient Training of Code Models }},
year={2024},
volume={50},
number={05},
ISSN={1939-3520},
pages={1080-1095},
abstract={ The costly human effort required to prepare the training data of machine learning (ML) models hinders their practical development and usage in software engineering (ML4Code), especially for those with limited budgets. Therefore, efficiently training models of code with less human effort has become an emergent problem. Active learning is such a technique to address this issue that allows developers to train a model with reduced data while producing models with desired performance, which has been well studied in computer vision and natural language processing domains. Unfortunately, there is no such work that explores the effectiveness of active learning for code models. In this paper, we bridge this gap by building the first benchmark to study this critical problem - active code learning. Specifically, we collect 11 acquisition functions (which are used for data selection in active learning) from existing works and adapt them for code-related tasks. Then, we conduct an empirical study to check whether these acquisition functions maintain performance for code data. The results demonstrate that feature selection highly affects active learning and using output vectors to select data is the best choice. For the code summarization task, active code learning is ineffective which produces models with over a 29.64% gap compared to the expected performance. Furthermore, we explore future directions of active code learning with an exploratory study. We propose to replace distance calculation methods with evaluation metrics and find a correlation between these evaluation-based distance methods and the performance of code models. },
keywords={Codes;Data models;Task analysis;Training;Feature extraction;Training data;Labeling},
doi={10.1109/TSE.2024.3376964},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3376964},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=may}

@ARTICLE{10472898,
author={Sanchez, Ana B. and Parejo, Jose A. and Segura, Sergio and Duran, Amador and Papadakis, Mike},
journal={ IEEE Transactions on Software Engineering },
title={{ Mutation Testing in Practice: Insights From Open-Source Software Developers }},
year={2024},
volume={50},
number={05},
ISSN={1939-3520},
pages={1130-1143},
abstract={ Mutation testing drives the creation and improvement of test cases by evaluating their ability to identify synthetic faults. Over the past decades, the technique has gained popularity in academic circles. In practice, however, little is known about its adoption and use. While there are some pilot studies applying mutation testing in industry, the overall usage of mutation testing among developers remains largely unexplored. To fill this gap, this paper presents the results of a qualitative study among open-source developers on the use of mutation testing. Specifically, we report the results of a survey of 104 contributors to open-source projects using a variety of mutation testing tools. The findings of our study provide helpful insights into the use of mutation testing in practice, including its main benefits and limitations. Overall, we observe a high degree of satisfaction with mutation testing across different programming languages and mutation testing tools. Developers find the technique helpful for improving the quality of test suites, detecting bugs, and improving code maintainability. Popularity, usability, and configurability emerge as key factors for the adoption of mutation tools, whereas performance stands overwhelmingly as their main limitation. These results lay the groundwork for new research contributions and tools that meet the needs of developers and boost the widespread adoption of mutation testing. },
keywords={Testing;Codes;Surveys;Java;Industries;Artificial intelligence;Software development management},
doi={10.1109/TSE.2024.3377378},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3377378},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=may}

@ARTICLE{10474052,
author={Frantz, Miles and Xiao, Ya and Pias, Tanmoy Sarkar and Meng, Na and Yao, Danfeng},
journal={ IEEE Transactions on Software Engineering },
title={{ Methods and Benchmark for Detecting Cryptographic API Misuses in Python }},
year={2024},
volume={50},
number={05},
ISSN={1939-3520},
pages={1118-1129},
abstract={ Extensive research has been conducted to explore cryptographic API misuse in Java. However, despite the tremendous popularity of the Python language, uncovering similar issues has not been fully explored. The current static code analysis tools for Python are unable to scan the increasing complexity of the source code. This limitation decreases the analysis depth, resulting in more undetected cryptographic misuses. In this research, we propose Cryptolation, a Static Code Analysis (SCA) tool that provides security guarantees for complex Python cryptographic code. Most existing analysis tools for Python solely focus on specific Frameworks such as Django or Flask. However, using a SCA approach, Cryptolation focuses on the language and not any framework. Cryptolation performs an inter-procedural data-flow analysis to handle many Python language features through variable inference (statically predicting what the variable value is) and SCA. Cryptolation covers 59 Python cryptographic modules and can identify 18 potential cryptographic misuses that involve complex language features. In this paper, we also provide a comprehensive analysis and a state-of-the-art benchmark for understanding the Python cryptographic Application Program Interface (API) misuses and their detection. Our state-of-the-art benchmark PyCryptoBench includes 1,836 Python cryptographic test cases that covers both 18 cryptographic rules and five language features. PyCryptoBench also provides a framework for evaluating and comparing different cryptographic scanners for Python. To evaluate the performance of our proposed cryptographic Python scanner, we evaluated Cryptolation against three other state-of-the-art tools: Bandit, Semgrep, and Dlint. We evaluated these four tools using our benchmark PyCryptoBench and manual evaluation of (four Top-Ranked and 939 Un-Ranked) real-world projects. Our results reveal that, overall, Cryptolation achieved the highest precision throughout our testing; and the highest accuracy on our benchmark. Cryptolation had 100% precision on PyCryptoBench, and the highest precision on the real-world projects. },
keywords={Cryptography;Python;Benchmark testing;Codes;Ciphers;Libraries;Encryption},
doi={10.1109/TSE.2024.3377182},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3377182},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=may}

@ARTICLE{10476504,
author={Liu, Jiaqi and Zhang, Fengming and Zhang, Xin and Yu, Zhiwen and Wang, Liang and Zhang, Yao and Guo, Bin},
journal={ IEEE Transactions on Software Engineering },
title={{ hmCodeTrans: Human–Machine Interactive Code Translation }},
year={2024},
volume={50},
number={05},
ISSN={1939-3520},
pages={1163-1181},
abstract={ Code translation, i.e., translating one kind of code language to another, plays an important role in scenarios such as application modernization and multi-language versions of applications on different platforms. Even the most advanced machine-based code translation methods can not guarantee an error-free result. Therefore, the participance of software engineer is necessary. Considering both accuracy and efficiency, it is suggested to work in a human-machine collaborative way. However, in many realistic scenarios, human and machine collaborate ineffectively - model translates first and then human makes further editing, without any interaction. To solve this problem, we propose hmCodeTrans, a novel method that achieves code translation in an interactive human-machine collaborative way. It can (1) save the human effort by introducing two novel human-machine collaboration patterns: prefix-based and segment-based ones, which feed the software engineer's sequential or scattered editing back to model and thus enabling the model to make a better retranslation; (2) reduce the response time based on two proposed modules: attention cache module that avoids duplicate prefix inference with cached attention information, and suffix splicing module that reduces invalid suffix inference by splicing a predefined suffix. The experiments are conducted on two real datasets. Results show that compared with the baselines, our approach can effectively save the human effort and reduce the response time. Last but not least, a user study involving five real software engineers is given, which validates that the proposed approach owns the lowest human effort and shows the users’ satisfaction towards the approach. },
keywords={Codes;Software;Collaboration;Human-machine systems;Natural languages;Time factors;Task analysis},
doi={10.1109/TSE.2024.3379583},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3379583},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=may}

@ARTICLE{10476628,
author={Huang, Rubing and Cui, Chenhui and Lian, Junlong and Towey, Dave and Sun, Weifeng and Chen, Haibo},
journal={ IEEE Transactions on Software Engineering },
title={{ Toward Cost-Effective Adaptive Random Testing: An Approximate Nearest Neighbor Approach }},
year={2024},
volume={50},
number={05},
ISSN={1939-3520},
pages={1182-1214},
abstract={ Adaptive Random Testing (ART) enhances the testing effectiveness (including fault-detection capability) of Random Testing (RT) by increasing the diversity of the random test cases throughout the input domain. Many ART algorithms have been investigated such as Fixed-Size-Candidate-Set ART (FSCS) and Restricted Random Testing (RRT), and have been widely used in many practical applications. Despite its popularity, ART suffers from the problem of high computational costs during test-case generation, especially as the number of test cases increases. Although several strategies have been proposed to enhance the ART testing efficiency, such as the forgetting strategy and the $k$k -dimensional tree strategy, these algorithms still face some challenges, including: (1) Although these algorithms can reduce the computation time, their execution costs are still very high, especially when the number of test cases is large; and (2) To achieve low computational costs, they may sacrifice some fault-detection capability. In this paper, we propose an approach based on Approximate Nearest Neighbors (ANNs), called Locality-Sensitive Hashing ART (LSH-ART). When calculating distances among different test inputs, LSH-ART identifies the approximate (not necessarily exact) nearest neighbors for candidates in an efficient way. LSH-ART attempts to balance ART testing effectiveness and efficiency. },
keywords={Subspace constraints;Testing;Software;Power capacitors;Strips;Shape;Computational efficiency},
doi={10.1109/TSE.2024.3379592},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3379592},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=may}

@ARTICLE{10477672,
author={Nader Palacio, David and Velasco, Alejandro and Cooper, Nathan and Rodriguez, Alvaro and Moran, Kevin and Poshyvanyk, Denys},
journal={ IEEE Transactions on Software Engineering },
title={{ Toward a Theory of Causation for Interpreting Neural Code Models }},
year={2024},
volume={50},
number={05},
ISSN={1939-3520},
pages={1215-1243},
abstract={ Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of NCMs appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces do${}_{\textbf{code}}$code , a post hoc interpretability method specific to NCMs that is capable of explaining model predictions. do${}_{\textbf{code}}$code is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of do${}_{\textbf{code}}$code are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact of spurious correlations by grounding explanations of model behavior in properties of programming languages. To demonstrate the practical benefit of do${}_{\textbf{code}}$code , we illustrate the insights that our framework can provide by performing a case study on two popular deep learning architectures and ten NCMs. The results of this case study illustrate that our studied NCMs are sensitive to changes in code syntax. All our NCMs, except for the BERT-like model, statistically learn to predict tokens related to blocks of code (e.g., brackets, parenthesis, semicolon) with less confounding bias as compared to other programming language constructs. These insights demonstrate the potential of do${}_{\textbf{code}}$code as a useful method to detect and facilitate the elimination of confounding bias in NCMs. },
keywords={Codes;Predictive models;Correlation;Adaptation models;Measurement;Task analysis;Software engineering},
doi={10.1109/TSE.2024.3379943},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3379943},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=may}

@ARTICLE{10477676,
author={Ding, Zhijun and Xu, Yuehao and Feng, Binbin and Jiang, Changjun},
journal={ IEEE Transactions on Software Engineering },
title={{ Microservice Extraction Based on a Comprehensive Evaluation of Logical Independence and Performance }},
year={2024},
volume={50},
number={05},
ISSN={1939-3520},
pages={1244-1263},
abstract={ Monolithic architectures are becoming increasingly difficult to cope with complex applications, and microservice architectures, which offer flexibility and logical independence in development and maintenance, are the new choice for companies and developers. Migrating a legacy monolithic architecture application to a microservice architecture rather than building it from scratch is considered an easy way to use it. To ensure that the migrated microservice applications can take advantage of their benefits, we need to propose a reasonable and effective microservice extraction method. Considering the single responsibility principle in the microservice design principle, most existing microservice extraction methods only pursue the high logical independence of the extraction results and pay little attention to whether the extraction results have good performance. Applications need to perform well, and studies have shown that poor microservice extraction schemes can negatively impact the performance of the migrated application. As a result, when extracting, we should also consider the performance of the results. A few studies consider the performance of extraction results, but only in terms of a few factors affecting performance, such as network overhead, rather than considering all factors affecting performance comprehensively, which leads to an inaccurate evaluation of performance. Therefore, oriented toward the most widely used managed languages today, we propose an effective Microservice Extraction method based on a Comprehensive Evaluation of logical independence and performance (MECE). Firstly, we propose a workflow-based approach to evaluate the performance of microservice extraction results by considering multiple influencing factors, focusing on the management cost ignored in existing studies, and designing an effective management cost evaluation model. After that, we propose a meta-heuristic search-based algorithm to obtain feasible microservice extraction results. In experiments based on actual deployments, the extraction results of the MECE method obtained a performance improvement of up to 46.15% without significant loss of logical independence compared to existing methods, which verifies the effectiveness of the method. },
keywords={Microservice architectures;Feature extraction;Costs;Data mining;Architecture;Data transfer;Business},
doi={10.1109/TSE.2024.3380194},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3380194},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=may}

@ARTICLE{10478256,
author={Brandt, Carolin and Khatami, Ali and Wessel, Mairieli and Zaidman, Andy},
journal={ IEEE Transactions on Software Engineering },
title={{ Shaken, Not Stirred: How Developers Like Their Amplified Tests }},
year={2024},
volume={50},
number={05},
ISSN={1939-3520},
pages={1264-1280},
abstract={ Test amplification makes systematic changes to existing, manually written tests to provide tests complementary to an automated test suite. We consider developer-centric test amplification, where the developer explores, judges and edits the amplified tests before adding them to their maintained test suite. However, it is as yet unclear which kind of selection and editing steps developers take before including an amplified test into the test suite. In this paper we conduct an open source contribution study, amplifying tests of open source Java projects from GitHub. We report which deficiencies we observe in the amplified tests while manually filtering and editing them to open 39 pull requests with amplified tests. We present a detailed analysis of the maintainer's feedback regarding proposed changes, requested information, and expressed judgment. Our observations provide a basis for practitioners to take an informed decision on whether to adopt developer-centric test amplification. As several of the edits we observe are based on the developer's understanding of the amplified test, we conjecture that developer-centric test amplification should invest in supporting the developer to understand the amplified tests. },
keywords={Fuzzing;Test pattern generators;Testing;Computer crashes;Writing;Java;Codes},
doi={10.1109/TSE.2024.3381015},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3381015},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=may}

@ARTICLE{10479078,
author={Tong, Haonan and Zhang, Dalin and Liu, Jiqiang and Xing, Weiwei and Lu, Lingyun and Lu, Wei and Wu, Yumei},
journal={ IEEE Transactions on Software Engineering },
title={{ MASTER: Multi-Source Transfer Weighted Ensemble Learning for Multiple Sources Cross-Project Defect Prediction }},
year={2024},
volume={50},
number={05},
ISSN={1939-3520},
pages={1281-1305},
abstract={ Multi-source cross-project defect prediction (MSCPDP) attempts to transfer defect knowledge learned from multiple source projects to the target project. MSCPDP has drawn increasing attention from academic and industry communities owing to its advantages compared with single-source cross-project defect prediction (SSCPDP). However, two main problems, which are how to effectively extract the transferable knowledge from each source dataset and how to measure the amount of knowledge transferred from each source dataset to the target dataset, seriously restrict the performance of existing MSCPDP models. In this paper, we propose a novel multi-source transfer weighted ensemble learning (MASTER) method for MSCPDP. MASTER measures the weight of each source dataset based on feature importance and distribution difference and then extracts the transferable knowledge based on the proposed feature-weighted transfer learning algorithm. Experiments are performed on 30 software projects. We compare MASTER with the latest state-of-the-art MSCPDP methods with statistical test in terms of famous effort-unaware measures (i.e., PD, PF, AUC, and MCC) and two widely used effort-aware measures ($P_{opt}20\%$Popt20% and IFA). The experiment results show that: 1) MASTER can substantially improve the prediction performance compared with the baselines, e.g., an improvement of at least 49.1% in MCC, 48.1% in IFA; 2) MASTER significantly outperforms each baseline on most datasets in terms of AUC, MCC, $P_{opt}20\%$Popt20% and IFA; 3) MSCPDP model significantly performs better than the mean case of SSCPDP model on most datasets and even outperforms the best case of SSCPDP on some datasets. It can be concluded that 1) it is very necessary to conduct MSCPDP, and 2) the proposed MASTER is a more promising alternative for MSCPDP. },
keywords={Predictive models;Training;Weight measurement;Feature extraction;Software;Genetic algorithms;Ensemble learning},
doi={10.1109/TSE.2024.3381235},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3381235},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=may}

@ARTICLE{10496593,
author={Li, Haofeng and Tan, Tian and Li, Yue and Lu, Jie and Meng, Haining and Cao, Liqing and Huang, Yongheng and Li, Lian and Gao, Lin and Di, Peng and Lin, Liang and Cui, ChenXi},
journal={ IEEE Transactions on Software Engineering },
title={{ Generic Sensitivity: Generics-Guided Context Sensitivity for Pointer Analysis }},
year={2024},
volume={50},
number={05},
ISSN={1939-3520},
pages={1144-1162},
abstract={ Generic programming has found widespread application in object-oriented languages like Java. However, existing context-sensitive pointer analyses fail to leverage the benefits of generic programming. This paper introduces generic sensitivity, a new context customization scheme targeting generics. We design our context customization scheme in such a way that generic instantiation sites, i.e., locations instantiating generic classes/methods with concrete types, are always preserved as key context elements. This is realized by augmenting contexts with a type variable lookup map, which is efficiently generated in a context-sensitive manner throughout the analysis process. We have implemented various variants of generic-sensitive analysis in WALA and conducted extensive experiments to compare it with state-of-the-art approaches, including both traditional and selective context-sensitivity methods. The evaluation results demonstrate that generic sensitivity effectively enhances existing context-sensitivity approaches, striking a new balance between efficiency and precision. For instance, it enables a 1-object-sensitive analysis to achieve overall better precision compared to a 2-object-sensitive analysis, with an average speedup of 12.6 times (up to 62 times). },
keywords={Sensitivity;Receivers;Java;Resource management;Codes;Software;Benchmark testing},
doi={10.1109/TSE.2024.3377645},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3377645},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=may}

@ARTICLE{10479047,
author={Qin, Boqin and Chen, Yilun and Liu, Haopeng and Zhang, Hua and Wen, Qiaoyan and Song, Linhai and Zhang, Yiying},
journal={ IEEE Transactions on Software Engineering },
title={{ Understanding and Detecting Real-World Safety Issues in Rust }},
year={2024},
volume={50},
number={06},
ISSN={1939-3520},
pages={1306-1324},
abstract={ Rust is a relatively new programming language designed for systems software development. Its objective is to combine the safety guarantees typically associated with high-level languages with the performance efficiency often found in executable programs implemented in low-level languages. The core design of Rust is a set of strict safety rules enforced through compile-time checks. However, to support more low-level controls, Rust also allows programmers to bypass its compiler checks by writing unsafe code. As the adoption of Rust grows in the development of safety-critical software, it becomes increasingly important to understand what safety issues may elude Rust's compiler checks and manifest in real Rust programs. In this paper, we conduct a comprehensive, empirical study of Rust safety issues by close, manual inspection of 70 memory bugs, 100 concurrency bugs, and 110 programming errors leading to unexpected execution panics from five open-source Rust projects, five widely-used Rust libraries, and two online security databases. Our study answers three important questions: what memory-safety issues real Rust programs have, what concurrency bugs Rust programmers make, and how unexpected panics in Rust programs are caused. Our study reveals interesting real-world Rust program behaviors and highlights new issues made by Rust programmers. Building upon the findings of our study, we design and implement five static detectors. After being applied to the studied Rust programs and another 12 selected Rust projects, our checkers pinpoint 96 previously unknown bugs and report a negligible number of false positives, confirming their effectiveness and the value of our empirical study. },
keywords={Computer bugs;Safety;Codes;Detectors;Runtime;Programming;Libraries},
doi={10.1109/TSE.2024.3380393},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3380393},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jun}

@ARTICLE{10485640,
author={Tang, Yutian and Liu, Zhijie and Zhou, Zhichao and Luo, Xiapu},
journal={ IEEE Transactions on Software Engineering },
title={{ ChatGPT vs SBST: A Comparative Assessment of Unit Test Suite Generation }},
year={2024},
volume={50},
number={06},
ISSN={1939-3520},
pages={1340-1359},
abstract={ Recent advancements in large language models (LLMs) have demonstrated exceptional success in a wide range of general domain tasks, such as question answering and following instructions. Moreover, LLMs have shown potential in various software engineering applications. In this study, we present a systematic comparison of test suites generated by the ChatGPT LLM and the state-of-the-art SBST tool EvoSuite. Our comparison is based on several critical factors, including correctness, readability, code coverage, and bug detection capability. By highlighting the strengths and weaknesses of LLMs (specifically ChatGPT) in generating unit test cases compared to EvoSuite, this work provides valuable insights into the performance of LLMs in solving software engineering problems. Overall, our findings underscore the potential of LLMs in software engineering and pave the way for further research in this area. },
keywords={Chatbots;Codes;Task analysis;Software;Question answering (information retrieval);Computer bugs;Benchmark testing},
doi={10.1109/TSE.2024.3382365},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3382365},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jun}

@ARTICLE{10486822,
author={Zheng, Zibin and Su, Jianzhong and Chen, Jiachi and Lo, David and Zhong, Zhijie and Ye, Mingxi},
journal={ IEEE Transactions on Software Engineering },
title={{ DAppSCAN: Building Large-Scale Datasets for Smart Contract Weaknesses in DApp Projects }},
year={2024},
volume={50},
number={06},
ISSN={1939-3520},
pages={1360-1373},
abstract={ The Smart Contract Weakness Classification Registry (SWC Registry) is a widely recognized list of smart contract weaknesses specific to the Ethereum platform. Despite the SWC Registry not being updated with new entries since 2020, the sustained development of smart contract analysis tools for detecting SWC-listed weaknesses highlights their ongoing significance in the field. However, evaluating these tools has proven challenging due to the absence of a large, unbiased, real-world dataset. To address this problem, we aim to build a large-scale SWC weakness dataset from real-world DApp projects. We recruited 22 participants and spent 44 person-months analyzing 1,199 open-source audit reports from 29 security teams. In total, we identified 9,154 weaknesses and developed two distinct datasets, i.e., DAppSCAN-Source and DAppSCAN-Bytecode. The DAppSCAN-Source dataset comprises 39,904 Solidity files, featuring 1,618 SWC weaknesses sourced from 682 real-world DApp projects. However, the Solidity files in this dataset may not be directly compilable for further analysis. To facilitate automated analysis, we developed a tool capable of automatically identifying dependency relationships within DApp projects and completing missing public libraries. Using this tool, we created DAppSCAN-Bytecode dataset, which consists of 6,665 compiled smart contract with 888 SWC weaknesses. Based on DAppSCAN-Bytecode, we conducted an empirical study to evaluate the performance of state-of-the-art smart contract weakness detection tools. The evaluation results revealed sub-par performance for these tools in terms of both effectiveness and success detection rate, indicating that future development should prioritize real-world datasets over simplistic toy contracts. },
keywords={Smart contracts;Decentralized applications;Security;Codes;Toy manufacturing industry;Libraries;Blockchains},
doi={10.1109/TSE.2024.3383422},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3383422},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jun}

@ARTICLE{10494069,
author={Li, Yinghua and Dang, Xueqi and Pian, Weiguo and Habib, Andrew and Klein, Jacques and Bissyande, Tegawende F.},
journal={ IEEE Transactions on Software Engineering },
title={{ Test Input Prioritization for Graph Neural Networks }},
year={2024},
volume={50},
number={06},
ISSN={1939-3520},
pages={1396-1424},
abstract={ GNNs have shown remarkable performance in a variety of classification tasks. The reliability of GNN models needs to be thoroughly validated before their deployment to ensure their accurate functioning. Therefore, effective testing is essential for identifying vulnerabilities in GNN models. However, given the complexity and size of graph-structured data, the cost of manual labelling of GNN test inputs can be prohibitively high for real-world use cases. Although several approaches have been proposed in the general domain of Deep Neural Network (DNN) testing to alleviate this labelling cost issue, these approaches are not suitable for GNNs because they do not account for the interdependence between GNN test inputs, which is crucial for GNN inference. In this paper, we propose NodeRank, a novel test prioritization approach specifically for GNNs, guided by ensemble learning-based mutation analysis. Inspired by traditional mutation testing, where specific operators are applied to mutate code statements to identify whether provided test cases reveal faults, NodeRank operates on a crucial premise: If a test input (node) can kill many mutated models and produce different prediction results with many mutated inputs, this input is considered more likely to be misclassified by the GNN model and should be prioritized higher. Through prioritization, these potentially misclassified inputs can be identified earlier with limited manual labeling cost. NodeRank introduces mutation operators suitable for GNNs, focusing on three key aspects: the graph structure, the features of the graph nodes, and the GNN model itself. NodeRank generates mutants and compares their predictions against that of the initial test inputs. Based on the comparison results, a mutation feature vector is generated for each test input and used as the input to ranking models for test prioritization. Leveraging ensemble learning techniques, NodeRank combines the prediction results of the base ranking models and produces a misclassification score for each test input, which can indicate the likelihood of this input being misclassified. NodeRank sorts all the test inputs based on their scores in descending order. To evaluate NodeRank, we build 124 GNN subjects (i.e., a pair of dataset and GNN model), incorporating both natural and adversarial contexts. Our results demonstrate that NodeRank outperforms all the compared test prioritization approaches in terms of both APFD and PFD, which are widely-adopted metrics in this field. Specifically, NodeRank achieves an average improvement of between 4.41% and 58.11% on original datasets and between 4.96% and 62.15% on adversarial datasets. },
keywords={Predictive models;Graph neural networks;Testing;Labeling;Ensemble learning;Artificial neural networks;Context modeling},
doi={10.1109/TSE.2024.3385538},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3385538},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jun}

@ARTICLE{10495888,
author={Zhong, Chenxing and Li, Shanshan and Huang, Huang and Liu, Xiaodong and Chen, Zhikun and Zhang, Yi and Zhang, He},
journal={ IEEE Transactions on Software Engineering },
title={{ Domain-Driven Design for Microservices: An Evidence-Based Investigation }},
year={2024},
volume={50},
number={06},
ISSN={1939-3520},
pages={1425-1449},
abstract={ MicroService Architecture (MSA), a predominant architectural style in recent years, still faces the arduous task of identifying the boundaries of microservices. Domain-Driven Design (DDD) is regarded as one of the major design methods for addressing this task in practice, which aims to iteratively build domain models using a series of patterns, principles, and practices. The adoption of DDD for MSA (DDD4M in short) can, however, present considerable challenges in terms of a sufficient understanding of the methodological requirements and the application domains. It is imperative to establish a systematic understanding about the various aspects of employing DDD4M and provide effective guidance. This study reports an empirical inquiry that integrates a systematic literature review and a confirmatory survey. By reviewing 34 scientific studies and consulting 63 practitioners, this study reveals several distinctive findings with regard to the state and challenges of as well as the possible solutions for DDD4M applications, from the 5W1H perspectives: When, Where, Why, Who, What, and How. The analysis and synthesis of evidence show a wide variation in understanding of domain modeling artifacts. The status quo indicates the need for further methodological support in terms of application process, domain model design and implementation, and domain knowledge acquisition and management. To advance the state-of-the-practice, our findings were organized into a preliminary checklist that intends to assist practitioners by illuminating a DDD4M application process and the specific key considerations along the way. },
keywords={Microservice architectures;Software;Surveys;Systematics;Analytical models;Bibliographies;Reviews},
doi={10.1109/TSE.2024.3385835},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3385835},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jun}

@ARTICLE{10496502,
author={Calinescu, Radu and Imrie, Calum and Mangal, Ravi and Rodrigues, Genaina Nunes and Pasareanu, Corina and Santana, Misael Alpizar and Vazquez, Gricel},
journal={ IEEE Transactions on Software Engineering },
title={{ Controller Synthesis for Autonomous Systems With Deep-Learning Perception Components }},
year={2024},
volume={50},
number={06},
ISSN={1939-3520},
pages={1374-1395},
abstract={ We present DeepDECS, a new method for the synthesis of correct-by-construction software controllers for autonomous systems that use deep neural network (DNN) classifiers for the perception step of their decision-making processes. Despite major advances in deep learning in recent years, providing safety guarantees for these systems remains very challenging. Our controller synthesis method addresses this challenge by integrating DNN verification with the synthesis of verified Markov models. The synthesised models correspond to discrete-event software controllers guaranteed to satisfy the safety, dependability and performance requirements of the autonomous system, and to be Pareto optimal with respect to a set of optimisation objectives. We evaluate the method in simulation by using it to synthesise controllers for mobile-robot collision limitation, and for maintaining driver attentiveness in shared-control autonomous driving. },
keywords={Artificial neural networks;Uncertainty;Robots;Autonomous systems;Collision avoidance;Software;Mobile robots},
doi={10.1109/TSE.2024.3385378},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3385378},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jun}

@ARTICLE{10497172,
author={Weeraddana, Nimmi and Alfadel, Mahmoud and McIntosh, Shane},
journal={ IEEE Transactions on Software Engineering },
title={{ Characterizing Timeout Builds in Continuous Integration }},
year={2024},
volume={50},
number={06},
ISSN={1939-3520},
pages={1450-1463},
abstract={ Compute resources that enable Continuous Integration (CI, i.e., the automatic build and test cycle applied to the change sets that development teams produce) are a shared commodity that organizations need to manage. To prevent (erroneous) builds from consuming a large amount of resources, CI service providers often impose a time limit. CI builds that exceed the time limit are automatically terminated. While imposing a time limit helps to prevent abuse of the service, builds that timeout (a) consume the maximum amount of resources that a CI service is willing to provide and (b) leave CI users without an indication of whether the change set will pass or fail the CI process. Therefore, understanding timeout builds and the factors that contribute to them is important for improving the stability and quality of a CI service. In this paper, we investigate the prevalence of timeout builds and the characteristics associated with them. By analyzing a curated dataset of 936 projects that adopt the CircleCI service and report at least one timeout build, we find that the median duration of a timeout build (19.7 minutes) is more than five times that of a build that produces a pass or fail result (3.4 minutes). To better understand the factors contributing to timeout builds, we model timeout builds using characteristics of project build history, build queued time, timeout tendency, size, and author experience based on data collected from 105,663 CI builds. Our model demonstrates a discriminatory power that vastly surpasses that of a random predictor (Area Under the Receiver Operating characteristic Curve, i.e., $AUROC$AUROC = 0.939) and is highly stable in its performance ($AUROC$AUROC optimism = 0.0001). Moreover, our model reveals that the build history and timeout tendency features are strong indicators of timeout builds, with the timeout status of the most recent build accounting for the largest proportion of the explanatory power. A longitudinal analysis of the incidences of timeout builds (i.e., a study conducted over a period of time) indicates that 64.03% of timeout builds occur consecutively. In such cases, it takes a median of 24 hours before a build that passes or fails occurs. Our results imply that CI providers should exploit build history to anticipate timeout builds. },
keywords={Codes;History;Software development management;Feature extraction;Analytical models;Timing;Resource management},
doi={10.1109/TSE.2024.3387840},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3387840},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jun}

@ARTICLE{10497542,
author={Wen, Xin-Cheng and Gao, Cuiyun and Luo, Feng and Wang, Haoyu and Li, Ge and Liao, Qing},
journal={ IEEE Transactions on Software Engineering },
title={{ LIVABLE: Exploring Long-Tailed Classification of Software Vulnerability Types }},
year={2024},
volume={50},
number={06},
ISSN={1939-3520},
pages={1325-1339},
abstract={ Prior studies generally focus on software vulnerability detection and have demonstrated the effectiveness of Graph Neural Network (GNN)-based approaches for the task. Considering the various types of software vulnerabilities and the associated different degrees of severity, it is also beneficial to determine the type of each vulnerable code for developers. In this paper, we observe that the distribution of vulnerability type is long-tailed in practice, where a small portion of classes have massive samples (i.e., head classes) but the others contain only a few samples (i.e., tail classes). Directly adopting previous vulnerability detection approaches tends to result in poor detection performance, mainly due to two reasons. First, it is difficult to effectively learn the vulnerability representation due to the over-smoothing issue of GNNs. Second, vulnerability types in tails are hard to be predicted due to the extremely few associated samples. To alleviate these issues, we propose a Long-taIled software VulnerABiLity typE classification approach, called LIVABLE. LIVABLE mainly consists of two modules, including (1) vulnerability representation learning module, which improves the propagation steps in GNN to distinguish node representations by a differentiated propagation method. A sequence-to-sequence model is also involved to enhance the vulnerability representations. (2) adaptive re-weighting module, which adjusts the learning weights for different types according to the training epochs and numbers of associated samples by a novel training loss. We verify the effectiveness of LIVABLE in both type classification and vulnerability detection tasks. For vulnerability type classification, the experiments on the Fan et al. dataset show that LIVABLE outperforms the state-of-the-art methods by 24.18% in terms of the accuracy metric, and also improves the performance in predicting tail classes by 7.7%. To evaluate the efficacy of the vulnerability representation learning module in LIVABLE, we further compare it with the recent vulnerability detection approaches on three benchmark datasets, which shows that the proposed representation learning module improves the best baselines by 4.03% on average in terms of accuracy. },
keywords={Tail;Codes;Software;Representation learning;Training;Source coding;Graph neural networks},
doi={10.1109/TSE.2024.3382361},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3382361},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jun}

@ARTICLE{10500740,
author={Xu, Qinghua and Yue, Tao and Ali, Shaukat and Arratibel, Maite},
journal={ IEEE Transactions on Software Engineering },
title={{ Pretrain, Prompt, and Transfer: Evolving Digital Twins for Time-to-Event Analysis in Cyber-Physical Systems }},
year={2024},
volume={50},
number={06},
ISSN={1939-3520},
pages={1464-1477},
abstract={ Cyber-physicalnd systems (CPSs), e.g., elevators and autonomous driving systems, are progressively permeating our everyday lives. To ensure their safety, various analyses need to be conducted, such as anomaly detection and time-to-event analysis (the focus of this paper). Recently, it has been widely accepted that digital Twins (DTs) can be an efficient method to aid in developing, maintaining, and safe and secure operation of CPSs. However, CPSs frequently evolve, e.g., with new or updated functionalities, which demand their corresponding DTs be co-evolved, i.e., in synchronization with the CPSs. To that end, we propose a novel method, named PPT, utilizing an uncertainty-aware transfer learning for DT evolution. Specifically, we first pretrain PPT with a pretraining dataset to acquire generic knowledge about the CPSs, followed by adapting it to a specific CPS with the help of prompt tuning. Results highlight that PPT is effective in time-to-event analysis in both elevator and autonomous driving case studies, on average, outperforming a baseline method by 7.31 and 12.58 in terms of Huber loss, respectively. The experiment results also affirm the effectiveness of transfer learning, prompt tuning, and uncertainty quantification in terms of reducing Huber loss by at least 21.32, 3.14, and 4.08, respectively, in both case studies. },
keywords={Elevators;Uncertainty;Mathematical models;Digital twins;Tuning;Transfer learning;Training},
doi={10.1109/TSE.2024.3388572},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3388572},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jun}

@ARTICLE{10500748,
author={Chen, Pengzhou and Chen, Tao and Li, Miqing},
journal={ IEEE Transactions on Software Engineering },
title={{ MMO: Meta Multi-Objectivization for Software Configuration Tuning }},
year={2024},
volume={50},
number={06},
ISSN={1939-3520},
pages={1478-1504},
abstract={ Software configuration tuning is essential for optimizing a given performance objective (e.g., minimizing latency). Yet, due to the software's intrinsically complex configuration landscape and expensive measurement, there has been a rather mild success, particularly in preventing the search from being trapped in local optima. To address this issue, in this paper we take a different perspective. Instead of focusing on improving the optimizer, we work on the level of optimization model and propose a meta multi-objectivization (MMO) model that considers an auxiliary performance objective (e.g., throughput in addition to latency). What makes this model distinct is that we do not optimize the auxiliary performance objective, but rather use it to make similarly-performing while different configurations less comparable (i.e. Pareto nondominated to each other), thus preventing the search from being trapped in local optima. Importantly, by designing a new normalization method, we show how to effectively use the MMO model without worrying about its weight—the only yet highly sensitive parameter that can affect its effectiveness. Experiments on 22 cases from 11 real-world software systems/environments confirm that our MMO model with the new normalization performs better than its state-of-the-art single-objective counterparts on 82% cases while achieving up to $2.09\times$2.09× speedup. For 68% of the cases, the new normalization also enables the MMO model to outperform the instance when using it with the normalization from our prior FSE work under pre-tuned best weights, saving a great amount of resources which would be otherwise necessary to find a good weight. We also demonstrate that the MMO model with the new normalization can consolidate recent model-based tuning tools on 68% of the cases with up to $1.22\times$1.22× speedup in general. },
keywords={Software;Tuning;Search problems;Throughput;Optimization;Storms;Software systems},
doi={10.1109/TSE.2024.3388910},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3388910},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jun}

@ARTICLE{10507163,
author={Liu, Zhijie and Tang, Yutian and Luo, Xiapu and Zhou, Yuming and Zhang, Liang Feng},
journal={ IEEE Transactions on Software Engineering },
title={{ No Need to Lift a Finger Anymore? Assessing the Quality of Code Generation by ChatGPT }},
year={2024},
volume={50},
number={06},
ISSN={1939-3520},
pages={1548-1584},
abstract={ Large language models (LLMs) have demonstrated impressive capabilities across various natural language processing (NLP) tasks, such as machine translation, question answering, summarization, and so on. Additionally, LLMs are also highly valuable in supporting software engineering tasks, particularly in the field of code generation. Automatic code generation is a process of automatically generating source code or executable code based on given specifications or requirements, improving developer productivity. In this study, we perform a systematic empirical assessment to the quality of code generation using ChatGPT, a recent state-of-the-art product LLM. We leverage 728 algorithm problems in five languages (i.e., C, C++, Java, Python, and JavaScript) and 18 CWEs with 54 code scenarios for the code generation task. Our evaluation encompasses a comprehensive analysis of code snippets generated by ChatGPT, focusing on three critical aspects: correctness, complexity, and security. We also specifically investigate ChatGPT's ability to engage in multi-round fixing process (i.e., ChatGPT's dialog ability, chatting between users and ChatGPT for fixing generated buggy code) of facilitating code generation. By delving into the generated code and examining the experimental results, this work provides valuable insights into the performance of ChatGPT in tackling code generation tasks over the three critical aspects. The experimental results demonstrate that (1) ChatGPT is better at generating functionally correct code for problems before 2021 in different languages than problems after 2021 with $48.14\%$48.14% advantage in Accepted rate on judgment platform, but ChatGPT's ability to directly fix erroneous code with multi-round fixing process to achieve correct functionality is relatively weak; (2) the distribution of cyclomatic and cognitive complexity levels for code snippets in different languages varies. Furthermore, the multi-round fixing process with ChatGPT generally preserves or increases the complexity levels of code snippets; (3) in algorithm scenarios with languages of C, C++, and Java, and CWE scenarios with languages of C and Python3, the code generated by ChatGPT has relevant vulnerabilities. However, the multi-round fixing process for vulnerable code snippets demonstrates promising results, with more than $89\%$89% of vulnerabilities successfully addressed; and (4) code generation may be affected by ChatGPT's non-determinism factor, resulting in variations of code snippets in functional correctness, complexity, and security. Overall, our findings uncover potential issues and limitations that arise in the ChatGPT-based code generation and lay the groundwork for improving AI and LLM-based code generation techniques. },
keywords={Codes;Chatbots;Task analysis;Complexity theory;Security;Transformers;Electronic mail},
doi={10.1109/TSE.2024.3392499},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3392499},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jun}

@ARTICLE{10508261,
author={Li, Bo and Quan, Haowei and Wang, Jiawei and Liu, Pei and Cai, Haipeng and Miao, Yuan and Yang, Yun and Li, Li},
journal={ IEEE Transactions on Software Engineering },
title={{ Neural Library Recommendation by Embedding Project-Library Knowledge Graph }},
year={2024},
volume={50},
number={06},
ISSN={1939-3520},
pages={1620-1638},
abstract={ The prosperity of software applications brings fierce market competition to developers. Employing third-party libraries (TPLs) to add new features to projects under development and to reduce the time to market has become a popular way in the community. However, given the tremendous TPLs ready for use, it is challenging for developers to effectively and efficiently identify the most suitable TPLs. To tackle this obstacle, we propose an innovative approach named PyRec to recommend potentially useful TPLs to developers for their projects. Taking Python project development as a use case, PyRec embeds Python projects, TPLs, contextual information, and relations between those entities into a knowledge graph. Then, it employs a graph neural network to capture useful information from the graph to make TPL recommendations. Different from existing approaches, PyRec can make full use of not only project-library interaction information but also contextual information to make more accurate TPL recommendations. Comprehensive evaluations are conducted based on 12,421 Python projects involving 963 TPLs, 9,675 extra entities, 121,474 library usage records, and 73,277 contextual records. Compared with five representative approaches, PyRec improves the recommendation performance significantly in all cases. },
keywords={Python;Libraries;Vectors;Software;Mobile applications;Knowledge graphs;Graph neural networks},
doi={10.1109/TSE.2024.3393504},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3393504},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jun}

@ARTICLE{10508627,
author={Xian, Zixiang and Huang, Rubing and Towey, Dave and Fang, Chunrong and Chen, Zhenyu},
journal={ IEEE Transactions on Software Engineering },
title={{ TransformCode: A Contrastive Learning Framework for Code Embedding via Subtree Transformation }},
year={2024},
volume={50},
number={06},
ISSN={1939-3520},
pages={1600-1619},
abstract={ Artificial intelligence (AI) has revolutionized software engineering (SE) by enhancing software development efficiency. The advent of pre-trained models (PTMs) leveraging transfer learning has significantly advanced AI for SE. However, existing PTMs that operate on individual code tokens suffer from several limitations: They are costly to train and fine-tune; and they rely heavily on labeled data for fine-tuning on task-specific datasets. In this paper, we present TransformCode, a novel framework that learns code embeddings in a contrastive learning manner. Our framework is encoder-agnostic and language-agnostic, which means that it can leverage any encoder model and handle any programming language. We also propose a novel data-augmentation technique called abstract syntax tree (AST) transformation, which applies syntactic and semantic transformations to the original code snippets, to generate more diverse and robust samples for contrastive learning. Our framework has several advantages over existing methods: (1) It is flexible and adaptable, because it can easily be extended to other downstream tasks that require code representation (such as code-clone detection and classification); (2) it is efficient and scalable, because it does not require a large model or a large amount of training data, and it can support any programming language; (3) it is not limited to unsupervised learning, but can also be applied to some supervised learning tasks by incorporating task-specific labels or objectives; and (4) it can also adjust the number of encoder parameters based on computing resources. We evaluate our framework on several code-related tasks, and demonstrate its effectiveness and superiority over the state-of-the-art methods such as SourcererCC, Code2vec, and InferCode. },
keywords={Codes;Task analysis;Self-supervised learning;Syntactics;Semantics;Vectors;Training},
doi={10.1109/TSE.2024.3393419},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3393419},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jun}

@ARTICLE{10508714,
author={Lin, Yalan and Wan, Chengcheng and Bai, Shuwen and Gu, Xiaodong},
journal={ IEEE Transactions on Software Engineering },
title={{ VarGAN: Adversarial Learning of Variable Semantic Representations }},
year={2024},
volume={50},
number={06},
ISSN={1939-3520},
pages={1505-1517},
abstract={ Variable names are of critical importance in code representation learning. However, due to diverse naming conventions, variables often receive arbitrary names, leading to long-tail, out-of-vocabulary (OOV), and other well-known problems. While the Byte-Pair Encoding (BPE) tokenizer has addressed the surface-level recognition of low-frequency tokens, it has not noticed the inadequate training of low-frequency identifiers by code representation models, resulting in an imbalanced distribution of rare and common identifiers. Consequently, code representation models struggle to effectively capture the semantics of low-frequency variable names. In this paper, we propose VarGAN, a novel method for variable name representations. VarGAN strengthens the training of low-frequency variables through adversarial training. Specifically, we regard the code representation model as a generator responsible for producing vectors from source code. Additionally, we employ a discriminator that detects whether the code input to the generator contains low-frequency variables. This adversarial setup regularizes the distribution of rare variables, making them overlap with their corresponding high-frequency counterparts in the vector space. Experimental results demonstrate that VarGAN empowers CodeBERT to generate code vectors that exhibit more uniform distribution for both low- and high-frequency identifiers. There is an improvement of 8% in similarity and relatedness scores compared to VarCLR in the IdBench benchmark. VarGAN is also validated in downstream tasks, where it exhibits enhanced capabilities in capturing token- and code-level semantics. },
keywords={Codes;Vectors;Generators;Training;Semantics;Task analysis;Generative adversarial networks},
doi={10.1109/TSE.2024.3391730},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3391730},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jun}

@ARTICLE{10515209,
author={Lin, Zewei and Chen, Jiachi and Wu, Jiajing and Zhang, Weizhe and Wang, Yongjuan and Zheng, Zibin},
journal={ IEEE Transactions on Software Engineering },
title={{ CRPWarner: Warning the Risk of Contract-Related Rug Pull in DeFi Smart Contracts }},
year={2024},
volume={50},
number={06},
ISSN={1939-3520},
pages={1534-1547},
abstract={ In recent years, Decentralized Finance (DeFi) has grown rapidly due to the development of blockchain technology and smart contracts. As of March 2023, the estimated global cryptocurrency market cap has reached approximately $949 billion. However, security incidents continue to plague the DeFi ecosystem, and one of the most notorious examples is the “Rug Pull” scam. This type of cryptocurrency scam occurs when the developer of a particular token project intentionally abandons the project and disappears with investors’ funds. Despite only emerging in recent years, Rug Pull events have already caused significant financial losses. In this work, we manually collected and analyzed 103 real-world rug pull events, categorizing them based on their scam methods. Two primary categories were identified: Contract-related Rug Pull (through malicious functions in smart contracts) and Transaction-related Rug Pull (through cryptocurrency trading without utilizing malicious functions). Based on the analysis of rug pull events, we propose CRPWarner (short for Contract-related Rug Pull Risk Warner) to identify malicious functions in smart contracts and issue warnings regarding potential rug pulls. We evaluated CRPWarner on 69 open-source smart contracts related to rug pull events and achieved a 91.8% precision, 85.9% recall, and 88.7% F1-score. Additionally, when evaluating CRPWarner on 13,484 real-world token contracts on Ethereum, it successfully detected 4168 smart contracts with malicious functions, including zero-day examples. The precision of large-scale experiments reaches 84.9%. },
keywords={Smart contracts;Cryptocurrency;Finance;Decentralized applications;Blockchains;Ecosystems;Source coding},
doi={10.1109/TSE.2024.3392451},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3392451},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jun}

@ARTICLE{10516612,
author={Sokolowski, Daniel and Spielmann, David and Salvaneschi, Guido},
journal={ IEEE Transactions on Software Engineering },
title={{ Automated Infrastructure as Code Program Testing }},
year={2024},
volume={50},
number={06},
ISSN={1939-3520},
pages={1585-1599},
abstract={ Infrastructure as Code (IaC) enables efficient deployment and operation, which are crucial to releasing software quickly. As setups can be complex, developers implement IaC programs in general-purpose programming languages like TypeScript and Python, using PL-IaC solutions like Pulumi and AWS CDK. The reliability of such IaC programs is even more relevant than in traditional software because a bug in IaC impacts the whole system. Yet, even though testing is a standard development practice, it is rarely used for IaC programs. For instance, in August 2022, less than 1 % of the public Pulumi IaC programs on GitHub implemented tests. Available IaC program testing techniques severely limit the development velocity or require much development effort. To solve these issues, we propose Automated Configuration Testing (ACT), a methodology to test IaC programs in many configurations quickly and with low effort. ACT automatically mocks all resource definitions in the IaC program and uses generator and oracle plugins for test generation and validation. We implement ACT in ProTI, a testing tool for Pulumi TypeScript with a type-based generator and oracle, and support for application specifications. Our evaluation with 6 081 programs from GitHub and artificial benchmarks shows that ProTI can directly be applied to existing IaC programs, quickly finds bugs where current techniques are infeasible, and enables reusing existing generators and oracles thanks to its pluggable architecture. },
keywords={Testing;Generators;Software;Cloud computing;Engines;Codes;Libraries},
doi={10.1109/TSE.2024.3393070},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3393070},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jun}

@ARTICLE{10518179,
author={Ghorbani, Negar and Singh, Tarandeep and Garcia, Joshua and Malek, Sam},
journal={ IEEE Transactions on Software Engineering },
title={{ Darcy: Automatic Architectural Inconsistency Resolution in Java }},
year={2024},
volume={50},
number={06},
ISSN={1939-3520},
pages={1639-1657},
abstract={ Many mainstream programming languages lack extensive support for architectural constructs, such as software components, which limits software developers in employing many benefits of architecture-based development. To address this issue, Java, one of the most popular and widely-used programming languages, has introduced the Java Platform Module System (JPMS) in its 9th and subsequent versions. JPMS provides the notion of architectural constructs, i.e., software components, as an encapsulation of modules that helps developers construct and maintain large applications efficiently—as well as improving the encapsulation, security, and maintainability of Java applications in general and the JDK itself. However, ensuring that module declarations reflect the actual usage of modules in an application remains a challenge that results in developers mistakenly introducing inconsistent module dependencies at both compile- and run-time. In this paper, we studied JPMS properties and architectural notions in-depth and defined a defect model consisting of eight inconsistent modular dependencies that may arise in Java applications. Based on this defect model, we also present Darcy, a framework that leverages the defect model and static analysis techniques to automatically detect and repair the specified inconsistent dependencies within Java applications at both compile- and run-time. The results of our experiments, conducted over 52 open-source Java 9+ applications, indicate that architectural inconsistencies are widespread and demonstrate Darcy's effectiveness for automated resolution of these inconsistencies. },
keywords={Java;Computer architecture;Security;Encapsulation;Maintenance engineering;Unified modeling language;Software systems},
doi={10.1109/TSE.2024.3396433},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3396433},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jun}

@ARTICLE{10539620,
author={Kan, Shuangxiang and Gao, Yuhao and Zhong, Zexin and Sui, Yulei},
journal={ IEEE Transactions on Software Engineering },
title={{ Cross-Language Taint Analysis: Generating Caller-Sensitive Native Code Specification for Java }},
year={2024},
volume={50},
number={06},
ISSN={1939-3520},
pages={1518-1533},
abstract={ Cross-language programming is a common practice within the software development industry, offering developers a multitude of advantages such as expressiveness, interoperability, and cross-platform compatibility, for developing large-scale applications. As an important example, JNI (Java Native Interface) programming is widely used in diverse scenarios where Java interacts with code written in other programming languages, such as C or C++. Conventional static analysis based on a single programming language faces challenges when it comes to tracing the flow of values across multiple modules that are coded in different programming languages. In this paper, we introduce CSS, a new Caller-Sensitive Specification approach designed to enhance the static taint analysis of Java programs employing JNI to interface with C/C++ code. In contrast to conservative specifications, this approach takes into consideration the calling context of the invoked C/C++ functions (or cross-language context), resulting in more precise and concise specifications for the side effects of native code. Furthermore, CSS specifically enhances the capabilities of Java analyzers, enabling them to perform precise static taint analysis across language boundaries into native code. The experimental results show that CSS can accurately summarize value-flow information and enhance the ability of Java monolingual static analyzers for cross-language taint flow tracking. },
keywords={Java;Codes;C++ languages;Libraries;Static analysis;Source coding;Security},
doi={10.1109/TSE.2024.3392254},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3392254},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jun}

@ARTICLE{10411501,
author={Rong, Guoping and Yu, Yongda and Zhang, Yifan and Zhang, He and Shen, Haifeng and Shao, Dong and Kuang, Hongyu and Wang, Min and Wei, Zhao and Xu, Yong and Wang, Juhong},
journal={ IEEE Transactions on Software Engineering },
title={{ Distilling Quality Enhancing Comments From Code Reviews to Underpin Reviewer Recommendation }},
year={2024},
volume={50},
number={07},
ISSN={1939-3520},
pages={1658-1674},
abstract={ Code review is an important practice in software development. One of its main objectives is for the assurance of code quality. For this purpose, the efficacy of code review is subject to the credibility of reviewers, i.e., reviewers who have demonstrated strong evidence of previously making quality-enhancing comments are more credible than those who have not. Code reviewer recommendation (CRR) is designed to assist in recommending suitable reviewers for a specific objective and, in this context, assurance of code quality. Its performance is susceptible to the relevance of its training dataset to this objective, composed of all reviewers’ historical review comments, which, however, often contains a plethora of comments that are irrelevant to the enhancement of code quality. Furthermore, recommendation accuracy has been adopted as the sole metric to evaluate a recommender's performance, which is inadequate as it does not take reviewers’ relevant credibility into consideration. These two issues form the ground truth problem in CRR as they both originate from the relevance of dataset used to train and evaluate CRR algorithms. To tackle this problem, we first propose the concept of Quality-Enhancing Review Comments (QERC), which includes three types of comments - change-triggering inline comments, informative general comments, and approve-to-merge comments. We then devise a set of algorithms and procedures to obtain a distilled dataset by applying QERC to the original dataset. We finally introduce a new metric – reviewer's credibility for quality enhancement (RCQE) – as a complementary metric to recommendation accuracy for evaluating the performance of recommenders. To validate the proposed QERC-based approach to CRR, we conduct empirical studies using real data from seven projects containing over 82K pull requests and 346K review comments. Results show that: (a) QERC can effectively address the ground truth problem by distilling quality-enhancing comments from the dataset containing original code reviews, (b) QERC can assist recommenders in finding highly credible reviewers at a slight cost of recommendation accuracy, and (c) even “wrong” recommendations using the distilled dataset are likely to be more credible than those using the original dataset. },
keywords={Codes;Measurement;Software;History;Emojis;Software algorithms;Training},
doi={10.1109/TSE.2024.3356819},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3356819},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jul}

@ARTICLE{10478254,
author={Laaber, Christoph and Yue, Tao and Ali, Shaukat},
journal={ IEEE Transactions on Software Engineering },
title={{ Evaluating Search-Based Software Microbenchmark Prioritization }},
year={2024},
volume={50},
number={07},
ISSN={1939-3520},
pages={1687-1703},
abstract={ Ensuring that software performance does not degrade after a code change is paramount. A solution is to regularly execute software microbenchmarks, a performance testing technique similar to (functional) unit tests, which, however, often becomes infeasible due to extensive runtimes. To address that challenge, research has investigated regression testing techniques, such as test case prioritization (TCP), which reorder the execution within a microbenchmark suite to detect larger performance changes sooner. Such techniques are either designed for unit tests and perform sub-par on microbenchmarks or require complex performance models, drastically reducing their potential application. In this paper, we empirically evaluate single- and multi-objective search-based microbenchmark prioritization techniques to understand whether they are more effective and efficient than greedy, coverage-based techniques. For this, we devise three search objectives, i.e., coverage to maximize, coverage overlap to minimize, and historical performance change detection to maximize. We find that search algorithms (SAs) are only competitive with but do not outperform the best greedy, coverage-based baselines. However, a simple greedy technique utilizing solely the performance change history (without coverage information) is equally or more effective than the best coverage-based techniques while being considerably more efficient, with a runtime overhead of less than $1$1%. These results show that simple, non-coverage-based techniques are a better fit for microbenchmarks than complex coverage-based techniques. },
keywords={Benchmark testing;Software;Search problems;Runtime;Source coding;Java;Software measurement},
doi={10.1109/TSE.2024.3380836},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3380836},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jul}

@ARTICLE{10482873,
author={Liu, Changshu and Cetin, Pelin and Patodia, Yogesh and Ray, Baishakhi and Chakraborty, Saikat and Ding, Yangruibo},
journal={ IEEE Transactions on Software Engineering },
title={{ Automated Code Editing With Search-Generate-Modify }},
year={2024},
volume={50},
number={07},
ISSN={1939-3520},
pages={1675-1686},
abstract={ Code editing is essential in evolving software development. In literature, several automated code editing tools are proposed, which leverage Information Retrieval-based techniques and Machine Learning-based code generation and code editing models. Each technique comes with its own promises and perils, and for this reason, they are often used together to complement their strengths and compensate for their weaknesses. This paper proposes a hybrid approach to better synthesize code edits by leveraging the power of code search, generation, and modification. Our key observation is that a patch that is obtained by search & retrieval, even if incorrect, can provide helpful guidance to a code generation model. However, a retrieval-guided patch produced by a code generation model can still be a few tokens off from the intended patch. Such generated patches can be slightly modified to create the intended patches. We developed a novel tool to solve this challenge: SarGaM, which is designed to follow a real developer's code editing behavior. Given an original code version, the developer may search for the related patches, generate or write the code, and then modify the generated code to adapt it to the right context. Our evaluation of SarGaM on edit generation shows superior performance w.r.t. the current state-of-the-art techniques. SarGaM also shows its effectiveness on automated program repair tasks. },
keywords={Codes;Transformers;Decoding;Task analysis;Maintenance engineering;Context modeling;Training},
doi={10.1109/TSE.2024.3376387},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3376387},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jul}

@ARTICLE{10504708,
author={Zhao, Yutong and Xiao, Lu and Wong, Sunny},
journal={ IEEE Transactions on Software Engineering },
title={{ A Platform-Agnostic Framework for Automatically Identifying Performance Issue Reports With Heuristic Linguistic Patterns }},
year={2024},
volume={50},
number={07},
ISSN={1939-3520},
pages={1704-1725},
abstract={ Software performance is critical for system efficiency, with performance issues potentially resulting in budget overruns, project delays, and market losses. Such problems are reported to developers through issue tracking systems, which are often under-tagged, as the manual tagging process is voluntary and time-consuming. Existing automated performance issue tagging techniques, such as keyword matching and machine/deep learning models, struggle due to imbalanced datasets and a high degree of variance. This paper presents a novel hybrid classification approach, combining Heuristic Linguistic Patterns (HLPs) with machine/deep learning models to enable practitioners to automatically identify performance-related issues. The proposed approach works across three progressive levels: HLP tagging, sentence tagging, and issue tagging, with a focus on linguistic analysis of issue descriptions. The authors evaluate the approach on three different datasets collected from different projects and issue-tracking platforms to prove that the proposed framework is accurate, project- and platform-agnostic, and robust to imbalanced datasets. Furthermore, this study also examined how the two unique techniques of the framework, including the fuzzy HLP matching and the Issue HLP Matrix, contribute to the accuracy. Finally, the study explored the effectiveness and impact of two off-the-shelf feature selection techniques, Boruta and RFE, with the proposed framework. The results showed that the proposed framework has great potential for practitioners to accurately (with up to 100% precision, 66% recall, and 79% F1-score) identify performance issues, with robustness to imbalanced data and good transferability to new projects and issue tracking platforms. },
keywords={Tagging;Linguistics;Feature extraction;Vectors;Manuals;Robustness;Training},
doi={10.1109/TSE.2024.3390623},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3390623},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jul}

@ARTICLE{10507153,
author={Bu, Hao and Sun, Meng},
journal={ IEEE Transactions on Software Engineering },
title={{ Clopper-Pearson Algorithms for Efficient Statistical Model Checking Estimation }},
year={2024},
volume={50},
number={07},
ISSN={1939-3520},
pages={1726-1746},
abstract={ Statistical model checking (SMC) is a simulation-based formal verification technique to deal with the scalability problem faced by traditional model checking. The main workflow of SMC is to perform iterative simulations. The number of simulations depends on users’ requirement for the verification results, which can be very large if users require a high level of confidence and precision. Therefore, how to perform as fewer simulations as possible while achieving the same level of confidence and precision is one of the core problems of SMC. In this paper, we consider the estimation problem of SMC. Most existing statistical model checkers use the Okamoto bound to decide the simulation number. Although the Okamoto bound is sound, it is well known to be overly conservative. The simulation number decided by the Okamoto bound is usually much higher than it actually needs, which leads to a waste of time and computation resources. To tackle this problem, we propose an efficient, sound and lightweight estimation algorithm using the Clopper-Pearson confidence interval. We perform comprehensive numerical experiments and case studies to evaluate the performance of our algorithm, and the results show that our algorithm uses 40%-60% fewer simulations than the Okamoto bound. Our algorithm can be directly integrated into existing model checkers to reduce the verification time of SMC estimation problems. },
keywords={Estimation;Model checking;Software algorithms;Testing;Numerical models;Costs;Computational modeling},
doi={10.1109/TSE.2024.3392720},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3392720},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jul}

@ARTICLE{10510589,
author={Tu, Haoxin and Jiang, Lingxiao and Hong, Jiaqi and Ding, Xuhua and Jiang, He},
journal={ IEEE Transactions on Software Engineering },
title={{ Concretely Mapped Symbolic Memory Locations for Memory Error Detection }},
year={2024},
volume={50},
number={07},
ISSN={1939-3520},
pages={1747-1767},
abstract={ Memory allocation is a fundamental operation for managing memory objects in many programming languages. Misusing allocated memory objects (e.g., buffer overflow and use-after-free) can have catastrophic consequences. Symbolic execution-based approaches have been used to detect such memory errors, benefiting from their capabilities in automatic path exploration and test case generation. However, existing symbolic execution engines still suffer from fundamental limitations in modeling dynamic memory layouts; they either represent the locations of memory objects as concrete addresses and thus limit their analyses only to specific address layouts and miss errors that may only occur when the objects are located at special addresses, or represent the locations as simple symbolic variables without sufficient constraints and thus suffer from memory state explosion when they execute read/write operations involving symbolic addresses. Such limitations hinder the existing symbolic execution engines from effectively detecting certain memory errors. In this study, we propose SymLoc, a symbolic execution-based approach that uses concretely mapped symbolic memory locations to alleviate the limitations mentioned above. Specifically, a new integration of three techniques is designed in SymLoc: (1) the symbolization of addresses and encoding of symbolic addresses into path constraints, (2) the symbolic memory read/write operations using a symbolic-concrete memory map, and (3) the automatic tracking of the uses of symbolic memory locations. We build SymLoc on top of the well-known symbolic execution engine KLEE and demonstrate its benefits in terms of memory error detection and code coverage capabilities. Our evaluation results show that: for address-specific spatial memory errors, SymLoc can detect 23 more errors in GNU Coreutils, Make, and m4 programs that are difficult for other approaches to detect, and cover 15% and 48% more unique lines of code in the programs than two baseline approaches; for temporal memory errors, SymLoc can detect 8%-64% more errors in the Juliet Test Suite than various existing state-of-the-art memory error detectors. We also present two case studies to show sample memory errors detected by SymLoc along with their root causes and implications. },
keywords={Engines;Memory management;Detectors;Codes;Resource management;Random access memory;Software reliability},
doi={10.1109/TSE.2024.3395412},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3395412},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jul}

@ARTICLE{10521881,
author={Tu, Haoxin and Zhou, Zhide and Jiang, He and Yusuf, Imam Nur Bani and Li, Yuxian and Jiang, Lingxiao},
journal={ IEEE Transactions on Software Engineering },
title={{ Isolating Compiler Bugs by Generating Effective Witness Programs With Large Language Models }},
year={2024},
volume={50},
number={07},
ISSN={1939-3520},
pages={1768-1788},
abstract={ Compiler bugs pose a significant threat to safety-critical applications, and promptly as well as effectively isolating these bugs is crucial for assuring the quality of compilers. However, the limited availability of debugging information on reported bugs complicates the compiler bug isolation task. Existing compiler bug isolation approaches convert the problem into a test program mutation problem, but they are still limited by ineffective mutation strategies or high human effort requirements. Drawing inspiration from the recent progress of pre-trained Large Language Models (LLMs), such as ChatGPT, in code generation, we propose a new approach named LLM4CBI to utilize LLMs to generate effective test programs for compiler bug isolation. However, using LLMs directly for test program mutation may not yield the desired results due to the challenges associated with formulating precise prompts and selecting specialized prompts. To overcome the challenges, three new components are designed in LLM4CBI. First, LLM4CBI utilizes a program complexity-guided prompt production component, which leverages data and control flow analysis to identify the most valuable variables and locations in programs for mutation. Second, LLM4CBI employs a memorized prompt selection component, which adopts reinforcement learning to select specialized prompts for mutating test programs continuously. Third, a test program validation component is proposed to select specialized feedback prompts to avoid repeating the same mistakes during the mutation process. Compared with the state-of-the-art approaches (DiWi and RecBi) over 120 real bugs from the two most popular compilers, namely GCC and LLVM, our evaluation demonstrates the advantages of LLM4CBI: It can isolate 69.70%/21.74% and 24.44%/8.92% more bugs than DiWi and RecBi within Top-1/Top-5 ranked results. Additionally, we demonstrate that the LLMs component (i.e., GPT-3.5) used in LLM4CBI can be easily replaced by other LLMs while still achieving reasonable results in comparison to related studies. },
keywords={Computer bugs;Program processors;Task analysis;Codes;Reinforcement learning;Production;Mathematical models},
doi={10.1109/TSE.2024.3397822},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3397822},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jul}

@ARTICLE{10530516,
author={Zhang, Bing and Ren, Rong and Liu, Jia and Jiang, Mingcai and Ren, Jiadong and Li, Jingyue},
journal={ IEEE Transactions on Software Engineering },
title={{ SQLPsdem: A Proxy-Based Mechanism Towards Detecting, Locating and Preventing Second-Order SQL Injections }},
year={2024},
volume={50},
number={07},
ISSN={1939-3520},
pages={1807-1826},
abstract={ Due to well-hidden and stage-triggered properties of second-order SQL injections in web applications, current approaches are ineffective in addressing them and still report high false negatives and false positives. To reduce false results, we propose a Proxy-based static analysis and dynamic execution mechanism towards detecting, locating and preventing second-order SQL injections (SQLPsdem). The static analysis first locates SQL statements in web applications and identifies all data sources and injection points (e.g., Post, Sessions, Database, File names) that injection attacks can exploit. After that, we reconstruct the SQL statements and use attack engines to jointly generate attacks to cover all the state-of-the-art attack patterns so as to exploit these applications. We then use proxy-based dynamic execution to capture the data transmitted between web applications and their databases. The data are the reconstructed SQL statements with variable values from the attack payloads. If a web application is vulnerable, the data will contain malicious attacks on the database. We match the data with rules formulated by attack patterns to detect first and second-order SQL injection vulnerabilities in web applications, particularly the second-order ones. We use a representative and complete coverage of attack patterns and precise matching rules to reduce false results. By escaping and truncating malicious payloads in the data transmitted from the web application to the database, we can eliminate the possible negative impact of the data on the database. In the evaluation, by generating 52,771 SQL injection attacks using four attack generators, SQLPsdem successfully detects 26 second-order (including 13 newly discovered ones) and 375 first-order SQL injection vulnerabilities in 12 open-source web applications. SQLPsdem can also 100% eliminate the malicious impact of the data with negligible overhead. },
keywords={SQL injection;Databases;Static analysis;Structured Query Language;Payloads;Syntactics;Servers},
doi={10.1109/TSE.2024.3400404},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3400404},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jul}

@ARTICLE{10531111,
author={Wang, Xite and Tian, Senping and Cui, Wei},
journal={ IEEE Transactions on Software Engineering },
title={{ ContractCheck: Checking Ethereum Smart Contracts in Fine-Grained Level }},
year={2024},
volume={50},
number={07},
ISSN={1939-3520},
pages={1789-1806},
abstract={ The blockchain has been the main computing scenario for smart contracts, and the decentralized infrastructure of the blockchain is effectively implemented in a de-trusted and executable environment. However, vulnerabilities in smart contracts are particularly vulnerable to exploitation by malicious attackers and have always been a key issue in blockchain security. Existing traditional tools are inefficient in detecting vulnerabilities and have a high rate of false positives when detecting contracts. Some neural network methods have improved the detection efficiency, but they are not competent for fine-grained (code line level) vulnerability detection. We propose the ContractCheck model for detecting contract vulnerabilities based on neural network methods. ContractCheck extracts fine-grained segments from the abstract syntax tree (AST) and function call graph of smart contract source code. Furthermore, the segments are parsed into token flow retaining semantic information as uint, which are used to generate numerical vector sequences that can be trained using neural network methods. We conduct multiple rounds of experiments using a dataset constructed from 36,885 smart contracts and identified the optimal ContractCheck model structure by employing the Fasttext embedding vector algorithm and constructing a composite model using CNN and BiGRU for training the network. Evaluation on other datasets demonstrates that ContractCheck exhibits significant improvement in contract-level detection performance compared to other methods, with an increase of 23.60% in F1 score over the best existing method. Particularly, it achieves fine-grained detection based on neural network methods. The cases provide indicate that ContractCheck can effectively assist developers in accurately locating the presence of vulnerabilities, thereby enhancing the security of Ethereum smart contracts. },
keywords={Smart contracts;Codes;Blockchains;Neural networks;Semantics;Security;Vectors},
doi={10.1109/TSE.2024.3400294},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3400294},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jul}

@ARTICLE{10535119,
author={Gong, Lina and Zhang, Haoxiang},
journal={ IEEE Transactions on Software Engineering },
title={{ MR${}^{2}$ 2-KG: A Multi-Relation Multi-Rationale Knowledge Graph for Modeling Software Engineering Knowledge on Stack Overflow }},
year={2024},
volume={50},
number={07},
ISSN={1939-3520},
pages={1867-1887},
abstract={ Stack Overflow is a knowledge sharing platform where its users create and share informative content from both inside and outside the site. Prior studies have leveraged the relation across Stack Overflow posts through internal links to build services and applications to enhance the accessibility of knowledge. However, they focused on studying a knowledge unit that consists of a question post and all the associated answer posts to represent the relation. It is unknown whether such representation of knowledge on Stack Overflow could comprehensively model various complex relations among webpages, such as questions, answers, internal and external links. In addition, the rationales behind sharing knowledge on Stack Overflow have yet to be explored among distinct user groups, such as askers, answerers, readers who wish to learn. Thus, in this study, we first investigate the real-world characteristics of Stack Overflow knowledge by abstracting the complex knowledge representation into relations among its building blocks. We observe that a question thread includes three basic knowledge relations to reassemble into complex knowledge, that is, the hierarchy relation within the associated answers in a question, the coupling relation between knowledge artifacts (i.e., question or answer posts) through internal links, and the complimentary relation between Stack Overflow posts and external websites. All these three basic knowledge relations are informative and could be caused by different rationales when the crowdsourced knowledge is shared on Stack Overflow. Our findings highlight that it is necessary to propose a comprehensive knowledge graph to represent the real-world knowledge on Stack Overflow. Therefore, we further propose a Multi-Relation Multi-Rationale Knowledge Graph (MR${}^{2}$ 2-KG), whose nodes represent questions, answers, and external webpages. Edges in the MR${}^{2}$ 2-KG represent the rationales included in the three structures (i.e., question answering, duplicate, priori, posterior, parallelism, containment, and working examples knowledge). In addition, we develop an automated approach to model the nodes and edges to represent Stack Overflow knowledge associated with a question thread. Our case study shows that the automated knowledge representation generation can achieve an ROC AUC of 96% and MCC of 89% to identify edges in the MR${}^{2}$ 2-KG. To further evaluate the applicability of MR${}^{2}$ 2-KG, we develop an answer generator to help developers efficiently identify the answers that meet their intent. Our user study of 100 real-world Java questions indicates the usefulness of MR${}^{2}$ 2-KG. Finally, we discuss the implications of our findings for developers, researchers, and Stack Overflow moderators. },
keywords={Knowledge graphs;Message systems;Java;Couplings;Question answering (information retrieval);Software engineering;Parallel processing},
doi={10.1109/TSE.2024.3403108},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3403108},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jul}

@ARTICLE{10535138,
author={Wang, Shangwen and Geng, Mingyang and Lin, Bo and Sun, Zhensu and Wen, Ming and Liu, Yepang and Li, Li and Bissyande, Tegawende F. and Mao, Xiaoguang},
journal={ IEEE Transactions on Software Engineering },
title={{ Fusing Code Searchers }},
year={2024},
volume={50},
number={07},
ISSN={1939-3520},
pages={1852-1866},
abstract={ Code search, which consists in retrieving relevant code snippets from a codebase based on a given query, provides developers with useful references during software development. Over the years, techniques alternatively adopting different mechanisms to compute the relevance score between a query and a code snippet have been proposed to advance the state of the art in this domain, including those relying on information retrieval, supervised learning, and pre-training. Despite that, the usefulness of existing techniques is still compromised since they cannot effectively handle all the diversified queries and code in practice. To tackle this challenge, we present Dancer, a data fusion based code searcher. Our intuition (also the basic hypothesis of this study) is that existing techniques may complement each other because of the intrinsic differences in their working mechanisms. We have validated this hypothesis via an exploratory study. Based on that, we propose to fuse the results generated by different code search techniques so that the advantage of each standalone technique can be fully leveraged. Specifically, we treat each technique as a retrieval system and leverage well-known data fusion approaches to aggregate the results from different systems. We evaluate six existing code search techniques on two large-scale datasets, and exploit eight classic data fusion approaches to incorporate their results. Our experiments show that the best fusion approach is able to outperform the standalone techniques by 35% - 550% and 65% - 825% in terms of MRR (mean reciprocal rank) on the two datasets, respectively. },
keywords={Codes;Data integration;Supervised learning;Information retrieval;Semantics;Vectors;Sun},
doi={10.1109/TSE.2024.3403042},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3403042},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jul}

@ARTICLE{10535730,
author={Li, Jia and Moeini, Behrad and Nejati, Shiva and Sabetzadeh, Mehrdad and McCallen, Michael},
journal={ IEEE Transactions on Software Engineering },
title={{ A Lean Simulation Framework for Stress Testing IoT Cloud Systems }},
year={2024},
volume={50},
number={07},
ISSN={1939-3520},
pages={1827-1851},
abstract={ The Internet of Things (IoT) connects a plethora of smart devices globally across various applications like smart cities, autonomous vehicles, and health monitoring. Simulation plays a key role in the testing of IoT systems, noting that field testing of a complete IoT product may be infeasible or prohibitively expensive. This paper addresses a specific yet important need in simulation-based testing for IoT: Stress testing of cloud systems that are increasingly employed in IoT applications. Existing stress testing solutions for IoT demand significant computational resources, making them ill-suited and costly. We propose a lean simulation framework designed for IoT cloud stress testing. The framework enables efficient simulation of a large array of IoT and edge devices that communicate with the cloud. To facilitate simulation construction for practitioners, we develop a domain-specific language (DSL), named IoTECS, for generating simulators from model-based specifications. We provide the syntax and semantics of IoTECS and implement IoTECS using Xtext and Xtend. We assess simulators generated from IoTECS specifications for stress testing two real-world systems: a cloud-based IoT monitoring system developed by our industry partner and an IoT-connected vehicle system. Our empirical results indicate that simulators created using IoTECS: (1) achieve best performance when configured with Docker containerization; (2) effectively assess the service capacity of our case-study systems, and (3) outperform industrial stress-testing baseline tools, JMeter and Locust, by a factor of 3.5 in terms of the number of IoT and edge devices they can simulate using identical hardware resources. To gain initial insights about the usefulness of IoTECS in practice, we interviewed two engineers from our industry partner who have firsthand experience with IoTECS. Feedback from these interviews suggests that IoTECS is effective in stress testing IoT cloud systems, saving significant time and effort. },
keywords={Internet of Things;Testing;Cloud computing;Stress;Containers;Industries;Performance evaluation},
doi={10.1109/TSE.2024.3402157},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3402157},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jul}

@ARTICLE{10542726,
author={Ayerdi, Jon and Terragni, Valerio and Jahangirova, Gunel and Arrieta, Aitor and Tonella, Paolo},
journal={ IEEE Transactions on Software Engineering },
title={{ GenMorph: Automatically Generating Metamorphic Relations via Genetic Programming }},
year={2024},
volume={50},
number={07},
ISSN={1939-3520},
pages={1888-1900},
abstract={ Metamorphic testing is a popular approach that aims to alleviate the oracle problem in software testing. At the core of this approach are Metamorphic Relations (MRs), specifying properties that hold among multiple test inputs and corresponding outputs. Deriving MRs is mostly a manual activity, since their automated generation is a challenging and largely unexplored problem. This paper presents GenMorph, a technique to automatically generate MRs for Java methods that involve inputs and outputs that are boolean, numerical, or ordered sequences. GenMorph uses an evolutionary algorithm to search for effective test oracles, i.e., oracles that trigger no false alarms and expose software faults in the method under test. The proposed search algorithm is guided by two fitness functions that measure the number of false alarms and the number of missed faults for the generated MRs. Our results show that GenMorph generates effective MRs for 18 out of 23 methods (mutation score > 20%). Furthermore, it can increase Randoop's fault detection capability in 7 out of 23 methods, and Evosuite's in 14 out of 23 methods. When compared with AutoMR, a state-of-the-art MR generator, GenMorph also outperformed its fault detection capability in 9 out of 10 methods. },
keywords={Testing;Java;Generators;Space exploration;Manuals;Genetic programming;Filters},
doi={10.1109/TSE.2024.3407840},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3407840},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jul}

@ARTICLE{10546471,
author={Bai, Shuotong and Liu, Huaxiao and Dai, Enyan and Liu, Lei},
journal={ IEEE Transactions on Software Engineering },
title={{ Improving Issue-PR Link Prediction via Knowledge-Aware Heterogeneous Graph Learning }},
year={2024},
volume={50},
number={07},
ISSN={1939-3520},
pages={1901-1920},
abstract={ Links between issues and pull requests (PRs) assist GitHub developers in tackling technical challenges, gaining development inspiration, and improving repository maintenance. In realistic repositories, these links are still insufficiently established. Aiming at this situation, existing works focus on issues and PRs themselves and employ text similarity with additional information like issue size to predict issue-PR links, yet their effectiveness is unsatisfactory. The limitation is that issues and PRs are not isolated on GitHub. Rather, they are related to multiple GitHub sources, including repositories and submitters, which, through their diverse relationships, can supply potential and crucial knowledge about technical domains, developmental insights, and cross-repository technical details. To this end, we propose Auto IP Linker (AIPL), which introduces the heterogeneous graph to model multiple GitHub sources with their relationships. Further, it leverages the metapath-based technique to reveal and incorporate the potential information for a more comprehensive understanding of issues and PRs. Firstly, we identify 4 types of GitHub sources related to issues and PRs (repositories, users, issues, PRs) as well as their relationships, and model them into task-specific heterogeneous graphs. Next, we analyze information transmitted among issues or PRs to reveal which knowledge is crucial for them. Based on our analysis, we formulate a series of metapaths and employ the metapath-based technique to incorporate various information for learning the knowledge-aware embedding of issues and PRs. Finally, we can infer whether an issue and a PR can be linked based on their embedding. We evaluate the performance of AIPL on real-world data sets collected from GitHub. The results show that, compared to the baselines, AIPL can achieve average improvements of 15.94%, 15.19%, 20.52%, and 18.50% in terms of Accuracy, Precision, Recall, and F1-score. },
keywords={Software development management;Task analysis;Open source software;Firewire;Codes;Maintenance;Information processing},
doi={10.1109/TSE.2024.3408448},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3408448},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jul}

@ARTICLE{10557505,
author={Yu, Shengcheng and Fang, Chunrong and Du, Mingzhe and Ding, Zimin and Chen, Zhenyu and Su, Zhendong},
journal={ IEEE Transactions on Software Engineering },
title={{ Practical, Automated Scenario-Based Mobile App Testing }},
year={2024},
volume={50},
number={07},
ISSN={1939-3520},
pages={1949-1966},
abstract={ The importance of mobile application (app) quality assurance is increasing with the rapid development of the mobile Internet. Automated test generation approaches, as a dominant direction of app quality assurance, follow specific models or strategies, targeting at optimizing the code coverage. Such approaches lead to a huge gap between testing execution and app business logic. Test scripts developed by human testers consider business logic by focusing on testing scenarios. Due to the GUI-intensive feature of mobile apps, human testers always understand app GUI to organize test scripts for scenarios. This inspires us to utilize domain knowledge from app GUI understanding for scenario-based test generation. In this paper, we propose a novel approach, ScenTest, for scenario-based mobile app testing with event knowledge graph (EKG) via GUI image understanding. ScenTest tries to start automated testing by imitating human practices and integrating domain knowledge into scenario-based mobile app testing, realizing fully automated testing on target testing scenarios for the first time. ScenTest extracts four kinds of entities and five kinds of corresponding relationships from crowdsourced test reports, where the test events and app GUI information are presented, and constructs the EKGs for specific scenarios. Then, ScenTest conducts test generation for specific scenarios on different apps with the guidance of EKG with the combination consideration of app current state and testing context. We conduct an evaluation on ScenTest on different aspects. The results show that the test generation of ScenTest on the basis of EKG is effective, and ScenTest reveals 150+ distinct real-world bugs in specific scenarios compared with representative baselines. },
keywords={Testing;Electrocardiography;Mobile applications;Graphical user interfaces;Logic;Business;Knowledge graphs},
doi={10.1109/TSE.2024.3414672},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3414672},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jul}

@ARTICLE{10562221,
author={Song, Da and Xie, Xuan and Song, Jiayang and Zhu, Derui and Huang, Yuheng and Juefei-Xu, Felix and Ma, Lei},
journal={ IEEE Transactions on Software Engineering },
title={{ LUNA: A Model-Based Universal Analysis Framework for Large Language Models }},
year={2024},
volume={50},
number={07},
ISSN={1939-3520},
pages={1921-1948},
abstract={ Over the past decade, Artificial Intelligence (AI) has had great success recently and is being used in a wide range of academic and industrial fields. More recently, Large Language Models (LLMs) have made rapid advancements that have propelled AI to a new level, enabling and empowering even more diverse applications and industrial domains with intelligence, particularly in areas like software engineering and natural language processing. Nevertheless, a number of emerging trustworthiness concerns and issues exhibited in LLMs, e.g., robustness and hallucination, have already recently received much attention, without properly solving which the widespread adoption of LLMs could be greatly hindered in practice. The distinctive characteristics of LLMs, such as the self-attention mechanism, extremely large neural network scale, and autoregressive generation usage contexts, differ from classic AI software based on Convolutional Neural Networks and Recurrent Neural Networks and present new challenges for quality analysis. Up to the present, it still lacks universal and systematic analysis techniques for LLMs despite the urgent industrial demand across diverse domains. Towards bridging such a gap, in this paper, we initiate an early exploratory study and propose a universal analysis framework for LLMs, named LUNA, which is designed to be general and extensible and enables versatile analysis of LLMs from multiple quality perspectives in a human-interpretable manner. In particular, we first leverage the data from desired trustworthiness perspectives to construct an abstract model as an auxiliary analysis asset and proxy, which is empowered by various abstract model construction methods built-in LUNA. To assess the quality of the abstract model, we collect and define a number of evaluation metrics, aiming at both the abstract model level and the semantics level. Then, the semantics, which is the degree of satisfaction of the LLM w.r.t. the trustworthiness perspective, is bound to and enriches the abstract model with semantics, which enables more detailed analysis applications for diverse purposes, e.g., abnormal behavior detection. To better understand the potential usefulness of our analysis framework LUNA, we conduct a large-scale evaluation, the results of which demonstrate that 1) the abstract model has the potential to distinguish normal and abnormal behavior in LLM, 2) LUNA is effective for the real-world analysis of LLMs in practice, and the hyperparameter settings influence the performance, 3) different evaluation metrics are in different correlations with the analysis performance. In order to encourage further studies in the quality assurance of LLMs, we made all of the code and more detailed experimental results data available on the supplementary website of this paper https://sites.google.com/view/llm-luna. },
keywords={Hidden Markov models;Analytical models;Measurement;Semantics;Codes;Task analysis;Transformers},
doi={10.1109/TSE.2024.3411928},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3411928},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jul}

@ARTICLE{10543182,
author={Xia, Yuanjie and Liao, Lizhi and Chen, Jinfu and Li, Heng and Shang, Weiyi},
journal={ IEEE Transactions on Software Engineering },
title={{ Reducing the Length of Field-Replay Based Load Testing }},
year={2024},
volume={50},
number={08},
ISSN={1939-3520},
pages={1967-1983},
abstract={ As software systems continuously grow in size and complexity, performance and load related issues have become more common than functional issues. Load testing is usually performed before software releases to ensure that the software system can still provide quality service under a certain load. Therefore, one of the common challenges of load testing is to design realistic workloads that can represent the actual workload in the field. In particular, one of the most widely adopted and intuitive approaches is to directly replay the field workloads in the load testing environment. However, replaying a lengthy, e.g., 48 hours, field workloads is rather resource- and time-consuming, and sometimes even infeasible for large-scale software systems that adopt a rapid release cycle. On the other hand, replaying a short duration of the field workloads may still result in unrealistic load testing. In this work, we propose an automated approach to reduce the length of load testing that is driven by replaying the field workloads. The intuition of our approach is: if the measured performance associated with a particular system behaviour is already stable, we can skip subsequent testing of this system behaviour to reduce the length of the field workloads. In particular, our approach first clusters execution logs that are generated during the system runtime to identify similar system behaviours during the field workloads. Then, we use statistical methods to determine whether the measured performance associated with a system behaviour has been stable. We evaluate our approach on three open-source projects (i.e., OpenMRS, TeaStore, and Apache James). The results show that our approach can significantly reduce the length of field workloads while the workloads-after-reduction produced by our approach are representative of the original set of workloads. More importantly, the load testing results obtained by replaying the workloads after the reduction have high correlation and similar trend with the original set of workloads. Practitioners can leverage our approach to perform realistic field-replay based load testing while saving the needed resources and time. Our approach sheds light on future research that aims to reduce the cost of load testing for large-scale software systems. },
keywords={Testing;Software systems;Software;Runtime;System performance;Predictive models;Load modeling},
doi={10.1109/TSE.2024.3408079},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3408079},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=aug}

@ARTICLE{10546473,
author={Al Ghanmi, Hanouf and Bahsoon, Rami},
journal={ IEEE Transactions on Software Engineering },
title={{ ExplanaSC: A Framework for Determining Information Requirements for Explainable Blockchain Smart Contracts }},
year={2024},
volume={50},
number={08},
ISSN={1939-3520},
pages={1984-2004},
abstract={ Blockchain smart contracts (SCs) have emerged as a transformative technology, enabling the automation and execution of contractual agreements without the need for intermediaries. However, as SCs evolve to become more complex in their decentralised decision-making abilities, there are notable difficulties in comprehending the underlying reasoning process and ensuring users’ understanding. The existing literature primarily focuses on the technical aspects of SC, overlooking the exploration of the decision-making process within these systems and the involvement of humans. In this paper, we propose a framework that integrates human-centered design principles by applying Situation Awareness (SA) and goal directed task analysis (GDTA) concepts to determine information requirements necessary to design eXplainable smart contracts (XSC). The framework provides a structured approach for requirements engineers to identify information that can keep users well-informed throughout the decision-making process. The framework considers factors such as the business logic model, data model, and roles and responsibilities model to define specific information requirements that shape SC behaviour and necessitate explanations. To guide the determination of information requirements, the framework categorises SC decision mechanisms into autonomy, governance, processing, and behaviour. The ExplanaSC framework promotes the generation of XSC explanations through three levels aligned with SA: XSC explanation for perception, XSC explanation for comprehension, and XSC explanation for projection. Overall, this framework contributes to the development of XSC systems and lays the foundation for more transparent, and trustworthy decentralised applications. The XSC explanations aims to facilitate user awareness of complex decision-making processes. The evaluation of the framework uses a case to exemplify the working of our framework, its added value and limitations, and consults experts in the field for feedback and refinements. },
keywords={Decision making;Blockchains;Artificial intelligence;Smart contracts;Data models;Automation;Task analysis},
doi={10.1109/TSE.2024.3408632},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3408632},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=aug}

@ARTICLE{10569989,
author={Haas, Roman and Nommer, Raphael and Juergens, Elmar and Apel, Sven},
journal={ IEEE Transactions on Software Engineering },
title={{ Optimization of Automated and Manual Software Tests in Industrial Practice: A Survey and Historical Analysis }},
year={2024},
volume={50},
number={08},
ISSN={1939-3520},
pages={2005-2020},
abstract={ Context: Both automated and manual software testing are widely applied in practice. While being essential for project success and software quality, they are very resource-intensive, thus motivating the pursuit for optimization. Goal: We aim at understanding to what extent test optimization techniques for automated testing from the field of test case selection, prioritization, and test suite minimization can be applied to manual testing processes in practice. Method: We have studied the automated and manual testing process of five industrial study subjects from five different domains with different technological backgrounds and assessed the costs and benefits of test optimization techniques in industrial practice. In particular, we have carried out a cost–benefit analysis of two language-agnostic optimization techniques (test impact analysis and Pareto testing a technique we introduce in this paper) on 2,622 real-world failures from our subject's histories. Results: Both techniques maintain most of the fault detection capability while significantly reducing the test runtime. For automated testing, optimized test suites detect, on average, 80% of failures, while saving 66% of execution time, as compared to 81% failure detection rate for manual test suites and an average time saving of 43%. We observe an average speedup of the time to first failure of around 49 compared to a random test ordering. Conclusion: Our results suggest that optimization techniques from automated testing can be transferred to manual testing in industrial practice, resulting in lower test execution time and much lower time-to-feedback, but coming with process-related limitations and requirements for a successful implementation. All study subjects implemented one of our test optimization techniques in their processes, which demonstrates the practical impact of our findings. },
keywords={Testing;Optimization;Manuals;Software;Codes;Industries;Software systems},
doi={10.1109/TSE.2024.3418191},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3418191},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=aug}

@ARTICLE{10575923,
author={Baranov, Eduard and Chakraborty, Sourav and Legay, Axel and Meel, Kuldeep S. and Vinodchandran, N. Variyam},
journal={ IEEE Transactions on Software Engineering },
title={{ A Scalable $t$t-Wise Coverage Estimator: Algorithms and Applications }},
year={2024},
volume={50},
number={08},
ISSN={1939-3520},
pages={2021-2039},
abstract={ Owing to the pervasiveness of software in our modern lives, software systems have evolved to be highly configurable. Combinatorial testing has emerged as a dominant paradigm for testing highly configurable systems. Often constraints are employed to define the environments where a given system is expected to work. Therefore, there has been a sustained interest in designing constraint-based test suite generation techniques. A significant goal of test suite generation techniques is to achieve $t$t-wise coverage for higher values of $t$t. Therefore, designing scalable techniques that can estimate $t$t-wise coverage for a given set of tests and/or the estimation of maximum achievable $t$t-wise coverage under a given set of constraints is of crucial importance. The existing estimation techniques face significant scalability hurdles. We designed scalable algorithms with mathematical guarantees to estimate (i) $t$t-wise coverage for a given set of tests, and (ii) maximum $t$t-wise coverage for a given set of constraints. In particular, $\mathsf{ApproxCov}$ApproxCov takes in a test set $\mathcal{U}$U and returns an estimate of the $t$t-wise coverage of $\mathcal{U}$U that is guaranteed to be within $(1\pm\varepsilon)$(1±ε)-factor of the ground truth with probability at least $1-\delta$1−δ for a given tolerance parameter $\varepsilon$ε and a confidence parameter $\delta$δ. A scalable framework ${\mathsf{ApproxMaxCov}}$ApproxMaxCov for a given formula ${\mathsf{F}}$F outputs an approximation which is guaranteed to be within $(1\pm\varepsilon)$(1±ε) factor of the maximum achievable $t$t-wise coverage under ${\mathsf{F}}$F, with probability $\geq 1-\delta$≥1−δ for a given tolerance parameter $\varepsilon$ε and a confidence parameter $\delta$δ. Our comprehensive evaluation demonstrates that $\mathsf{ApproxCov}$ApproxCov and ${\mathsf{ApproxMaxCov}}$ApproxMaxCov can handle benchmarks that are beyond the reach of current state-of-the-art approaches. In this paper we present proofs of correctness of $\mathsf{ApproxCov}$ApproxCov, ${\mathsf{ApproxMaxCov}}$ApproxMaxCov, and of their generalizations. We show how the algorithms can improve the scalability of a test suite generator while maintaining its effectiveness. In addition, we compare several test suite generators on different feature combination sizes $t$t. },
keywords={Scalability;Computational modeling;Combinatorial testing;Benchmark testing;Software systems;Estimation;Software algorithms},
doi={10.1109/TSE.2024.3419919},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3419919},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=aug}

@ARTICLE{10579540,
author={Biagiola, Matteo and Tonella, Paolo},
journal={ IEEE Transactions on Software Engineering },
title={{ Boundary State Generation for Testing and Improvement of Autonomous Driving Systems }},
year={2024},
volume={50},
number={08},
ISSN={1939-3520},
pages={2040-2053},
abstract={ Recent advances in Deep Neural Networks (DNNs) and sensor technologies are enabling autonomous driving systems (ADSs) with an ever-increasing level of autonomy. However, assessing their dependability remains a critical concern. State-of-the-art ADS testing approaches modify the controllable attributes of a simulated driving environment until the ADS misbehaves. In such approaches, environment instances in which the ADS is successful are discarded, despite the possibility that they could contain hidden driving conditions in which the ADS may misbehave. In this paper, we present GenBo (GENerator of BOundary state pairs), a novel test generator for ADS testing. GenBo mutates the driving conditions of the ego vehicle (position, velocity and orientation), collected in a failure-free environment instance, and efficiently generates challenging driving conditions at the behavior boundary (i.e., where the model starts to misbehave) in the same environment instance. We use such boundary conditions to augment the initial training dataset and retrain the DNN model under test. Our evaluation results show that the retrained model has, on average, up to 3$\times$× higher success rate on a separate set of evaluation tracks with respect to the original DNN model. },
keywords={Testing;Artificial neural networks;Autonomous vehicles;Generators;Vehicle dynamics;Training;Roads},
doi={10.1109/TSE.2024.3420816},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3420816},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=aug}

@ARTICLE{10584343,
author={Kazemi, Farshad and Lamothe, Maxime and McIntosh, Shane},
journal={ IEEE Transactions on Software Engineering },
title={{ Characterizing the Prevalence, Distribution, and Duration of Stale Reviewer Recommendations }},
year={2024},
volume={50},
number={08},
ISSN={1939-3520},
pages={2096-2109},
abstract={ The appropriate assignment of reviewers is a key factor in determining the value that organizations can derive from code review. While inappropriate reviewer recommendations can hinder the benefits of the code review process, identifying these assignments is challenging. Stale reviewers, i.e., those who no longer contribute to the project, are one type of reviewer recommendation that is certainly inappropriate. Understanding and minimizing this type of recommendation can thus enhance the benefits of the code review process. While recent work demonstrates the existence of stale reviewers, to the best of our knowledge, attempts have yet to be made to characterize and mitigate them. In this paper, we study the prevalence and potential effects. We then propose and assess a strategy to mitigate stale recommendations in existing code reviewer recommendation tools. By applying five code reviewer recommendation approaches (LearnRec, RetentionRec, cHRev, Sofia, and WLRRec) to three thriving open-source systems with 5,806 contributors, we observe that, on average, 12.59% of incorrect recommendations are stale due to developer turnover; however, fewer stale recommendations are made when the recency of contributions is considered by the recommendation objective function. We also investigate which reviewers appear in stale recommendations and observe that the top reviewers account for a considerable proportion of stale recommendations. For instance, in 15.31% of cases, the top-3 reviewers account for at least half of the stale recommendations. Finally, we study how long stale reviewers linger after the candidate leaves the project, observing that contributors who left the project 7.7 years ago are still suggested to review change sets. Based on our findings, we propose separating the reviewer contribution recency from the other factors that are used by the CRR objective function to filter out developers who have not contributed during a specified duration. By evaluating this strategy with different intervals, we assess the potential impact of this choice on the recommended reviewers. The proposed filter reduces the staleness of recommendations, i.e., the Staleness Reduction Ratio (SRR) improves between 21.44%–92.39%. Yet since the strategy may increase active reviewer workload, careful project-specific exploration of the impact of the cut-off setting is crucial. },
keywords={Reviews;Codes;Linear programming;Software;Personnel;Delays;Task analysis},
doi={10.1109/TSE.2024.3422369},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3422369},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=aug}

@ARTICLE{10584357,
author={Fang, Chunrong and Sun, Weisong and Chen, Yuchen and Chen, Xiao and Wei, Zhao and Zhang, Quanjun and You, Yudu and Luo, Bin and Liu, Yang and Chen, Zhenyu},
journal={ IEEE Transactions on Software Engineering },
title={{ Esale: Enhancing Code-Summary Alignment Learning for Source Code Summarization }},
year={2024},
volume={50},
number={08},
ISSN={1939-3520},
pages={2077-2095},
abstract={ (Source) code summarization aims to automatically generate succinct natural language summaries for given code snippets. Such summaries play a significant role in promoting developers to understand and maintain code. Inspired by neural machine translation, deep learning-based code summarization techniques widely adopt an encoder-decoder framework, where the encoder transforms given code snippets into context vectors, and the decoder decodes context vectors into summaries. Recently, large-scale pre-trained models for source code (e.g., CodeBERT and UniXcoder) are equipped with encoders capable of producing general context vectors and have achieved substantial improvements on the code summarization task. However, although they are usually trained mainly on code-focused tasks and can capture general code features, they still fall short in capturing specific features that need to be summarized. In a nutshell, they fail to learn the alignment between code snippets and summaries (code-summary alignment for short). In this paper, we propose a novel approach to improve code summarization based on summary-focused tasks. Specifically, we exploit a multi-task learning paradigm to train the encoder on three summary-focused tasks to enhance its ability to learn code-summary alignment, including unidirectional language modeling (ULM), masked language modeling (MLM), and action word prediction (AWP). Unlike pre-trained models that mainly predict masked tokens in code snippets, we design ULM and MLM to predict masked words in summaries. Intuitively, predicting words based on given code snippets would help learn the code-summary alignment. In addition, existing work shows that AWP affects the prediction of the entire summary. Therefore, we further introduce the domain-specific task AWP to enhance the ability of the encoder to learn the alignment between action words and code snippets. We evaluate the effectiveness of our approach, called Esale, by conducting extensive experiments on four datasets, including two widely used datasets JCSD and PCSD, a cross-project Java dataset CPJD, and a multilingual language dataset CodeSearchNet. Experimental results show that Esale significantly outperforms state-of-the-art baselines in all three widely used metrics, including BLEU, METEOR, and ROUGE-L. Moreover, the human evaluation proves that the summaries generated by Esale are more informative and closer to the ground-truth summaries. },
keywords={Codes;Task analysis;Vectors;Predictive models;Source coding;Decoding;Software},
doi={10.1109/TSE.2024.3422274},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3422274},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=aug}

@ARTICLE{10584421,
author={Yang, Minghao and Yang, Shunkun and Wong, W. Eric},
journal={ IEEE Transactions on Software Engineering },
title={{ Multi-Objective Software Defect Prediction via Multi-Source Uncertain Information Fusion and Multi-Task Multi-View Learning }},
year={2024},
volume={50},
number={08},
ISSN={1939-3520},
pages={2054-2076},
abstract={ Effective software defect prediction (SDP) is important for software quality assurance. Numerous advanced SDP methods have been proposed recently. However, how to consider the task correlations and achieve multi-objective SDP accurately and efficiently still remains to be further explored. In this paper, we propose a novel multi-objective SDP method via multi-source uncertain information fusion and multi-task multi-view learning (MTMV) to accurately and efficiently predict the proneness, location, and type of defects. Firstly, multi-view features are extracted from multi-source static analysis results, reflecting uncertain defect location distribution and semantic information. Then, a novel MTMV model is proposed to fully fuse the uncertain defect information in multi-view features and realize effective multi-objective SDP. Specifically, the convolutional GRU encoders capture the consistency and complementarity of multi-source defect information to automatically filter the noise of false and missed alarms, and reduce location and type uncertainty of static analysis results. A global attention mechanism combined with the hard parameter sharing in MTMV fuse features according to their global importance of all tasks for balanced learning. Then, considering the latent task and feature correlations, multiple task-specific decoders jointly optimize all SDP tasks by sharing the learning experience. Through the extensive experiments on 14 datasets, the proposed method significantly improves the prediction performance over 12 baseline methods for all SDP objectives. The average improvements are 30.7%, 31.2%, and 32.4% for defect proneness, location, and type prediction, respectively. Therefore, the proposed multi-objective SDP method can provide more sufficient and precise insights for developers to significantly improve the efficiency of software analysis and testing. },
keywords={Feature extraction;Codes;Task analysis;Static analysis;Multitasking;Correlation;Accuracy},
doi={10.1109/TSE.2024.3421591},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3421591},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=aug}

@ARTICLE{10586831,
author={Cui, Lei and Yin, Junnan and Cui, Jiancong and Ji, Yuede and Liu, Peng and Hao, Zhiyu and Yun, Xiaochun},
journal={ IEEE Transactions on Software Engineering },
title={{ API2Vec++: Boosting API Sequence Representation for Malware Detection and Classification }},
year={2024},
volume={50},
number={08},
ISSN={1939-3520},
pages={2142-2162},
abstract={ Analyzing malware based on API call sequences is an effective approach, as these sequences reflect the dynamic execution behavior of malware. Recent advancements in deep learning have facilitated the application of these techniques to mine valuable information from API call sequences. However, these methods typically operate on raw sequences and may not effectively capture crucial information, especially in the case of multi-process malware, due to the API call interleaving problem. Furthermore, they often fail to capture contextual behaviors within or across processes, which is particularly important for identifying and classifying malicious activities. Motivated by this, we present API2Vec++, a graph-based API embedding method for malware detection and classification. First, we construct a graph model to represent the raw sequence. Specifically, we design the Temporal Process Graph (TPG) to model inter-process behaviors and the Temporal API Property Graph (TAPG) to model intra-process behaviors. Compared to our previous graph model, the TAPG model exposes operations with associated behaviors within the process through node properties and thus enhances detection and classification abilities. Using these graphs, we develop a heuristic random walk algorithm to generate numerous paths that can capture fine-grained malicious familial behavior. By pre-training these paths using the BERT model, we generate embeddings of paths and APIs, which can then be used for malware detection and classification. Experiments on a real-world malware dataset demonstrate that API2Vec++ outperforms state-of-the-art embedding methods and detection/classification methods in both accuracy and robustness, particularly for multi-process malware. },
keywords={Malware;Logic;Legged locomotion;Task analysis;Feature extraction;Encoding;Runtime},
doi={10.1109/TSE.2024.3422990},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3422990},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=aug}

@ARTICLE{10586843,
author={Elekes, Marton and Molnar, Vince and Micskei, Zoltan},
journal={ IEEE Transactions on Software Engineering },
title={{ To Do or Not to Do: Semantics and Patterns for Do Activities in UML PSSM State Machines }},
year={2024},
volume={50},
number={08},
ISSN={1939-3520},
pages={2124-2141},
abstract={ State machines are used in engineering many types of software-intensive systems. UML State Machines extend simple finite state machines with powerful constructs. Among the many extensions, there is one seemingly simple and innocent language construct that fundamentally changes state machines’ reactive model of computation: doActivity behaviors. DoActivity behaviors describe behavior that is executed independently from the state machine once entered in a given state, typically modeling complex computation or communication as background tasks. However, the UML specification or textbooks are vague about how the doActivity behavior construct should be appropriately used. This lack of guidance is a severe issue as, when improperly used, doActivities can cause concurrent, non-deterministic bugs that are especially challenging to find and could ruin a seemingly correct software design. The Precise Semantics of UML State Machines (PSSM) specification introduced detailed operational semantics for state machines. To the best of our knowledge, there is no rigorous review yet of doActivity's semantics as specified in PSSM. We analyzed the semantics by collecting evidence from cross-checking the text of the specification, its semantic model and executable test cases, and the simulators supporting PSSM. We synthesized insights about subtle details and emergent behaviors relevant to tool developers and advanced modelers. We reported inconsistencies and missing clarifications in more than 20 issues to the standardization committee. Based on these insights, we studied 11 patterns for doActivities detailing the consequences of using a doActivity in a given situation and discussing countermeasures or alternative design choices. We hope that our analysis of the semantics and the patterns help vendors develop conformant simulators or verification tools and engineers design better state machine models. },
keywords={Unified modeling language;Semantics;Current measurement;Computational modeling;Temperature measurement;Concurrent computing;Analytical models},
doi={10.1109/TSE.2024.3422845},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3422845},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=aug}

@ARTICLE{10586898,
author={Tsoukalas, Dimitrios and Mittas, Nikolaos and Arvanitou, Elvira-Maria and Ampatzoglou, Apostolos and Chatzigeorgiou, Alexander and Kechagias, Dionysios},
journal={ IEEE Transactions on Software Engineering },
title={{ Local and Global Explainability for Technical Debt Identification }},
year={2024},
volume={50},
number={08},
ISSN={1939-3520},
pages={2110-2123},
abstract={ In recent years, we have witnessed an important increase in research focusing on how machine learning (ML) techniques can be used for software quality assessment and improvement. However, the derived methodologies and tools lack transparency, due to the black-box nature of the employed machine learning models, leading to decreased trust in their results. To address this shortcoming, in this paper we extend the state-of-the-art and -practice by building explainable AI models on top of machine learning ones, to interpret the factors (i.e. software metrics) that constitute a module as in risk of having high technical debt (HIGH TD), to obtain thresholds for metric scores that are alerting for poor maintainability, and finally, we dig further to achieve local interpretation that explains the specific problems of each module, pinpointing to specific opportunities for improvement during TD management. To achieve this goal, we have developed project-specific classifiers (characterizing modules as HIGH and NOT-HIGH TD) for 21 open-source projects, and we explain their rationale using the SHapley Additive exPlanation (SHAP) analysis. Based on our analysis, complexity, comments ratio, cohesion, nesting of control flow statements, coupling, refactoring activity, and code churn are the most important reasons for characterizing classes as in HIGH TD risk. The analysis is complemented with global and local means of interpretation, such as metric thresholds and case-by-case reasoning for characterizing a class as in-risk of having HIGH TD. The results of the study are compared against the state-of-the-art and are interpreted from the point of view of both researchers and practitioners. },
keywords={Codes;Software;Software measurement;Complexity theory;Object oriented modeling;Informatics;Feature extraction},
doi={10.1109/TSE.2024.3422427},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3422427},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=aug}

@ARTICLE{10587162,
author={Chakraborty, Partha and Arumugam, Krishna Kanth and Alfadel, Mahmoud and Nagappan, Meiyappan and McIntosh, Shane},
journal={ IEEE Transactions on Software Engineering },
title={{ Revisiting the Performance of Deep Learning-Based Vulnerability Detection on Realistic Datasets }},
year={2024},
volume={50},
number={08},
ISSN={1939-3520},
pages={2163-2177},
abstract={ The impact of software vulnerabilities on everyday software systems is concerning. Although deep learning-based models have been proposed for vulnerability detection, their reliability remains a significant concern. While prior evaluation of such models reports impressive recall/F1 scores of up to 99%, we find that these models underperform in practical scenarios, particularly when evaluated on the entire codebases rather than only the fixing commit. In this paper, we introduce a comprehensive dataset (Real-Vul) designed to accurately represent real-world scenarios for evaluating vulnerability detection models. We evaluate DeepWukong, LineVul, ReVeal, and IVDetect vulnerability detection approaches and observe a surprisingly significant drop in performance, with precision declining by up to 95 percentage points and F1 scores dropping by up to 91 percentage points. A closer inspection reveals a substantial overlap in the embeddings generated by the models for vulnerable and uncertain samples (non-vulnerable or vulnerability not reported yet), which likely explains why we observe such a large increase in the quantity and rate of false positives. Additionally, we observe fluctuations in model performance based on vulnerability characteristics (e.g., vulnerability types and severity). For example, the studied models achieve 26 percentage points better F1 scores when vulnerabilities are related to information leaks or code injection rather than when vulnerabilities are related to path resolution or predictable return values. Our results highlight the substantial performance gap that still needs to be bridged before deep learning-based vulnerability detection is ready for deployment in practical settings. We dive deeper into why models underperform in realistic settings and our investigation revealed overfitting as a key issue. We address this by introducing an augmentation technique, potentially improving performance by up to 30%. We contribute (a) an approach to creating a dataset that future research can use to improve the practicality of model evaluation; (b) Real-Vul– a comprehensive dataset that adheres to this approach; and (c) empirical evidence that the deep learning-based models struggle to perform in a real-world setting. },
keywords={Codes;Synthetic data;Software systems;Security;Training;Source coding;Software reliability},
doi={10.1109/TSE.2024.3423712},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3423712},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=aug}

@ARTICLE{10596932,
author={Qiu, Fangcheng and Liu, Zhongxin and Hu, Xing and Xia, Xin and Chen, Gang and Wang, Xinyu},
journal={ IEEE Transactions on Software Engineering },
title={{ Vulnerability Detection via Multiple-Graph-Based Code Representation }},
year={2024},
volume={50},
number={08},
ISSN={1939-3520},
pages={2178-2199},
abstract={ During software development and maintenance, vulnerability detection is an essential part of software quality assurance. Even though many program-analysis-based and machine-learning-based approaches have been proposed to automatically detect vulnerabilities, they rely on explicit rules or patterns defined by security experts and suffer from either high false positives or high false negatives. Recently, an increasing number of studies leverage deep learning techniques, especially Graph Neural Network (GNN), to detect vulnerabilities. These approaches leverage program analysis to represent the program semantics as graphs and perform graph analysis to detect vulnerabilities. However, they suffer from two main problems: (i) Existing GNN-based techniques do not effectively learn the structural and semantic features from source code for vulnerability detection. (ii) These approaches tend to ignore fine-grained information in source code. To tackle these problems, in this paper, we propose a novel vulnerability detection approach, named MGVD (M ultiple-G raph-Based V ulnerability D etection), to detect vulnerable functions. To effectively learn the structural and semantic features from source code, MGVD uses three different ways to represent each function into multiple forms, i.e., two statement graphs and a sequence of tokens. Then we encode such representations to a three-channel feature matrix. The feature matrix contains the structural feature and the semantic feature of the function. And we add a weight allocation layer to distribute the weights between structural and semantic features. To overcome the second problem, MGVD constructs each graph representation of the input function using multiple different graphs instead of a single graph. Each graph focuses on one statement in the function and its nodes denote the related statements and their fine-grained code elements. Finally, MGVD leverages CNN to identify whether this function is vulnerable based on such feature matrix. We conduct experiments on 3 vulnerability datasets with a total of 30,341 vulnerable functions and 127,931 non-vulnerable functions. The experimental results show that our method outperforms the state-of-the-art by 9.68% – 10.28% in terms of F1-score. },
keywords={Semantics;Codes;Source coding;Graph neural networks;Software;Feature extraction;Deep learning},
doi={10.1109/TSE.2024.3427815},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3427815},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=aug}

@ARTICLE{10599338,
author={Masoudian, Maryam and Huang, Heqing and Amini, Morteza and Zhang, Charles},
journal={ IEEE Transactions on Software Engineering },
title={{ Mole: Efficient Crash Reproduction in Android Applications With Enforcing Necessary UI Events }},
year={2024},
volume={50},
number={08},
ISSN={1939-3520},
pages={2200-2218},
abstract={ To improve the quality of Android apps, developers use automated debugging and testing solutions to determine whether the previously found crashes are reproducible. However, existing GUI fuzzing solutions for Android apps struggle to reproduce crashes efficiently based solely on a crash stack trace. This trace provides the location in the app where the crash occurs. GUI fuzzing solutions currently in use rely on heuristics to generate UI events. Unfortunately, these events often do not align with the investigation of an app's UI event space to reach a specific location of code. Hence, they generate numerous events unrelated to the crash, leading to an event explosion. To address this issue, a precise static UI model of widgets and screens can greatly enhance the efficiency of a fuzzing tool in its search. Building such a model requires considering all possible combinations of event sequences on widgets since the execution order of events is not statically determined. However, this approach presents scalability challenges in complex apps with several widgets. In this paper, we propose a directed-based fuzzing solution to reduce an app's event domain to the necessary ones to trigger a crash. Our insight is that the dependencies between widgets in their visual presentation and attribute states provide valuable information in precisely identifying events that trigger a crash. We propose an attribute-sensitive reachability analysis (ASRA) to track dependent widgets in reachable paths to the crash point and distinguish between events in terms of their relevancy to be generated in the crash reproduction process. With instrumentation, we inject code to prune irrelevant events, reducing the event domain to search at run time. We used four famous fuzzing tools, Monkey, Ape, Stoat, and FastBot2, to assess the impact of our solution in decreasing the crash reproduction time and increasing the possibility of reproducing a crash. Our results show that the success ratio of reproducing a crash has increased for one-fourth of crashes. In addition, the average reproduction time of a crash becomes at least 2x faster. Wilcoxon Mann-Whitney test shows this enhancement is significant when our tool is used compared to baseline and insensitive reachability analysis. },
keywords={Computer crashes;Fuzzing;Codes;Explosions;Graphical user interfaces;Layout;XML},
doi={10.1109/TSE.2024.3428543},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3428543},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=aug}

@ARTICLE{10541859,
author={Lyu, Yunbo and Kang, Hong Jin and Widyasari, Ratnadira and Lawall, Julia and Lo, David},
journal={ IEEE Transactions on Software Engineering },
title={{ Evaluating SZZ Implementations: An Empirical Study on the Linux Kernel }},
year={2024},
volume={50},
number={09},
ISSN={1939-3520},
pages={2219-2239},
abstract={ The SZZ algorithm is used to connect bug-fixing commits to the earlier commits that introduced bugs. This algorithm has many applications and many variants have been devised. However, there are some types of commits that cannot be traced by the SZZ algorithm, referred to as “ghost commits”. The evaluation of how these ghost commits impact the SZZ implementations remains limited. Moreover, these implementations have been evaluated on datasets created by software engineering researchers from information in bug trackers and version controlled histories. Since Oct 2013, the Linux kernel developers have started labelling bug-fixing patches with the commit identifiers of the corresponding bug-inducing commit(s) as a standard practice. As of v6.1-rc5, 76,046 pairs of bug-fixing patches and bug-inducing commits are available. This provides a unique opportunity to evaluate the SZZ algorithm on a large dataset that has been created and reviewed by project developers, entirely independently of the biases of software engineering researchers. In this paper, we apply six SZZ implementations to 76,046 pairs of bug-fixing patches and bug-introducing commits from the Linux kernel. Our findings reveal that SZZ algorithms experience a more significant decline in recall on our dataset ($\downarrow 13.8\%$↓13.8%) as compared to prior findings reported by Rosa et al., and the disparities between the individual SZZ algorithms diminish. Moreover, we find that 17.47% of bug-fixing commits are ghost commits. Finally, we propose Tracing-Commit SZZ (TC-SZZ), that traces all commits in the change history of lines modified or deleted in bug-fixing commits. Applying TC-SZZ to all failure cases, excluding ghost commits, we found that TC-SZZ could identify 17.7% of them. Our further analysis based on git log found that 34.6% of bug-inducing commits were in the function history, 27.5% in the file history (but not in the function history), and 37.9% not in the file history. We further evaluated the effectiveness of ChatGPT in boosting the SZZ algorithm's ability to identify bug-inducing commits in the function history, in the file history and not in the file history. },
keywords={History;Software algorithms;Kernel;Linux;Computer bugs;Codes;Chatbots},
doi={10.1109/TSE.2024.3406718},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3406718},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=sep}

@ARTICLE{10599336,
author={Nashaat, Mona and Miller, James},
journal={ IEEE Transactions on Software Engineering },
title={{ Towards Efficient Fine-Tuning of Language Models With Organizational Data for Automated Software Review }},
year={2024},
volume={50},
number={09},
ISSN={1939-3520},
pages={2240-2253},
abstract={ Large language models like BERT and GPT possess significant capabilities and potential impacts across various applications. Software engineers often use these models for code-related tasks, including generating, debugging, and summarizing code. Nevertheless, large language models still have several flaws, including model hallucination. (e.g., generating erroneous code and producing outdated and inaccurate programs) and the substantial computational resources and energy required for training and fine-tuning. To tackle these challenges, we propose CodeMentor, a framework for few-shot learning to train large language models with the data available within the organization. We employ the framework to train a language model for code review activities, such as code refinement and review generation. The framework utilizes heuristic rules and weak supervision techniques to leverage available data, such as previous review comments, issue reports, and related code updates. Then, the framework employs the constructed dataset to fine-tune LLMs for code review tasks. Additionally, the framework integrates domain expertise by employing reinforcement learning with human feedback. This allows domain experts to assess the generated code and enhance the model performance. Also, to assess the performance of the proposed model, we evaluate it with four state-of-the-art techniques in various code review tasks. The experimental results attest that CodeMentor enhances the performance in all tasks compared to the state-of-the-art approaches, with an improvement of up to 22.3%, 43.4%, and 24.3% in code quality estimation, review generation, and bug report summarization tasks, respectively. },
keywords={Codes;Reviews;Task analysis;Data models;Large language models;Computational modeling;Training},
doi={10.1109/TSE.2024.3428324},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3428324},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=sep}

@ARTICLE{10602548,
author={Marcen, Ana C. and Iglesias, Antonio and Lapena, Raul and Perez, Francisca and Cetina, Carlos},
journal={ IEEE Transactions on Software Engineering },
title={{ A Systematic Literature Review of Model-Driven Engineering Using Machine Learning }},
year={2024},
volume={50},
number={09},
ISSN={1939-3520},
pages={2269-2293},
abstract={ Model-driven engineering (MDE) is a software engineering paradigm based on the systematic use of models. Over the past few years, engineers have significantly increased the use of MDE, which has been reported as a successful paradigm for developing industrial software. Recently, there have also been remarkable advancements in the Artificial Intelligence (AI) domain, with a significant increase in advanced Machine Learning (ML) techniques. The advances in both fields have led to a surge in works that dwell within the intersection of ML and MDE. This work places the focus on systematically reviewing works that leverage ML to solve MDE problems. We have reviewed a total of 9,194 papers, selecting 98 studies for further analysis. The results of our Systematic Literature Review (SLR) bring light to the current state of the art and trends in the field, discussing the drift in the usage of the different available ML techniques along with the remaining research gaps and open challenges. Our SLR has the potential to produce a positive impact in the research community by steering it towards ML techniques that have been successfully applied to solve MDE challenges. },
keywords={Unified modeling language;Systematics;Machine learning;Vectors;Bibliographies;Codes;Analytical models},
doi={10.1109/TSE.2024.3430514},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3430514},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=sep}

@ARTICLE{10606105,
author={Mo, Qi and Wang, Jianeng and Xie, Zhongwen and Liu, Cong and Dai, Fei},
journal={ IEEE Transactions on Software Engineering },
title={{ Enforcing Correctness of Collaborative Business Processes Using Plans }},
year={2024},
volume={50},
number={09},
ISSN={1939-3520},
pages={2313-2336},
abstract={ Generally, a collaborative business process is a distributed process, in which a set of parallel business processes are involved. These business processes have complementary competencies and knowledge, and cooperate with each other to achieve their common business goals. To ensure the correctness of collaborative business processes, we propose a novel plan-based correctness enforcement approach in this article, which is privacy-preserving, available and efficient. This approach first requires participating organizations to define their business processes. Then, each participating organization employs a set of reduction rules to build the public process of its business process, in which all internal private activities and the flows formed by them are removed. Next, a set of correct plans is generated from these public processes. A plan is essentially a process fragment without alternative routings. From the external perspective (i.e., ignoring all internal private activities and the flows formed by them), a parallel execution of the business processes corresponding to these public processes follows only one such plan. Lastly, each participating organization independently refactors its business process using these resulting correct plans. Using the message places (corresponding to the actual communication interfaces), these refactored processes are composed in parallel. Thus, a correct and loosely coupled enforced process is constructed. This approach is evaluated on actual collaborative business processes, and the experimental results show that compared with state-of-the-art enforcement proposals, it can achieve correctness enforcement while protecting the business privacy of organizations and is available. Meanwhile, it is also more efficient and scalable, even a collaborative business process with tens of millions of states can be enforced within a few seconds. },
keywords={Collaboration;Organizations;Proposals;Privacy;Petri nets;Explosions;Big Data},
doi={10.1109/TSE.2024.3431585},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3431585},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=sep}

@ARTICLE{10606107,
author={Yu, Zhe and Chakraborty, Joymallya and Menzies, Tim},
journal={ IEEE Transactions on Software Engineering },
title={{ FairBalance: How to Achieve Equalized Odds With Data Pre-Processing }},
year={2024},
volume={50},
number={09},
ISSN={1939-3520},
pages={2294-2312},
abstract={ This research seeks to benefit the software engineering society by providing a simple yet effective pre-processing approach to achieve equalized odds fairness in machine learning software. Fairness issues have attracted increasing attention since machine learning software is increasingly used for high-stakes and high-risk decisions. It is the responsibility of all software developers to make their software accountable by ensuring that the machine learning software do not perform differently on different sensitive demographic groups—satisfying equalized odds. Different from prior works which either optimize for an equalized odds related metric during the learning process like a black-box, or manipulate the training data following some intuition; this work studies the root cause of the violation of equalized odds and how to tackle it. We found that equalizing the class distribution in each demographic group with sample weights is a necessary condition for achieving equalized odds without modifying the normal training process. In addition, an important partial condition for equalized odds (zero average odds difference) can be guaranteed when the class distributions are weighted to be not only equal but also balanced (1:1). Based on these analyses, we proposed FairBalance, a pre-processing algorithm which balances the class distribution in each demographic group by assigning calculated weights to the training data. On eight real-world datasets, our empirical results show that, at low computational overhead, the proposed pre-processing algorithm FairBalance can significantly improve equalized odds without much, if any damage to the utility. FairBalance also outperforms existing state-of-the-art approaches in terms of equalized odds. To facilitate reuse, reproduction, and validation, we made our scripts available at https://github.com/hil-se/FairBalance. },
keywords={Software;Machine learning;Training data;Measurement;Ethics;Machine learning algorithms;Data models},
doi={10.1109/TSE.2024.3431445},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3431445},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=sep}

@ARTICLE{10606356,
author={Fakhoury, Sarah and Naik, Aaditya and Sakkas, Georgios and Chakraborty, Saikat and Lahiri, Shuvendu K.},
journal={ IEEE Transactions on Software Engineering },
title={{ LLM-Based Test-Driven Interactive Code Generation: User Study and Empirical Evaluation }},
year={2024},
volume={50},
number={09},
ISSN={1939-3520},
pages={2254-2268},
abstract={ Large language models (LLMs) have shown great potential in automating significant aspects of coding by producing natural code from informal natural language (NL) intent. However, given NL is informal, it does not lend easily to checking that the generated code correctly satisfies the user intent. In this paper, we propose a novel interactive workflow TiCoder for guided intent clarification (i.e., partial formalization) through tests to support the generation of more accurate code suggestions. Through a mixed methods user study with 15 programmers, we present an empirical evaluation of the effectiveness of the workflow to improve code generation accuracy. We find that participants using the proposed workflow are significantly more likely to correctly evaluate AI generated code, and report significantly less task-induced cognitive load. Furthermore, we test the potential of the workflow at scale with four different state-of-the-art LLMs on two python datasets, using an idealized proxy for a user feedback. We observe an average absolute improvement of 45.97% in the pass@1 code generation accuracy for both datasets and across all LLMs within 5 user interactions, in addition to the automatic generation of accompanying unit tests. },
keywords={Codes;Accuracy;Natural languages;Artificial intelligence;Python;Task analysis;Benchmark testing},
doi={10.1109/TSE.2024.3428972},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3428972},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=sep}

@ARTICLE{10609742,
author={Shin, Jiho and Hemmati, Hadi and Wei, Moshi and Wang, Song},
journal={ IEEE Transactions on Software Engineering },
title={{ Assessing Evaluation Metrics for Neural Test Oracle Generation }},
year={2024},
volume={50},
number={09},
ISSN={1939-3520},
pages={2337-2349},
abstract={ Recently, deep learning models have shown promising results in test oracle generation. Neural Oracle Generation (NOG) models are commonly evaluated using static (automatic) metrics which are mainly based on textual similarity of the output, e.g. BLEU, ROUGE-L, METEOR, and Accuracy. However, these textual similarity metrics may not reflect the testing effectiveness of the generated oracle within a test suite, which is often measured by dynamic (execution-based) test adequacy metrics such as code coverage and mutation score. In this work, we revisit existing oracle generation studies plus gpt-3.5 to empirically investigate the current standing of their performance in textual similarity and test adequacy metrics. Specifically, we train and run four state-of-the-art test oracle generation models on seven textual similarity and two test adequacy metrics for our analysis. We apply two different correlation analyses between these two different sets of metrics. Surprisingly, we found no significant correlation between the textual similarity metrics and test adequacy metrics. For instance, gpt-3.5 on the jackrabbit-oak project had the highest performance on all seven textual similarity metrics among the studied NOGs. However, it had the lowest test adequacy metrics compared to all the studied NOGs. We further conducted a qualitative analysis to explore the reasons behind our observations. We found that oracles with high textual similarity metrics but low test adequacy metrics tend to have complex or multiple chained method invocations within the oracle's parameters, making them hard for the model to generate completely, affecting the test adequacy metrics. On the other hand, oracles with low textual similarity metrics but high test adequacy metrics tend to have to call different assertion types or a different method that functions similarly to the ones in the ground truth. Overall, this work complements prior studies on test oracle generation with an extensive performance evaluation on textual similarity and test adequacy metrics and provides guidelines for better assessment of deep learning applications in software test generation in the future. },
keywords={Measurement;Correlation;Testing;Software;Codes;Accuracy;Electronic mail},
doi={10.1109/TSE.2024.3433463},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3433463},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=sep}

@ARTICLE{10613788,
author={Xu, Xiaoyan and Cogo, Filipe R. and McIntosh, Shane},
journal={ IEEE Transactions on Software Engineering },
title={{ Mitigating the Uncertainty and Imprecision of Log-Based Code Coverage Without Requiring Additional Logging Statements }},
year={2024},
volume={50},
number={09},
ISSN={1939-3520},
pages={2350-2362},
abstract={ Understanding code coverage is an important precursor to software maintenance activities (e.g., better testing). Although modern code coverage tools provide key insights, they typically rely on code instrumentation, resulting in significant performance overhead. An alternative approach to code instrumentation is to process an application's source code and the associated log traces in tandem. This so-called “log-based code coverage” approach does not impose the same performance overhead as code instrumentation. Chen et al. proposed LogCoCo — a tool that implements log-based code coverage for Java. While LogCoCo breaks important new ground, it has fundamental limitations, namely: uncertainty due to the lack of logging statements in conditional branches, and imprecision caused by dependency injection. In this study, we propose Log2Cov, a tool that generates log-based code coverage for programs written in Python and addresses uncertainty and imprecision issues. We evaluate Log2Cov on three large and active open-source systems. More specifically, we compare the performance of Log2Cov to that of Coverage.py, an instrumentation-based coverage tool for Python. Our results indicate that 1) Log2Cov achieves high precision without introducing runtime overhead; and 2) uncertainty and imprecision can be reduced by up to 11% by statically analyzing the program's source code and execution logs, without requiring additional logging instrumentation from developers. While our enhancements make substantial improvements, we find that future work is needed to handle conditional statements and exception handling blocks to achieve parity with instrumentation-based approaches. We conclude the paper by drawing attention to these promising directions for future work. },
keywords={Codes;Instruments;Uncertainty;Software;Python;Runtime;Source coding},
doi={10.1109/TSE.2024.3435067},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3435067},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=sep}

@ARTICLE{10620003,
author={Li, Zheng and Saldias-Vallejos, Nicolas and Seco, Diego and Rodriguez, Maria Andrea and Ranjan, Rajiv},
journal={ IEEE Transactions on Software Engineering },
title={{ Long Live the Image: On Enabling Resilient Production Database Containers for Microservice Applications }},
year={2024},
volume={50},
number={09},
ISSN={1939-3520},
pages={2363-2378},
abstract={ Microservices architecture advocates decentralized data ownership for building software systems. Particularly, in the Database per Service pattern, each microservice is supposed to maintain its own database and to handle the data related to its functionality. When implementing microservices in practice, however, there seems to be a paradox: The de facto technology (i.e., containerization) for microservice implementation is claimed to be unsuitable for the microservice component (i.e., database) in production environments, mainly due to the data persistence issues (e.g., dangling volumes) and security concerns. As a result, the existing discussions generally suggest replacing database containers with cloud database services, while leaving the on-premises microservice implementation out of consideration. After identifying three statelessness-dominant application scenarios, we proposed container-native data persistence as a conditional solution to enable resilient database containers in production. In essence, this data persistence solution distinguishes stateless data access (i.e., reading) from stateful data processing (i.e., creating, updating, and deleting), and thus it aims at the development of stateless microservices for suitable applications. In addition to developing our proposal, this research is particularly focused on its validation, via prototyping the solution and evaluating its performance, and via applying this solution to two real-world microservice applications. From the industrial perspective, the validation results have proved the feasibility, usability, and efficiency of fully containerized microservices for production in applicable situations. From the academic perspective, this research has shed light on the operation-side micro-optimization of individual microservices, which fundamentally expands the scope of “software micro-optimization” and reveals new research opportunities. },
keywords={Databases;Containers;Microservice architectures;Production;Usability;Runtime;Prototypes},
doi={10.1109/TSE.2024.3436623},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3436623},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=sep}

@ARTICLE{10623236,
author={Murali, Aniruddhan and Alfadel, Mahmoud and Nagappan, Meiyappan and Xu, Meng and Sun, Chengnian},
journal={ IEEE Transactions on Software Engineering },
title={{ AddressWatcher: Sanitizer-Based Localization of Memory Leak Fixes }},
year={2024},
volume={50},
number={09},
ISSN={1939-3520},
pages={2398-2411},
abstract={ Memory leak bugs are a major problem in C/C++ programs. They occur when memory objects are not deallocated. Developers need to manually deallocate these objects to prevent memory leaks. As such, several techniques have been proposed to automatically fix memory leaks. Although proposed approaches have merit in automatically fixing memory leaks, they present limitations. Static-based approaches attempt to trace the complete semantics of memory object across all paths. However, they have scalability-related challenges when the target program has a large number of paths (path explosion). On the other hand, dynamic approaches can spell out precise semantics of memory object only on a single execution path (it does not consider multiple execution paths). In this paper, we complement prior approaches by designing and implementing a novel framework named AddressWatcher. AddressWatcher allows the semantics of a memory object to be tracked on multiple execution paths. Addresswatcher accomplishes this by using a leak database that allows one to store and compare different execution paths of a leak over several test cases. Also, AddressWatcher performs lightweight instrumentation during compile time that is utilized during the program execution to watch and track memory leak read/writes. We conduct an evaluation of AddressWatcher over five popular packages, namely binutils, openssh, tmux, openssl and git. In 23 out of 50 real-world memory leak bugs, AddressWatcher correctly points to a free location to fix memory leaks. Finally, we submit 25 Pull Requests across 12 popular OSS repositories using AddressWatcher suggestions. Among these, 21 were merged leading to 5 open issues being addressed. In fact, our critical fix prompted a new version release for the calc repository, a program used to find large primes. Furthermore, our contributions through these PRs sparked intense discussions and appreciation in various repositories such as coturn, h2o, and radare2. },
keywords={Instruments;Computer bugs;Codes;Semantics;Resource management;Memory management;Leak detection},
doi={10.1109/TSE.2024.3438119},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3438119},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=sep}

@ARTICLE{10623254,
author={Shi, Chenghang and Li, Haofeng and Sui, Yulei and Lu, Jie and Li, Lian and Xue, Jingling},
journal={ IEEE Transactions on Software Engineering },
title={{ Pearl: A Multi-Derivation Approach to Efficient CFL-Reachability Solving }},
year={2024},
volume={50},
number={09},
ISSN={1939-3520},
pages={2379-2397},
abstract={ Context-free language (CFL) reachability is a fundamental framework for formulating program analyses. CFL-reachability analysis works on top of an edge-labeled graph by deriving reachability relations and adding them as labeled edges to the graph. Existing CFL-reachability algorithms typically adopt a single-reachability relation derivation (SRD) strategy, i.e., one reachability relation is derived at a time. Unfortunately, this strategy can lead to redundancy, hindering the efficiency of the analysis. To address this problem, this paper proposes Pearl, a multi-derivation approach that reduces derivation redundancy for CFL-reachability solving, which significantly improves the efficiency of CFL-reachability analysis. Our key insight is that multiple edges can be simultaneously derived via batch propagation of reachability relations. We also tailor our multi-derivation approach to tackle transitive relations that frequently arise when solving CFL-reachability. Specifically, we present a highly efficient transitive-aware variant, PearlPG, which enhances Pearl with propagation graphs, a lightweight but effective graph representation, to further diminish redundant derivations. We evaluate the performance of our approach on two clients, i.e., context-sensitive value-flow analysis and field-sensitive alias analysis for C/C++. By eliminating a large amount of redundancy, our approach outperforms two baselines including the standard CFL-reachability algorithm and a state-of-the-art solver Pocr specialized for fast transitivity solving. In particular, the empirical results demonstrate that, for value-flow analysis and alias analysis respectively, PearlPG runs 3.09$\times$× faster on average (up to 4.44$\times$×) and 2.25$\times$× faster on average (up to 3.31$\times$×) than Pocr, while also consuming less memory. },
keywords={Production;Standards;Termination of employment;Redundancy;Optimization;Heuristic algorithms;Symbols},
doi={10.1109/TSE.2024.3437684},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3437684},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=sep}

@ARTICLE{10624678,
author={Clerissi, Diego and Denaro, Giovanni and Mobilio, Marco and Mariani, Leonardo},
journal={ IEEE Transactions on Software Engineering },
title={{ DBInputs: Exploiting Persistent Data to Improve Automated GUI Testing }},
year={2024},
volume={50},
number={09},
ISSN={1939-3520},
pages={2412-2436},
abstract={ The generation of syntactically and semantically valid input data, able to exercise functionalities imposing constraints on the validity of the inputs, is a key challenge in automatic GUI (Graphical User Interface) testing. Existing test case generation techniques often rely on manually curated catalogs of values, although they might require significant effort to be created and maintained, and could hardly scale to applications with several input forms. Alternatively, it is possible to extract values from external data sources, such as the Web or publicly available knowledge bases. However, external sources are unlikely to provide the domain-specific and application-specific data that are often required to thoroughly exercise applications. This paper proposes DBInputs, a novel approach that automatically identifies domain-specific and application-specific inputs to effectively fulfill the validity constraints present in the tested GUI screens. The approach exploits syntactic and semantic similarities between the identifiers of the input fields shown on GUI screens and those of the tables of the target GUI application database, and extracts valid inputs from such database, automatically resolving the mismatch between the user interface and the database schema. DBInputs can properly cope with system testing and maintenance testing efforts, since databases are naturally and inexpensively available in those phases. Our experiments with 4 Web applications and 11 Mobile apps provide evidence that DBInputs can outperform techniques like random input selection and Link, a competing approach for searching inputs from knowledge bases, in both Web and Mobile domains. },
keywords={Graphical user interfaces;Databases;Testing;Generators;Syntactics;Semantics;Soft sensors},
doi={10.1109/TSE.2024.3439002},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3439002},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=sep}

@ARTICLE{10632563,
author={Kourtis, Georgios and Dixon, Clare and Fisher, Michael},
journal={ IEEE Transactions on Software Engineering },
title={{ Parameterized Verification of Leader/Follower Systems via Arithmetic Constraints }},
year={2024},
volume={50},
number={09},
ISSN={1939-3520},
pages={2458-2471},
abstract={ We introduce a variant of a formalism appearing in recent work geared towards modelling systems in which a distinguished entity (leader) orchestrates the operation of an arbitrary number of identical entities (followers). Our variant is better suited for the verification of system properties involving complex arithmetic conditions. Whereas the original formalism is translated into a tractable fragment of first-order temporal logic, aiming to utilize automated (first-order temporal logic) theorem provers for verification, our variant is translated into linear integer arithmetic, aiming to utilize satisfiability modulo theories (SMT) solvers for verification. In particular, for any given system specified in our formalism, we prove, for any natural number n, the existence of a linear integer arithmetic formula whose models are in one-to-one correspondence with certain counting abstractions (profiles) of executions of the system for n time steps. Thus, one is able to verify, for any natural number n, that all executions for n time steps of any such system have a given property by establishing that said formula logically entails the property. To highlight the practical utility of our approach, we specify and verify three consensus protocols, actively used in distributed database systems and low-power wireless networks. },
keywords={Arithmetic;Automata;Logic;Control systems;Polynomials;Formal verification;Europe},
doi={10.1109/TSE.2024.3440587},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3440587},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=sep}

@ARTICLE{10634302,
author={Yang, Guang and Zhou, Yu and Chen, Xiang and Zhang, Xiangyu and Zhuo, Terry Yue and Chen, Taolue},
journal={ IEEE Transactions on Software Engineering },
title={{ Chain-of-Thought in Neural Code Generation: From and for Lightweight Language Models }},
year={2024},
volume={50},
number={09},
ISSN={1939-3520},
pages={2437-2457},
abstract={ Large Language Models (LLMs) have demonstrated remarkable potential in code generation. The integration of Chain of Thought (CoT) reasoning can further boost their performance. However, current CoT methods often require manual writing or LLMs with over 100 billion parameters to generate, impeding their applicability in resource-constrained scenarios. In this study, we investigate lightweight Language Models ($\ell$ℓLMs), which are defined to have fewer than 10 billion parameters. Empirically, we find that most $\ell$ℓLMs cannot generate high-quality CoTs when prompted by the few-shot method, but can take advantage of high-quality CoTs generated elsewhere to improve their performance in code generation. Based on these findings, we design a novel approach COTTON which can leverage $\ell$ℓLMs to automatically generate CoTs for code generation. We synthesize new datasets and conduct extensive experiments on various benchmarks. The results show that the CoTs generated by COTTON outperform the baselines in terms of automated and human evaluation metrics. In particular, the CoTs generated by COTTON boost various $\ell$ℓLMs to achieve higher performance gains than those generated by LLMs such as ChatGLM (130B), and are competitive with those generated by Gemini and gpt-3.5-turbo. The results also reveal that COTTON not only improves the performance of $\ell$ℓLMs, but also enhances the performance of LLMs. Our study showcases the potential of $\ell$ℓLMs in software engineering applications. },
keywords={Codes;Cotton;Task analysis;Computational modeling;Benchmark testing;Training;Software engineering},
doi={10.1109/TSE.2024.3440503},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3440503},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=sep}

@ARTICLE{10636096,
author={Cui, Mohan and Xu, Hui and Tian, Hongliang and Zhou, Yangfan},
journal={ IEEE Transactions on Software Engineering },
title={{ rCanary: Detecting Memory Leaks Across Semi-Automated Memory Management Boundary in Rust }},
year={2024},
volume={50},
number={09},
ISSN={1939-3520},
pages={2472-2484},
abstract={ Rust is an effective system programming language that guarantees memory safety via compile-time verifications. It employs a novel ownership-based resource management model to facilitate automated deallocation. This model is anticipated to eliminate memory leaks. However, we observed that user intervention drives it into semi-automated memory management and makes it error-prone to cause leaks. In contrast to violating memory-safety guarantees restricted by the unsafe keyword, the boundary of leaking memory is implicit, and the compiler would not emit any warnings for developers. In this paper, we present rCanary, a static, non-intrusive, and fully automated model checker to detect leaks across the semi-automated boundary. We design an encoder to abstract data with heap allocation and formalize a refined leak-free memory model based on boolean satisfiability. It can generate SMT-Lib2 format constraints for Rust MIR and is implemented as a Cargo component. We evaluate rCanary by using flawed package benchmarks collected from the pull requests of open-source Rust projects. The results indicate that it is possible to recall all these defects with acceptable false positives. We further apply our tool to more than 1,200 real-world crates from crates.io and GitHub, identifying 19 crates having memory leaks. Our analyzer is also efficient, that costs 8.4 seconds per package. },
keywords={Memory management;Manuals;Safety;Resource management;Program processors;Computer languages;Vectors},
doi={10.1109/TSE.2024.3443624},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3443624},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=sep}

@ARTICLE{10589540,
author={Jiang, Shuai and Fu, Cai and He, Shuai and Lv, Jianqiang and Han, Lansheng and Hu, Hong},
journal={ IEEE Transactions on Software Engineering },
title={{ BinCola: Diversity-Sensitive Contrastive Learning for Binary Code Similarity Detection }},
year={2024},
volume={50},
number={10},
ISSN={1939-3520},
pages={2485-2497},
abstract={ Binary Code Similarity Detection (BCSD) is a fundamental binary analysis technique in the area of software security. Recently, advanced deep learning algorithms are integrated into BCSD platforms to achieve superior performance on well-known benchmarks. However, real-world large programs embed more complex diversities due to different compilers, various optimization levels, multiple architectures and even obfuscations. Existing BCSD solutions suffer from low accuracy issues in such complicated real-world application scenarios. In this paper, we propose BinCola, a novel Transformer-based dual diversity-sensitive contrastive learning framework that comprehensively considers the diversity of compiler options and candidate functions in the real-world application scenarios and employs the attention mechanism to fuse multi-granularity function features for enhancing generality and scalability. BinCola simultaneously compares multiple candidate functions across various compilation option scenarios to learn the differences caused by distinct compiler options and different candidate functions. We evaluate BinCola's performance in a variety of ways, including binary similarity detection and real-world vulnerability search in multiple application scenarios. The results demonstrate that BinCola achieves superior performance compared to state-of-the-art (SOTA) methods, with improvements of 2.80%, 33.62%, 22.41%, and 34.25% in cross-architecture, cross-optimization level, cross-compiler, and cross-obfuscation scenarios, respectively. },
keywords={Feature extraction;Contrastive learning;Vectors;Source coding;Software;Semantics;Training},
doi={10.1109/TSE.2024.3411072},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3411072},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=oct}

@ARTICLE{10623245,
author={Liang, Anda and Murphy-Hill, Emerson and Weimer, Westley and Huang, Yu},
journal={ IEEE Transactions on Software Engineering },
title={{ A Controlled Experiment in Age and Gender Bias When Reading Technical Articles in Software Engineering }},
year={2024},
volume={50},
number={10},
ISSN={1939-3520},
pages={2498-2511},
abstract={ Online platforms and communities are a critical part of modern software engineering, yet are often affected by human biases. While previous studies investigated human biases and their potential harms against the efficiency and fairness of online communities, they have mainly focused on the open source and Q & A platforms, such as GitHub and Stack Overflow, but overlooked the audience-focused online platforms for delivering programming and SE-related technical articles, where millions of software engineering practitioners share, seek for, and learn from high-quality software engineering articles (i.e., technical articles for SE). Furthermore, most of the previous work has revealed gender and race bias, but we have little knowledge about the effect of age on software engineering practice. In this paper, we propose to investigate the effect of authors’ demographic information (gender and age) on the evaluation of technical articles on software engineering and potential behavioral differences among participants. We conducted a survey-based and controlled human study and collected responses from 540 participants to investigate developers’ evaluation of technical articles for software engineering. By controlling the gender and age of the author profiles of technical articles for SE, we found that raters tend to have more positive content depth evaluations for younger male authors when compared to older male authors and that male participants conduct technical article evaluations faster than female participants, consistent with prior study findings. Surprisingly, different from other software engineering evaluation activities (e.g., code review, pull request, etc.), we did not find a significant difference in the genders of authors on the evaluation outcome of technical articles in SE. },
keywords={Software engineering;Software;Software development management;Reviews;Codes;Electronic mail;Industries},
doi={10.1109/TSE.2024.3437355},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3437355},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=oct}

@ARTICLE{10636040,
author={Zhou, Ziyi and Li, Mingchen and Yu, Huiqun and Fan, Guisheng and Yang, Penghui and Huang, Zijie},
journal={ IEEE Transactions on Software Engineering },
title={{ Learning to Generate Structured Code Summaries From Hybrid Code Context }},
year={2024},
volume={50},
number={10},
ISSN={1939-3520},
pages={2512-2528},
abstract={ Code summarization aims to automatically generate natural language descriptions for code, and has become a rapidly expanding research area in the past decades. Unfortunately, existing approaches mainly focus on the “one-to-one” mapping from methods to short descriptions, which hinders them from becoming practical tools: 1) The program context is ignored, so they have difficulty in predicting keywords outside the target method; 2) They are typically trained to generate brief function descriptions with only one sentence in length, and therefore have difficulty in providing specific information. These drawbacks are partially due to the limitations of public code summarization datasets. In this paper, we first build a large code summarization dataset including different code contexts and summary content annotations, and then propose a deep learning framework that learns to generate structured code summaries from hybrid program context, named StructCodeSum. It provides both an LLM-based approach and a lightweight approach which are suitable for different scenarios. Given a target method, StructCodeSum predicts its function description, return description, parameter description, and usage description through hybrid code context, and ultimately builds a Javadoc-style code summary. The hybrid code context consists of path context, class context, documentation context and call context of the target method. Extensive experimental results demonstrate: 1) The hybrid context covers more than 70% of the summary tokens in average and significantly boosts the model performance; 2) When generating function descriptions, StructCodeSum outperforms the state-of-the-art approaches by a large margin; 3) According to human evaluation, the quality of the structured summaries generated by our approach is better than the documentation generated by Code Llama. },
keywords={Codes;Hybrid power systems;Documentation;Task analysis;Annotations;Source coding;Context modeling},
doi={10.1109/TSE.2024.3439562},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3439562},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=oct}

@ARTICLE{10636098,
author={Khatoonabadi, SayedHassan and Abdellatif, Ahmad and Costa, Diego Elias and Shihab, Emad},
journal={ IEEE Transactions on Software Engineering },
title={{ Predicting the First Response Latency of Maintainers and Contributors in Pull Requests }},
year={2024},
volume={50},
number={10},
ISSN={1939-3520},
pages={2529-2543},
abstract={ The success of a Pull Request (PR) depends on the responsiveness of the maintainers and the contributor during the review process. Being aware of the expected waiting times can lead to better interactions and managed expectations for both the maintainers and the contributor. In this paper, we propose a machine-learning approach to predict the first response latency of the maintainers following the submission of a PR, and the first response latency of the contributor after receiving the first response from the maintainers. We curate a dataset of 20 large and popular open-source projects on GitHub and extract 21 features to characterize projects, contributors, PRs, and review processes. Using these features, we then evaluate seven types of classifiers to identify the best-performing models. We also conduct permutation feature importance and SHAP analyses to understand the importance and the impact of different features on the predicted response latencies. We find that our CatBoost models are the most effective for predicting the first response latencies of both maintainers and contributors. Compared to a dummy classifier that always returns the majority class, these models achieved an average improvement of 29% in AUC-ROC and 51% in AUC-PR for maintainers, as well as 39% in AUC-ROC and 89% in AUC-PR for contributors across the studied projects. The results indicate that our models can aptly predict the first response latencies using the selected features. We also observe that PRs submitted earlier in the week, containing an average number of commits, and with concise descriptions are more likely to receive faster first responses from the maintainers. Similarly, PRs with a lower first response latency from maintainers, that received the first response of maintainers earlier in the week, and containing an average number of commits tend to receive faster first responses from the contributors. Additionally, contributors with a higher acceptance rate and a history of timely responses in the project are likely to both obtain and provide faster first responses. Moreover, we show the effectiveness of our approach in a cross-project setting. Finally, we discuss key guidelines for maintainers, contributors, and researchers to help facilitate the PR review process. },
keywords={Reviews;Predictive models;Feature extraction;Software development management;History;Time measurement;Machine learning},
doi={10.1109/TSE.2024.3443741},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3443741},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=oct}

@ARTICLE{10638820,
author={Caldas, Ricardo and Garcia, Juan Antonio Pinera and Schiopu, Matei and Pelliccione, Patrizio and Rodrigues, Genaina and Berger, Thorsten},
journal={ IEEE Transactions on Software Engineering },
title={{ Runtime Verification and Field-Based Testing for ROS-Based Robotic Systems }},
year={2024},
volume={50},
number={10},
ISSN={1939-3520},
pages={2544-2567},
abstract={ Robotic systems are becoming pervasive and adopted in increasingly many domains, such as manufacturing, healthcare, and space exploration. To this end, engineering software has emerged as a crucial discipline for building maintainable and reusable robotic systems. The field of robotics software engineering research has received increasing attention, fostering autonomy as a fundamental goal. However, robotics developers are still challenged trying to achieve this goal given that simulation is not able to deliver solutions to realistically emulate real-world phenomena. Robots also need to operate in unpredictable and uncontrollable environments, which require safe and trustworthy self-adaptation capabilities implemented in software. Typical techniques to address the challenges are runtime verification, field-based testing, and mitigation techniques that enable fail-safe solutions. However, there is no clear guidance to architect ROS-based systems to enable and facilitate runtime verification and field-based testing. This paper aims to fill in this gap by providing guidelines that can help developers and quality assurance (QA) teams when developing, verifying or testing their robots in the field. These guidelines are carefully tailored to address the challenges and requirements of testing robotics systems in real-world scenarios. We conducted (i) a literature review on studies addressing runtime verification and field-based testing for robotic systems, (ii) mined ROS-based applications repositories, and (iii) validated the applicability, clarity, and usefulness via two questionnaires with 55 answers overall. We contribute 20 guidelines: 8 for developers and 12 for QA teams formulated for researchers and practitioners in robotic software engineering. Finally, we map our guidelines to open challenges thus far in runtime verification and field-based testing for ROS-based systems and, we outline promising research directions in the field. Guidelines website and replication package: https://ros-rvft.github.io. },
keywords={Robots;Testing;Runtime;Guidelines;Software;Quality assurance;Robot kinematics},
doi={10.1109/TSE.2024.3444697},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3444697},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=oct}

@ARTICLE{10643035,
author={Tian, Xiangbo and Ying, Shi and Li, Tiangang and Yuan, Mengting and Wang, Ruijin and Zhao, Yishi and Shang, Jianga},
journal={ IEEE Transactions on Software Engineering },
title={{ iTCRL: Causal-Intervention-Based Trace Contrastive Representation Learning for Microservice Systems }},
year={2024},
volume={50},
number={10},
ISSN={1939-3520},
pages={2583-2601},
abstract={ Nowadays, microservice architecture has become mainstream way of cloud applications delivery. Distributed tracing is crucial to preserve the observability of microservice systems. However, existing trace representation approaches only concentrate on operations, relationships and metrics related to service invocations. They ignore service events that denotes meaningful, singular point in time during the service's duration. In this paper, we propose iTCRL, a novel trace contrastive representation learning approach based on causal intervention. This approach first constructs a unified graph representation for each trace to describe the runtime status of service events in traces and the complex relationships between them. Then, Causal-intervention-based Trace Contrastive Learning is proposed, which learns trace representations from causal perspective based on the unified graph representations of traces. It uses causal intervention to generate contrastive views, heterogeneous graph neural network-based trace encoder to learn trace representations, and direct causal effect to guide the training of trace encoder. Experimental results on three datasets show that iTCRL outperforms all baselines in terms of trace classification, trace anomaly detection, trace sampling and noise robustness, and also validate the contribution of Causal-intervention-based Trace Contrastive Learning. },
keywords={Vectors;Measurement;Microservice architectures;Time factors;Runtime;Representation learning;Contrastive learning},
doi={10.1109/TSE.2024.3446532},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3446532},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=oct}

@ARTICLE{10643775,
author={Nitin, Vikram and Mulhern, Anne and Arora, Sanjay and Ray, Baishakhi},
journal={ IEEE Transactions on Software Engineering },
title={{ Yuga: Automatically Detecting Lifetime Annotation Bugs in the Rust Language }},
year={2024},
volume={50},
number={10},
ISSN={1939-3520},
pages={2602-2613},
abstract={ The Rust programming language is becoming increasingly popular among systems programmers due to its efficient performance and robust memory safety guarantees. Rust employs an ownership model to ensure these guarantees by allowing each value to be owned by only one identifier at a time. It uses the concept of borrowing and lifetimes to enable other variables to temporarily borrow values. Despite its benefits, security vulnerabilities have been reported in Rust projects, often attributed to the use of “unsafe” Rust code. These vulnerabilities, in part, arise from incorrect lifetime annotations on function signatures. However, existing tools fail to detect these bugs, primarily because such bugs are rare, challenging to detect through dynamic analysis, and require explicit memory models. To overcome these limitations, we characterize incorrect lifetime annotations as a source of memory safety bugs and leverage this understanding to devise a novel static analysis tool, Yuga, to detect potential lifetime annotation bugs. Yuga uses a multi-phase analysis approach, starting with a quick pattern-matching algorithm to identify potential buggy components and then conducting a flow and field-sensitive alias analysis to confirm the bugs. We also curate new datasets of lifetime annotation bugs. Yuga successfully detects bugs with good precision on these datasets, and we make the code and datasets publicly available. },
keywords={Computer bugs;Safety;Annotations;Codes;Security;Static analysis;Memory management},
doi={10.1109/TSE.2024.3447671},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3447671},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=oct}

@ARTICLE{10645745,
author={Paltenghi, Matteo and Pandita, Rahul and Henley, Austin Z. and Ziegler, Albert},
journal={ IEEE Transactions on Software Engineering },
title={{ Follow-Up Attention: An Empirical Study of Developer and Neural Model Code Exploration }},
year={2024},
volume={50},
number={10},
ISSN={1939-3520},
pages={2568-2582},
abstract={ Recent neural models of code, such as OpenAI Codex and AlphaCode, have demonstrated remarkable proficiency at code generation due to the underlying attention mechanism. However, it often remains unclear how the models actually process code, and to what extent their reasoning and the way their attention mechanism scans the code matches the patterns of developers. A poor understanding of the model reasoning process limits the way in which current neural models are leveraged today, so far mostly for their raw prediction. To fill this gap, this work studies how the processed attention signal of three open large language models - CodeGen, InCoder and GPT-J - agrees with how developers look at and explore code when each answers the same sensemaking questions about code. Furthermore, we contribute an open-source eye-tracking dataset comprising 92 manually-labeled sessions from 25 developers engaged in sensemaking tasks. We empirically evaluate five heuristics that do not use the attention and ten attention-based post-processing approaches of the attention signal of CodeGen against our ground truth of developers exploring code, including the novel concept of follow-up attention which exhibits the highest agreement between model and human attention. Our follow-up attention method can predict the next line a developer will look at with 47% accuracy. This outperforms the baseline prediction accuracy of 42.3%, which uses the session history of other developers to recommend the next line. These results demonstrate the potential of leveraging the attention signal of pre-trained models for effective code exploration. },
keywords={Codes;Task analysis;Gaze tracking;Transformers;Cognition;Attention mechanisms;Visualization},
doi={10.1109/TSE.2024.3445338},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3445338},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=oct}

@ARTICLE{10645815,
author={Lu, Chengjie and Ali, Shaukat and Yue, Tao},
journal={ IEEE Transactions on Software Engineering },
title={{ EpiTESTER: Testing Autonomous Vehicles With Epigenetic Algorithm and Attention Mechanism }},
year={2024},
volume={50},
number={10},
ISSN={1939-3520},
pages={2614-2632},
abstract={ Testing autonomous vehicles (AVs) under various environmental scenarios that lead the vehicles to unsafe situations is challenging. Given the infinite possible environmental scenarios, it is essential to find critical scenarios efficiently. To this end, we propose a novel testing method, named EpiTESTER, by taking inspiration from epigenetics, which enables species to adapt to sudden environmental changes. In particular, EpiTESTER adopts gene silencing as its epigenetic mechanism, which regulates gene expression to prevent the expression of a certain gene, and the probability of gene expression is dynamically computed as the environment changes. Given different data modalities (e.g., images, lidar point clouds) in the context of AV, EpiTESTER benefits from a multi-model fusion transformer to extract high-level feature representations from environmental factors. Next, it calculates probabilities based on these features with the attention mechanism. To assess the cost-effectiveness of EpiTESTER, we compare it with a probabilistic search algorithm (Simulated Annealing, SA), a classical genetic algorithm (GA) (i.e., without any epigenetic mechanism implemented), and EpiTESTER with equal probability for each gene. We evaluate EpiTESTER with six initial environments from CARLA, an open-source simulator for autonomous driving research, and two end-to-end AV controllers, Interfuser and TCP. Our results show that EpiTESTER achieved a promising performance in identifying critical scenarios compared to the baselines, showing that applying epigenetic mechanisms is a good option for solving practical problems. },
keywords={Epigenetics;Testing;Genetic algorithms;Attention mechanisms;Transformers;Feature extraction;Pedestrians},
doi={10.1109/TSE.2024.3449429},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3449429},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=oct}

@ARTICLE{10648982,
author={Sun, Weifeng and Guo, Zhenting and Yan, Meng and Liu, Zhongxin and Lei, Yan and Zhang, Hongyu},
journal={ IEEE Transactions on Software Engineering },
title={{ Method-Level Test-to-Code Traceability Link Construction by Semantic Correlation Learning }},
year={2024},
volume={50},
number={10},
ISSN={1939-3520},
pages={2656-2676},
abstract={ Test-to-code traceability links (TCTLs) establish links between test artifacts and code artifacts. These links enable developers and testers to quickly identify the specific pieces of code tested by particular test cases, thus facilitating more efficient debugging, regression testing, and maintenance activities. Various approaches, based on distinct concepts, have been proposed to establish method-level TCTLs, specifically linking unit tests to corresponding focal methods. Static methods, such as naming-convention-based methods, use heuristic- and similarity-based strategies. However, such methods face the following challenges: 1. Developers, driven by specific scenarios and development requirements, may deviate from naming conventions, leading to TCTL identification failures. 2. Static methods often overlook the rich semantics embedded within tests, leading to erroneous associations between tests and semantically unrelated code fragments. Although dynamic methods achieve promising results, they require the project to be compilable and the tests to be executable, limiting their usability. This limitation is significant for downstream tasks requiring massive test-code pairs, as not all projects can meet these requirements. To tackle the abovementioned limitations, we propose a novel static method-level TCTL approach, named TestLinker. For the first challenge of existing static approaches, TestLinker introduces a two-phase TCTL framework to accommodate different project types in a triage manner. As for the second challenge, we employ the semantic correlation learning, which learns and establishes the semantic correlations between tests and focal methods based on Pre-trained Code Models (PCMs). TestLinker further establishes mapping rules to accurately link the recommended function name to the concrete production function declaration. Empirical evaluation on a meticulously labeled dataset reveals that TestLinker significantly outperforms traditional static techniques, showing average F1-score improvements ranging from 73.48% to 202.00%. Moreover, compared to state-of-the-art dynamic methods, TestLinker, which only leverages static information, demonstrates comparable or even better performance, with an average F1-score increase of 37.40%. },
keywords={Codes;Task analysis;Phase change materials;Semantics;Software;Production;Correlation},
doi={10.1109/TSE.2024.3449917},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3449917},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=oct}

@ARTICLE{10654326,
author={Liu, Jingwen and Jin, Wuxia and Zhou, Junhui and Feng, Qiong and Fan, Ming and Wang, Haijun and Liu, Ting},
journal={ IEEE Transactions on Software Engineering },
title={{ 3Erefactor: Effective, Efficient and Executable Refactoring Recommendation for Software Architectural Consistency }},
year={2024},
volume={50},
number={10},
ISSN={1939-3520},
pages={2633-2655},
abstract={ As software continues to evolve and business functions become increasingly complex, architectural inconsistency arises when the implementation architecture deviates from the expected architecture design. This architectural problem makes maintenance difficult and requires significant effort to refactor. To assist labor-intensive refactoring, automated refactoring has received much attention such as searching for optimal refactoring solutions. However, there are still three limitations: The recommended refactorings are insufficiently effective in addressing architectural consistency; the search process for refactoring solution is inefficient; and there is a lack of executable refactoring solutions. To address these limitations, we propose an effective, efficient, and executable refactoring recommendation approach namely the 3Erefactor for software architectural consistency. To achieve effective refactoring, 3Erefactor uses NSGA-II to generate refactoring solutions that minimize architectural inconsistencies at module level and entity level. To achieve efficient refactoring, 3Erefactor leverages architecture recovery technique to locate files requiring refactoring, helping accelerate the convergence of refactoring algorithm. To achieve executable refactoring, 3Erefactor designs a set of refactoring executability constraint strategies during the refactoring solution search and generation, including improving refactoring pre-conditions and removing invalid operations in refactoring solutions. We evaluated our approach on six open source systems. Statistical analysis of our experiments shows that, the refactoring solution generated by 3Erefactor performed significantly better than 3 state-of-the-art approaches in terms of reducing the number of architectural inconsistencies, improving the efficiency of the refactoring algorithm and improving the executability of refactorings. },
keywords={Computer architecture;Codes;Software;Convergence;Software algorithms;Source coding;Software systems},
doi={10.1109/TSE.2024.3449564},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3449564},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=oct}

@ARTICLE{10659717,
author={Chakraborty, Partha and Alfadel, Mahmoud and Nagappan, Meiyappan},
journal={ IEEE Transactions on Software Engineering },
title={{ RLocator: Reinforcement Learning for Bug Localization }},
year={2024},
volume={50},
number={10},
ISSN={1939-3520},
pages={2695-2708},
abstract={ Software developers spend a significant portion of time fixing bugs in their projects. To streamline this process, bug localization approaches have been proposed to identify the source code files that are likely responsible for a particular bug. Prior work proposed several similarity-based machine-learning techniques for bug localization. Despite significant advances in these techniques, they do not directly optimize the evaluation measures. We argue that directly optimizing evaluation measures can positively contribute to the performance of bug localization approaches. Therefore, in this paper, we utilize Reinforcement Learning (RL) techniques to directly optimize the ranking metrics. We propose RLocator, a Reinforcement Learning-based bug localization approach. We formulate RLocator using a Markov Decision Process (MDP) to optimize the evaluation measures directly. We present the technique and experimentally evaluate it based on a benchmark dataset of 8,316 bug reports from six highly popular Apache projects. The results of our evaluation reveal that RLocator achieves a Mean Reciprocal Rank (MRR) of 0.62, a Mean Average Precision (MAP) of 0.59, and a Top 1 score of 0.46. We compare RLocator with three state-of-the-art bug localization tools, FLIM, BugLocator, and BL-GAN. Our evaluation reveals that RLocator outperforms both approaches by a substantial margin, with improvements of 38.3% in MAP, 36.73% in MRR, and 23.68% in the Top K metric. These findings highlight that directly optimizing evaluation measures considerably contributes to performance improvement of the bug localization problem. },
keywords={Computer bugs;Source coding;Location awareness;Measurement;Feature extraction;Reinforcement learning;Software},
doi={10.1109/TSE.2024.3452595},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3452595},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=oct}

@ARTICLE{10664637,
author={Kang, Sungmin and Yoon, Juyeon and Askarbekkyzy, Nargiz and Yoo, Shin},
journal={ IEEE Transactions on Software Engineering },
title={{ Evaluating Diverse Large Language Models for Automatic and General Bug Reproduction }},
year={2024},
volume={50},
number={10},
ISSN={1939-3520},
pages={2677-2694},
abstract={ Bug reproduction is a critical developer activity that is also challenging to automate, as bug reports are often in natural language and thus can be difficult to transform to test cases consistently. As a result, existing techniques mostly focused on crash bugs, which are easier to automatically detect and verify. In this work, we overcome this limitation by using large language models (LLMs), which have been demonstrated to be adept at natural language processing and code generation. By prompting LLMs to generate bug-reproducing tests, and via a post-processing pipeline to automatically identify promising generated tests, our proposed technique Libro could successfully reproduce about one-third of all bugs in the widely used Defects4J benchmark. Furthermore, our extensive evaluation on 15 LLMs, including 11 open-source LLMs, suggests that open-source LLMs also demonstrate substantial potential, with the StarCoder LLM achieving 70% of the reproduction performance of the closed-source OpenAI LLM code-davinci-002 on the large Defects4J benchmark, and 90% of performance on a held-out bug dataset likely not part of any LLM's training data. In addition, our experiments on LLMs of different sizes show that bug reproduction using Libro improves as LLM size increases, providing information as to which LLMs can be used with the Libro pipeline. },
keywords={Computer bugs;Codes;Pipelines;Large language models;Debugging;Java;Computational modeling},
doi={10.1109/TSE.2024.3450837},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3450837},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=oct}

@ARTICLE{10510659,
author={Zhang, Tanghaoran and Lu, Yao and Yu, Yue and Mao, Xinjun and Zhang, Yang and Zhao, Yuxin},
journal={ IEEE Transactions on Software Engineering },
title={{ How Do Developers Adapt Code Snippets to Their Contexts? An Empirical Study of Context-Based Code Snippet Adaptations }},
year={2024},
volume={50},
number={11},
ISSN={1939-3520},
pages={2712-2731},
abstract={ Reusing code snippets from online programming Q&A communities has become a common development practice, in which developers often need to adapt code snippets to their code contexts to satisfy their own programming needs. However, how developers make these code adaptations based on contexts is still unclear. To bridge this gap, we first conduct a semi-structured interview of 21 developers to investigate their adaptation practices and perceived challenges during this process. The result suggests that code snippet adaptation is a challenging and exhausting task for developers, as they should tailor the snippets to guarantee their correctness and quality with laborious work. We also note that developers all resort to their intra-file context to complete adaptations, which motivates us to further study how developers performed context-based adaptations (CAs) in real scenarios. To this end, we conduct a quantitative study on an adaptation dataset comprising 300 code snippet reuse cases with 1,384 adaptations from Stack Overflow to GitHub. For each adaptation, we manually annotate its intention and relationship with the context. Based on our annotated data, we employ frequent itemset mining to obtain four CA patterns from our dataset, including Fortification, Code Wiring, Attribute-ization and Parameterization. Our main findings reveal that: (1) more than half of the code snippet reuse cases include CAs and 23.3% of the adaptations are CAs; (2) more than half of the CAs are corrective adaptations and variable is the primary adapted language construct; (3) attribute is the most frequently utilized context and 88% of the local contexts are within the nearest 10 LOCs; and (4) CAs towards different intentions are repetitive, which are useful for automatic adaptation. Overall, our study provides valuable insights into code snippet adaptation and has important implications for research, practice, and tool design. },
keywords={Codes;Interviews;Programming;Task analysis;Software development management;Protocols;Wiring},
doi={10.1109/TSE.2024.3395519},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3395519},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=nov}

@ARTICLE{10538301,
author={Wang, Haoye and Gao, Zhipeng and Hu, Xing and Lo, David and Grundy, John and Wang, Xinyu},
journal={ IEEE Transactions on Software Engineering },
title={{ Just-In-Time TODO-Missed Commits Detection }},
year={2024},
volume={50},
number={11},
ISSN={1939-3520},
pages={2732-2752},
abstract={ TODO comments play an important role in helping developers to manage their tasks and communicate with other team members. TODO comments are often introduced by developers as a type of technical debt, such as a reminder to add/remove features or a request to optimize the code implementations. These can all be considered as notifications for developers to revisit regarding the current suboptimal solutions. TODO comments often bring short-term benefits – higher productivity or shorter development cost – and indicate attention needs to be paid for the long-term software quality. Unfortunately, due to their lack of knowledge or experience and/or the time constraints, developers sometimes may forget or even not be aware of suboptimal implementations. The loss of the TODO comments for these suboptimal solutions may hurt the software quality and reliability in the long-term. Therefore it is beneficial to remind the developers of the suboptimal solutions whenever they change the code. In this work, we refer this problem to the task of detecting TODO-missed commits, and we propose a novel approach named TDReminder (TODO comment Reminder) to address the task. With the help of TDReminder, developers can identify possible missing TODO commits just-in-time when submitting a commit. Our approach has two phases: offline training and online inference. We first embed code change and commit message into contextual vector representations using two neural encoders respectively. The association between these representations is learned by our model automatically. In the online inference phase, TDReminder leverages the trained model to compute the likelihood of a commit being a TODO-missed commit. We evaluate TDReminder on datasets crawled from 10k popular Python and Java repositories in GitHub respectively. Our experimental results show that TDReminder outperforms a set of benchmarks by a large margin in TODO-missed commits detection. Moreover, to better help developers use TDReminder in practice, we have incorporated Large Language Models (LLMs) with our approach to provide explainable recommendations. The user study shows that our tool can effectively inform developers not only “when” to add TODOs, but also “where” and “what” TODOs should be added, verifying the value of our tool in practical application. },
keywords={Codes;Task analysis;Training;Python;Stars;Software quality;Software development management},
doi={10.1109/TSE.2024.3405005},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3405005},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=nov}

@ARTICLE{10545607,
author={Mahmood, Wardah and Calikli, Gul and Struber, Daniel and Lammel, Ralf and Mukelabai, Mukelabai and Berger, Thorsten},
journal={ IEEE Transactions on Software Engineering },
title={{ Virtual Platform: Effective and Seamless Variability Management for Software Systems }},
year={2024},
volume={50},
number={11},
ISSN={1939-3520},
pages={2753-2785},
abstract={ Customization is a general trend in software engineering, demanding systems that support variable stakeholder requirements. Two opposing strategies are commonly used to create variants: software clone & own and software configuration with an integrated platform. Organizations often start with the former, which is cheap and agile, but does not scale. The latter scales by establishing an integrated platform that shares software assets between variants, but requires high up-front investments or risky migration processes. So, could we have a method that allows an easy transition or even combine the benefits of both strategies? We propose a method and tool that supports a truly incremental development of variant-rich systems, exploiting a spectrum between the opposing strategies. We design, formalize, and prototype a variability-management framework: the virtual platform. Virtual platform bridges clone & own and platform-oriented development. Relying on programming-language independent conceptual structures representing software assets, it offers operators for engineering and evolving a system, comprising: traditional, asset-oriented operators and novel, feature-oriented operators for incrementally adopting concepts of an integrated platform. The operators record meta-data that is exploited by other operators to support the transition. Among others, they eliminate expensive feature-location effort or the need to trace clones. A cost-and-benefit analysis of using the virtual platform to simulate the development of a real-world variant-rich system shows that it leads to benefits in terms of saved effort and time for clone detection and feature location. Furthermore, we present a user study indicating that the virtual platform effectively supports exploratory and hands-on tasks, outperforming manual development concerning correctness. We also observed that participants were significantly faster when performing typical variability management tasks using the virtual platform. Furthermore, participants perceived manual development to be significantly more difficult than using the virtual platform, preferring virtual platform for all our tasks. We supplement our findings with recommendations on when to use virtual platform and on incorporating the virtual platform in practice. },
keywords={Cloning;Task analysis;Organizations;Software;Manuals;Computer science;Prototypes},
doi={10.1109/TSE.2024.3406224},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3406224},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=nov}

@ARTICLE{10551691,
author={Jiang, Hanzhi and Shi, Lin and Che, Meiru and Zhang, Yuxia and Wang, Qing},
journal={ IEEE Transactions on Software Engineering },
title={{ Bringing Open Source Communication and Development Together: A Cross-Platform Study on Gitter and GitHub }},
year={2024},
volume={50},
number={11},
ISSN={1939-3520},
pages={2807-2826},
abstract={ Recently, a growing body of research has realized that live chat via modern communication platforms plays an increasingly important role in OSS (Open Source Software) collaborative development. Among these platforms, Gitter has emerged as a popular choice since it is directed toward GitHub projects by account sharing and activity subscribing. But little is known about how Gitter affects the OSS development on GitHub. Who are the developers being active in both social and technical platforms? How important are they? In this paper, we perform a comprehensive cross-platform study on Gitter and GitHub, two representative platforms for live communication and distributed development, to explore the characteristics of cross-platform contributors (CPCs) and whether live chat can provoke open source development. This study yields interesting findings: 1) Despite CPCs being small in quantity yet account for a much bigger amount of communication and development; 2) Gitter continually attracts new contributors; 3) Communication on Gitter has a positive impact on the contributions of OSS developers; and 4) Inactive developers on GitHub still participate in discussions on Gitter. Based on our findings, we provide recommendations for OSS communities and developers and shed light on future research directions. We believe that the findings and insights will inspire the OSS communities, enable a broader view of the interplay between Gitter and GitHub, and enhance the sustainability of the OSS ecosystem. },
keywords={Software development management;Codes;Software;Electronic mail;Collaboration;Social networking (online);Computer bugs},
doi={10.1109/TSE.2024.3410292},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3410292},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=nov}

@ARTICLE{10555543,
author={Jiang, Muhui and Zheng, Xiaoye and Chang, Rui and Zhou, Yajin and Luo, Xiapu},
journal={ IEEE Transactions on Software Engineering },
title={{ Examiner-Pro: Testing Arm Emulators Across Different Privileges }},
year={2024},
volume={50},
number={11},
ISSN={1939-3520},
pages={2786-2806},
abstract={ Emulators are commonly employed to construct dynamic analysis frameworks due to their ability to perform fine-grained tracing, monitor full system functionality, and run on diverse operating systems and architectures. Nonetheless, the consistency of emulators with the real devices, remains uncertain. To address this issue, our objective is to automatically identify inconsistent instructions that exhibit different behavior between emulators and real devices across distinct privileges, including user-level and system-level privilege. We target the Arm architecture, which provides machine-readable specifications. Based on the specification, we propose a sufficient test case generator by designing and implementing the first symbolic execution engine for the Arm architecture specification language (ASL). We generated 2,774,649 representative instruction streams and developed a differential testing engine, Examiner Pro. With this engine, we compared the behavior of real Arm devices across different instruction sets (A32, A64, T16, and T32) with the popular QEMU emulator, both at the user-level and system-level. To demonstrate the generalizability of Examiner Pro, we also tested two other emulators, namely Unicorn and Angr. We find that undefined implementation in Arm manual and bugs of emulators are the major causes of inconsistencies. Furthermore, we discover 17 bugs, which influence commonly used instructions (e.g., BLX). With the inconsistent instructions, we build three security applications and demonstrate the capability of these instructions on detecting emulators, anti-emulation, and anti-fuzzing. },
keywords={Streams;Testing;Encoding;Engines;Computer architecture;Generators;Computer bugs},
doi={10.1109/TSE.2024.3406900},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3406900},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=nov}

@ARTICLE{10606318,
author={Chen, Zimin and Fang, Sen and Monperrus, Martin},
journal={ IEEE Transactions on Software Engineering },
title={{ Supersonic: Learning to Generate Source Code Optimizations in C/C++ }},
year={2024},
volume={50},
number={11},
ISSN={1939-3520},
pages={2849-2864},
abstract={ Software optimization refines programs for resource efficiency while preserving functionality. Traditionally, it is a process done by developers and compilers. This paper introduces a third option, automated optimization at the source code level. We present Supersonic, a neural approach targeting minor source code modifications for optimization. Using a seq2seq model, Supersonic is trained on C/C++ program pairs ($x_{t}$xt, $x_{t+1}$xt+1), where $x_{t+1}$xt+1 is an optimized version of $x_{t}$xt, and outputs a diff. Supersonic's performance is benchmarked against OpenAI's GPT-3.5-Turbo and GPT-4 on competitive programming tasks. The experiments show that Supersonic not only outperforms both models on the code optimization task but also minimizes the extent of the change with a model more than 600x smaller than GPT-3.5-Turbo and 3700x smaller than GPT-4. },
keywords={Optimization;Codes;Training;Source coding;Task analysis;Decoding;Vectors},
doi={10.1109/TSE.2024.3423769},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3423769},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=nov}

@ARTICLE{10659742,
author={Zhou, Xin and Xu, Bowen and Kim, Kisub and Han, DongGyun and Nguyen, Hung Huu and Le-Cong, Thanh and He, Junda and Le, Bach and Lo, David},
journal={ IEEE Transactions on Software Engineering },
title={{ Leveraging Large Language Model for Automatic Patch Correctness Assessment }},
year={2024},
volume={50},
number={11},
ISSN={1939-3520},
pages={2865-2883},
abstract={ Automated Program Repair (APR) techniques have shown more and more promising results in fixing real-world bugs. Despite the effectiveness, APR techniques still face an overfitting problem: a generated patch can be incorrect although it passes all tests. It is time-consuming to manually evaluate the correctness of generated patches that can pass all available test cases. To address this problem, many approaches have been proposed to automatically assess the correctness of patches generated by APR techniques. These approaches are mainly evaluated within the cross-validation setting. However, for patches generated by a new or unseen APR tool, users are implicitly required to manually label a significant portion of these patches (e.g., 90% in 10-fold cross-validation) in the cross-validation setting before inferring the remaining patches (e.g., 10% in 10-fold cross-validation). To mitigate the issue, in this study, we propose LLM4PatchCorrect, the patch correctness assessment by adopting a large language model for code. Specifically, for patches generated by a new or unseen APR tool, LLM4PatchCorrect does not need labeled patches of this new or unseen APR tool for training but directly queries the large language model for code to get predictions on the correctness labels without training. In this way, LLM4PatchCorrect can reduce the manual labeling effort when building a model to automatically assess the correctness of generated patches of new APR tools. To provide knowledge regarding the automatic patch correctness assessment (APCA) task to the large language model for code, LLM4PatchCorrect leverages bug descriptions, execution traces, failing test cases, test coverage, and labeled patches generated by existing APR tools, before deciding the correctness of the unlabeled patches of a new or unseen APR tool. Additionally, LLM4PatchCorrect prioritizes labeled patches from existing APR tools that exhibit semantic similarity to those generated by new APR tools, enhancing the accuracy achieved by LLM4PatchCorrect for patches from new APR tools. Our experimental results showed that LLM4PatchCorrect can achieve an accuracy of 84.4% and an F1-score of 86.5% on average although no labeled patch of the new or unseen APR tool is available. In addition, our proposed technique significantly outperformed the prior state-of-the-art. },
keywords={Computer bugs;Codes;Task analysis;Large language models;Feature extraction;Training;Manuals},
doi={10.1109/TSE.2024.3452252},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3452252},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=nov}

@ARTICLE{10666791,
author={Zhang, Fangyuan and Fan, Lingling and Chen, Sen and Cai, Miaoying and Xu, Sihan and Zhao, Lida},
journal={ IEEE Transactions on Software Engineering },
title={{ Does the Vulnerability Threaten Our Projects? Automated Vulnerable API Detection for Third-Party Libraries }},
year={2024},
volume={50},
number={11},
ISSN={1939-3520},
pages={2906-2920},
abstract={ Developers usually use third-party libraries (TPLs) to facilitate the development of their projects to avoid reinventing the wheels, however, the vulnerable TPLs indeed cause severe security threats. The majority of existing research only considered whether projects used vulnerable TPLs but neglected whether the vulnerable code of the TPLs was indeed used by the projects, which inevitably results in false positives and further requires additional patching efforts and maintenance costs (e.g., dependency conflict issues after version upgrades). To mitigate such a problem, we propose VAScanner, which can effectively identify vulnerable root methods causing vulnerabilities in TPLs and further identify all vulnerable APIs of TPLs used by Java projects. Specifically, we first collect the initial patch methods from the patch commits and extract accurate patch methods by employing a patch-unrelated sifting mechanism, then we further identify the vulnerable root methods for each vulnerability by employing an augmentation mechanism. Based on them, we leverage backward call graph analysis to identify all vulnerable APIs for each vulnerable TPL version and construct a database consisting of 90,749 (2,410,779 with library versions) vulnerable APIswith 1.45% false positive proportion with a 95% confidence interval (CI) of [1.31%, 1.59%] from 362 TPLs with 14,775 versions. The database serves as a reference database to help developers detect vulnerable APIs of TPLs used by projects. Our experiments show VAScanner eliminates 5.78% false positives and 2.16% false negatives owing to the proposed sifting and augmentation mechanisms. Besides, it outperforms the state-of-the-art method-level vulnerability detection tool in analyzing direct dependencies, Eclipse Steady, achieving more effective detection of vulnerable APIs. Furthermore, to investigate the real impact of vulnerabilities on real open-source projects, we exploit VAScanner to conduct a large-scale analysis on 3,147 projects that depend on vulnerable TPLs, and find only 21.51% of projects (with 1.83% false positive proportion and a 95% CI of [0.71%, 4.61%]) were threatened through vulnerable APIs, demonstrating that VAScanner can potentially reduce false positives significantly. },
keywords={Libraries;Databases;Codes;Software;Accuracy;Java;Security},
doi={10.1109/TSE.2024.3454960},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3454960},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=nov}

@ARTICLE{10666908,
author={Chen, Xin and Sun, Tian and Zhuang, Dongling and Yu, Dongjin and Jiang, He and Zhou, Zhide and Li, Sicheng},
journal={ IEEE Transactions on Software Engineering },
title={{ HetFL: Heterogeneous Graph-Based Software Fault Localization }},
year={2024},
volume={50},
number={11},
ISSN={1939-3520},
pages={2884-2905},
abstract={ Automated software fault localization has become one of the hot spots on which researchers have focused in recent years. Existing studies have shown that learning-based techniques can effectively localize faults leveraging various information. However, there exist two problems in these techniques. The first is that they simply represent various information without caring the contribution of different information. The second is that the data imbalance problem is not considered in these techniques. Thus, their effectiveness is limited in practice. In this paper, we propose HetFL, a novel heterogeneous graph-based software fault localization technique to aggregate different information into a heterogeneous graph in which program entities and test cases are regarded as nodes, and coverage, change histories, and call relationships are viewed as edges. HetFL first extracts textual and structure information from source code as attributes of nodes and integrates them to form an attribute vector. Then, for a given node, HetFL finds its neighbor nodes based on the types of edges and aggregates corresponding neighbor nodes to form type vectors. After that, the attribute vector and all the type vectors of each node are aggregated to generate the final vector representation by an attention mechanism. Finally, we leverage a convolution neural network (CNN) to obtain the suspicious score of each method. To validate the effectiveness of HetFL, experiments are conducted on the widely used dataset Defects4J (v1.2.0). The experimental results show that HetFL can localize 217 faults within Top-1 that is 25 higher than the state-of-the-art technique DeepFL, and achieve 6.37 and 5.58 in terms of MAR and MFR which improve DeepFL by 9.0% and 5.6%, respectively. In addition, we also perform experiments on the latest version of Defects4J (v2.0.0). The experimental results show that HetFL has better performance than the baseline methods. },
keywords={Location awareness;Software;History;Vectors;Computer science;Codes;Aggregates},
doi={10.1109/TSE.2024.3454605},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3454605},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=nov}

@ARTICLE{10677447,
author={Tiwari, Deepika and Monperrus, Martin and Baudry, Benoit},
journal={ IEEE Transactions on Software Engineering },
title={{ Mimicking Production Behavior With Generated Mocks }},
year={2024},
volume={50},
number={11},
ISSN={1939-3520},
pages={2921-2946},
abstract={ Mocking allows testing program units in isolation. A developer who writes tests with mocks faces two challenges: design realistic interactions between a unit and its environment; and understand the expected impact of these interactions on the behavior of the unit. In this paper, we propose to monitor an application in production to generate tests that mimic realistic execution scenarios through mocks. Our approach operates in three phases. First, we instrument a set of target methods for which we want to generate tests, as well as the methods that they invoke, which we refer to as mockable method calls. Second, in production, we collect data about the context in which target methods are invoked, as well as the parameters and the returned value for each mockable method call. Third, offline, we analyze the production data to generate test cases with realistic inputs and mock interactions. The approach is automated and implemented in an open-source tool called rick. We evaluate our approach with three real-world, open-source Java applications. rick monitors the invocation of $128$128 methods in production across the three applications and captures their behavior. Based on this captured data, rick generates test cases that include realistic initial states and test inputs, as well as mocks and stubs. All the generated test cases are executable, and $52.4\%$52.4% of them successfully mimic the complete execution context of the target methods observed in production. The mock-based oracles are also effective at detecting regressions within the target methods, complementing each other in their fault-finding ability. We interview $5$5 developers from the industry who confirm the relevance of using production observations to design mocks and stubs. Our experimental findings clearly demonstrate the feasibility and added value of generating mocks from production interactions. },
keywords={Production;Testing;Monitoring;Java;Test pattern generators;Libraries;Interviews},
doi={10.1109/TSE.2024.3458448},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3458448},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=nov}

@ARTICLE{10682972,
author={Muqeet, Asmar and Yue, Tao and Ali, Shaukat and Arcaini, Paolo},
journal={ IEEE Transactions on Software Engineering },
title={{ Mitigating Noise in Quantum Software Testing Using Machine Learning }},
year={2024},
volume={50},
number={11},
ISSN={1939-3520},
pages={2947-2961},
abstract={ Quantum Computing (QC) promises computational speedup over classic computing. However, noise exists in near-term quantum computers. Quantum software testing (for gaining confidence in quantum software's correctness) is inevitably impacted by noise, i.e., it is impossible to know if a test case failed due to noise or real faults. Existing testing techniques test quantum programs without considering noise, i.e., by executing tests on ideal quantum computer simulators. Consequently, they are not directly applicable to test quantum software on real quantum computers or noisy simulators. Thus, we propose a noise-aware approach (named $\mathit{QOIN}$QOIN) to alleviate the noise effect on test results of quantum programs. $\mathit{QOIN}$QOIN employs machine learning techniques (e.g., transfer learning) to learn the noise effect of a quantum computer and filter it from a program's outputs. Such filtered outputs are then used as the input to perform test case assessments (determining the passing or failing of a test case execution against a test oracle). We evaluated $\mathit{QOIN}$QOIN on IBM's 23 noise models, Google's two available noise models, and Rigetti's Quantum Virtual Machine, with six real-world and 800 artificial programs. We also generated faulty versions of these programs to check if a failing test case execution can be determined under noise. Results show that $\mathit{QOIN}$QOIN can reduce the noise effect by more than $80\%$80% on most noise models. We used an existing test oracle to evaluate $\mathit{QOIN}$QOIN's effectiveness in quantum software testing. The results showed that $\mathit{QOIN}$QOIN attained scores of $99\%$99%, $75\%$75%, and $86\%$86% for precision, recall, and F1-score, respectively, for the test oracle across six real-world programs. For artificial programs, $\mathit{QOIN}$QOIN achieved scores of $93\%$93%, $79\%$79%, and $86\%$86% for precision, recall, and F1-score respectively. This highlights $\mathit{QOIN}$QOIN's effectiveness in learning noise patterns for noise-aware quantum software testing. },
keywords={Noise;Quantum computing;Qubit;Computers;Software testing;Logic gates;Computational modeling},
doi={10.1109/TSE.2024.3462974},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3462974},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=nov}

@ARTICLE{10684067,
author={Xie, Xiaoyuan and Li, Xingpeng and Chen, Songqiang},
journal={ IEEE Transactions on Software Engineering },
title={{ Metamorphic Testing of Image Captioning Systems via Image-Level Reduction }},
year={2024},
volume={50},
number={11},
ISSN={1939-3520},
pages={2962-2982},
abstract={ The Image Captioning (IC) technique is widely used to describe images in natural language. However, even state-of-the-art IC systems can still produce incorrect captions and lead to misunderstandings. Recently, some IC system testing methods have been proposed. However, these methods still rely on pre-annotated information and hence cannot really alleviate the difficulty in identifying the test oracle. Furthermore, their methods artificially manipulate objects, which may generate unreal images as test cases and thus lead to less meaningful testing results. Thirdly, existing methods have various requirements on the eligibility of source test cases, and hence cannot fully utilize the given images to perform testing. To tackle these issues, in this paper, we propose ReIC to perform metamorphic testing for the IC systems with some image-level reduction transformations like image cropping and stretching. Instead of relying on the pre-annotated information, ReIC uses a localization method to align objects in the caption with corresponding objects in the image, and checks whether each object is correctly described or deleted in the caption after transformation. With the image-level reduction transformations, ReIC does not artificially manipulate any objects and hence can avoid generating unreal follow-up images. Additionally, it eliminates the requirement on the eligibility of source test cases during the metamorphic transformation process, as well as decreases the ambiguity and boosts the diversity among the follow-up test cases, which consequently enables testing to be performed on any test image and reveals more distinct valid violations. We employ ReIC to test five popular IC systems. The results demonstrate that ReIC can sufficiently leverage the provided test images to generate follow-up cases of good realism, and effectively detect a great number of distinct violations, without the need for any pre-annotated information. },
keywords={Testing;Visualization;Integrated circuit modeling;Annotations;System testing;Object recognition;Natural languages},
doi={10.1109/TSE.2024.3463747},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3463747},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=nov}

@ARTICLE{10684841,
author={Zhang, Pengcheng and Wang, Ben and Luo, Xiapu and Dong, Hai},
journal={ IEEE Transactions on Software Engineering },
title={{ SCAnoGenerator: Automatic Anomaly Injection for Ethereum Smart Contracts }},
year={2024},
volume={50},
number={11},
ISSN={1939-3520},
pages={2983-3006},
abstract={ Although many tools have been developed to detect anomalies in smart contracts, the evaluation of these analysis tools has been hindered by the lack of adequate anomalistic real-world contracts (i.e., smart contracts with addresses on Ethereum to achieve certain purposes). This problem prevents conducting reliable performance assessments on the analysis tools. An effective way to solve this problem is to inject anomalies into real-world contracts and automatically label the locations and types of the injected anomalies. SolidiFI, as the first and only tool in this area, was developed to automatically inject anomalies into Ethereum smart contracts. However, SolidiFI is subject to the limitations from its methodologies (e.g., its injection accuracy and authenticity are low). To address these limitations, we propose an approach called SCAnoGenerator. SCAnoGenerator supports Solidity 0.5.x, 0.6.x, 0.7.x and enables automatic anomaly injection for Ethereum smart contracts via analyzing the contracts’ control and data flows. Based on this approach, we develop an open-source tool, which can inject 20 types of anomalies into smart contracts. The extensive experiments show that SCAnoGenerator outperforms SolidiFI on the number of injected anomaly types, injection accuracy, and injection authenticity. The experimental results also reveal that existing analysis tools can only partially detect the anomalies injected by SCAnoGenerator. },
keywords={Smart contracts;Cryptocurrency;Blockchains;Open source software;Codes;Accuracy;Generators},
doi={10.1109/TSE.2024.3464539},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3464539},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=nov}

@ARTICLE{10684883,
author={Hu, Danniell and Santiesteban, Priscila and Endres, Madeline and Weimer, Westley},
journal={ IEEE Transactions on Software Engineering },
title={{ Towards a Cognitive Model of Dynamic Debugging: Does Identifier Construction Matter? }},
year={2024},
volume={50},
number={11},
ISSN={1939-3520},
pages={3007-3021},
abstract={ Debugging is a vital and time-consuming process in software engineering. Recently, researchers have begun using neuroimaging to understand the cognitive bases of programming tasks by measuring patterns of neural activity. While exciting, prior studies have only examined small sub-steps in isolation, such as comprehending a method without writing any code or writing a method from scratch without reading any already-existing code. We propose a simple multi-stage debugging model in which programmers transition between Task Comprehension, Fault Localization, Code Editing, Compiling, and Output Comprehension activities. We conduct a human study of $n=28$n=28 participants using a combination of functional near-infrared spectroscopy and standard coding measurements (e.g., time taken, tests passed, etc.). Critically, we find that our proposed debugging stages are both neurally and behaviorally distinct. To the best of our knowledge, this is the first neurally-justified cognitive model of debugging. At the same time, there is significant interest in understanding how programmers from different backgrounds, such as those grappling with challenges in English prose comprehension, are impacted by code features when debugging. We use our cognitive model of debugging to investigate the role of one such feature: identifier construction. Specifically, we investigate how features of identifier construction impact neural activity while debugging by participants with and without reading difficulties. While we find significant differences in cognitive load as a function of morphology and expertise, we do not find significant differences in end-to-end programming outcomes (e.g., time, correctness, etc.). This nuanced result suggests that prior findings on the cognitive importance of identifier naming in isolated sub-steps may not generalize to end-to-end debugging. Finally, in a result relevant to broadening participation in computing, we find no behavioral outcome differences for participants with reading difficulties. },
keywords={Debugging;Functional near-infrared spectroscopy;Codes;Programming;Biological system modeling;Neural activity;Brain modeling},
doi={10.1109/TSE.2024.3465222},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3465222},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=nov}

@ARTICLE{10689456,
author={Sun, Yi and Wang, Chengpeng and Fan, Gang and Shi, Qingkai and Zhang, Xiangyu},
journal={ IEEE Transactions on Software Engineering },
title={{ Fast and Precise Static Null Exception Analysis With Synergistic Preprocessing }},
year={2024},
volume={50},
number={11},
ISSN={1939-3520},
pages={3022-3036},
abstract={ Pointer operations are common in programs written in modern programming languages such as C/C++ and Java. While widely used, pointer operations often suffer from bugs like null pointer exceptions that make software systems vulnerable and unstable. However, precisely verifying the absence of null pointer exceptions is notoriously slow as we need to inspect a huge number of pointer-dereferencing operations one by one via expensive techniques like SMT solving. We observe that, among all pointer-dereferencing operations in a program, a large number can be proven to be safe by lightweight preprocessing. Thus, we can avoid employing costly techniques to verify their nullity. The impacts of lightweight preprocessing techniques are significantly less studied and ignored by recent works. In this paper, we propose a new technique, BONA, which leverages the synergistic effects of two classic preprocessing analyses. The synergistic effects between the two preprocessing analyses allow us to recognize a lot more safe pointer operations before a follow-up costly nullity verification, thus improving the scalability of the whole null exception analysis. We have implemented our synergistic preprocessing procedure in two state-of-the-art static analyzers, KLEE and Pinpoint. The evaluation results demonstrate that BONA itself is fast and can finish in a few seconds for programs that KLEE and Pinpoint may require several minutes or even hours to analyze. Compared to the vanilla versions of KLEE and Pinpoint, BONA respectively enables them to achieve up to 1.6x and 6.6x speedup (1.2x and 3.8x on average) with less than 0.5% overhead. Such a speedup is significant enough as it allows KLEE and Pinpoint to check more pointer-dereferencing operations in a given time budget and, thus, discover over a dozen previously unknown null pointer exceptions in open-source projects. },
keywords={Static analysis;Codes;Computer bugs;Electronic mail;Detectors;Sun;Sensitivity},
doi={10.1109/TSE.2024.3466551},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3466551},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=nov}

@ARTICLE{10697930,
author={Pan, Rongqi and Ghaleb, Taher A. and Briand, Lionel C.},
journal={ IEEE Transactions on Software Engineering },
title={{ LTM: Scalable and Black-Box Similarity-Based Test Suite Minimization Based on Language Models }},
year={2024},
volume={50},
number={11},
ISSN={1939-3520},
pages={3053-3070},
abstract={ Test suites tend to grow when software evolves, making it often infeasible to execute all test cases with the allocated testing budgets, especially for large software systems. Test suite minimization (TSM) is employed to improve the efficiency of software testing by removing redundant test cases, thus reducing testing time and resources while maintaining the fault detection capability of the test suite. Most existing TSM approaches rely on code coverage (white-box) or model-based features, which are not always available to test engineers. Recent TSM approaches that rely only on test code (black-box) have been proposed, such as ATM and FAST-R. The former yields higher fault detection rates (FDR) while the latter is faster. To address scalability while retaining a high FDR, we propose LTM (Language model-based Test suite Minimization), a novel, scalable, and black-box similarity-based TSM approach based on large language models (LLMs), which is the first application of LLMs in the context of TSM. To support similarity measurement using test method embeddings, we investigate five different pre-trained language models: CodeBERT, GraphCodeBERT, UniXcoder, StarEncoder, and CodeLlama, on which we compute two similarity measures: Cosine Similarity and Euclidean Distance. Our goal is to find similarity measures that are not only computationally more efficient but can also better guide a Genetic Algorithm (GA), which is used to search for optimal minimized test suites, thus reducing the overall search time. Experimental results show that the best configuration of LTM (UniXcoder/Cosine) outperforms ATM in three aspects: (a) achieving a slightly greater saving rate of testing time ($41.72\%$41.72% versus $41.02\%$41.02%, on average); (b) attaining a significantly higher fault detection rate ($0.84$0.84 versus $0.81$0.81, on average); and, most importantly, (c) minimizing test suites nearly five times faster on average, with higher gains for larger test suites and systems, thus achieving much higher scalability. },
keywords={Minimization;Codes;Fault detection;Closed box;Scalability;Time measurement;Genetic algorithms;Source coding;Vectors;Unified modeling language},
doi={10.1109/TSE.2024.3469582},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3469582},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=nov}

@ARTICLE{10706805,
author={Yin, Xin and Ni, Chao and Wang, Shaohua},
journal={ IEEE Transactions on Software Engineering },
title={{ Multitask-Based Evaluation of Open-Source LLM on Software Vulnerability }},
year={2024},
volume={50},
number={11},
ISSN={1939-3520},
pages={3071-3087},
abstract={ This paper proposes a pipeline for quantitatively evaluating interactive Large Language Models (LLMs) using publicly available datasets. We carry out an extensive technical evaluation of LLMs using Big-Vul covering four different common software vulnerability tasks. This evaluation assesses the multi-tasking capabilities of LLMs based on this dataset. We find that the existing state-of-the-art approaches and pre-trained Language Models (LMs) are generally superior to LLMs in software vulnerability detection. However, in software vulnerability assessment and location, certain LLMs (e.g., CodeLlama and WizardCoder) have demonstrated superior performance compared to pre-trained LMs, and providing more contextual information can enhance the vulnerability assessment capabilities of LLMs. Moreover, LLMs exhibit strong vulnerability description capabilities, but their tendency to produce excessive output significantly weakens their performance compared to pre-trained LMs. Overall, though LLMs perform well in some aspects, they still need improvement in understanding the subtle differences in code vulnerabilities and the ability to describe vulnerabilities to fully realize their potential. Our evaluation pipeline provides valuable insights into the capabilities of LLMs in handling software vulnerabilities. },
keywords={Software;Training;Biological system modeling;Codes;Software quality;Large language models;Source coding;Software systems;Software engineering;Nickel},
doi={10.1109/TSE.2024.3470333},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3470333},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=nov}

@ARTICLE{10711218,
author={Tang, Lingxiao and Ni, Chao and Huang, Qiao and Bao, Lingfeng},
journal={ IEEE Transactions on Software Engineering },
title={{ Enhancing Bug-Inducing Commit Identification: A Fine-Grained Semantic Analysis Approach }},
year={2024},
volume={50},
number={11},
ISSN={1939-3520},
pages={3037-3052},
abstract={ The SZZ algorithm and its variants have been extensively utilized for identifying bug-inducing commits based on bug-fixing commits. However, these algorithms face challenges when there are no deletion lines in the bug-fixing commit. Previous studies have attempted to address this issue by tracing back all lines in the block that encapsulates the added lines. However, this method is too coarse-grained and suffers from low precision. To address this issue, we propose a novel method in this paper called Sem-SZZ, which is based on fine-grained semantic analysis. Initially, we observe that a significant number of bug-inducing commits can be identified by tracing back the unmodified lines near added lines, resulting in improved precision and F1-score. Building on this observation, we conduct a more fine-grained semantic analysis. We begin by performing program slicing to extract the program part near the added lines. Subsequently, we compare the program's states between the previous version and the current version, focusing on data flow and control flow differences based on the extracted program part. Finally, we extract statements contributing to the bug based on these differences and utilize them to locate bug-inducing commits. We also extend our approach to fit the scenario where the bug-fixing commits contain deleted lines. Experimental results demonstrate that Sem-SZZ outperforms the state-of-the-art methods in identifying bug-inducing commits, regardless of whether the bug-fixing commit contains deleted lines. },
keywords={Computer bugs;Noise;Software algorithms;Semantics;Reliability;Buildings;Process control;Nickel;Linux;Chaos},
doi={10.1109/TSE.2024.3468296},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3468296},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=nov}

@ARTICLE{10711885,
author={Zhang, Zejun and Xing, Zhenchang and Zhao, Dehai and Xu, Xiwei and Zhu, Liming and Lu, Qinghua},
journal={ IEEE Transactions on Software Engineering },
title={{ Automated Refactoring of Non-Idiomatic Python Code With Pythonic Idioms }},
year={2024},
volume={50},
number={11},
ISSN={1939-3520},
pages={2827-2848},
abstract={ Compared to other programming languages (e.g., Java), Python has more idioms to make Python code concise and efficient. Although Pythonic idioms are well accepted in the Python community, Python programmers are often faced with many challenges in using them, for example, being unaware of certain Pythonic idioms or not knowing how to use them properly. Based on an analysis of 7,577 Python repositories on GitHub, we find that non-idiomatic Python code that can be implemented with Pythonic idioms occurs frequently and widely. To assist Python developers in adopting Pythonic idioms, we design and implement an automatic refactoring tool named RIdiom to refactor code with Pythonic idioms. We identify twelve Pythonic idioms by systematically contrasting the abstract syntax grammar of Python and Java. Then we define the syntactic patterns for detecting non-idiomatic code for each Pythonic idiom. Finally, we devise atomic AST-rewriting operations and refactoring steps to refactor non-idiomatic code into idiomatic code. Our approach is evaluated on 1,814 code refactorings, achieving a precision of 0.99 and a recall of 0.87, underscoring its effectiveness. We further evaluate the tool's utility in helping developers refactor code with Pythonic idioms. A user study involving 14 students demonstrates a 112.9% improvement in correctness and a 35.5% speedup when referring to the tool-generated code pairs. Additionally, the 120 pull requests that refactor non-idiomatic code with Pythonic idioms, submitted to GitHub projects, resulted in 79 responses. Among these, 49 accepted and praised the refactorings, with 42 merging the refactorings into their repositories. },
keywords={Codes;Python;Software development management;Syntactics;Programming;Java;Benchmark testing;Reliability;Grammar;Australia},
doi={10.1109/TSE.2024.3420886},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3420886},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=nov}

@ARTICLE{10752650,
author={Uchitel, Sebastian and Chechik, Marsha and Penta, Massimiliano Di and Adams, Bram and Aguirre, Nazareno and Bavota, Gabriele and Bianculli, Domenico and Blincoe, Kelly and Cavalcanti, Ana and Dittrich, Yvonne and Ferrucci, Filomena and Hoda, Rashina and Huang, LiGuo and Lo, David and Lyu, Michael R. and Ma, Lei and Maletic, Jonathan I. and Mariani, Leonardo and McMillan, Collin and Menzies, Tim and Monperrus, Martin and Moreno, Ana and Nagappan, Nachiappan and Pasquale, Liliana and Pelliccione, Patrizio and Pradel, Michael and Purandare, Rahul and Ryu, Sukyoung and Sabetzadeh, Mehrdad and Serebrenik, Alexander and Sun, Jun and Tantithamthavorn, Kla and Treude, Christoph and Wimmer, Manuel and Xiong, Yingfei and Yue, Tao and Zaidman, Andy and Zhang, Tao and Zhong, Hao},
journal={ IEEE Transactions on Software Engineering },
title={{ Scoping Software Engineering for AI: The TSE Perspective }},
year={2024},
volume={50},
number={11},
ISSN={1939-3520},
pages={2709-2711},
abstract={ },
keywords={},
doi={10.1109/TSE.2024.3470368},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3470368},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=nov}

@ARTICLE{10599334,
author={Su, Zhuo and Yu, Zehong and Wang, Dongyan and Yang, Yixiao and Wang, Rui and Chang, Wanli and Cui, Aiguo and Jiang, Yu},
journal={ IEEE Transactions on Software Engineering },
title={{ HSTCG: State-Aware Simulink Model Test Case Generation With Heuristic Strategy }},
year={2024},
volume={50},
number={12},
ISSN={1939-3520},
pages={3088-3103},
abstract={ Simulink has gained widespread recognition as a valuable tool for system design. As systems grow increasingly complex, particularly in terms of their internal states, this complexity poses new challenges for existing model testing methodologies. Traditional techniques such as constraint solving and random search encounter difficulties when attempting to explore the intricate logic embedded within these models. In this paper, we introduce HSTCG, a state-aware test case generation method for Simulink models with heuristic strategy. HSTCG solves only one iteration of the model each time to get the test input that can cover a target branch, then executes the model once to obtain and update the new model state based on the solved input dynamically. Then, it solves the remaining branches based on the new model state iteratively until all the coverage requirements are satisfied. To improve the efficiency of test case generation, we also designed a heuristic strategy containing heuristic branch searching, repeated state filter and unreached branch filter to minimize the times of constraint solving. We implemented HSTCG and evaluated it on several benchmark Simulink models. Compared to the built-in Simulink Design Verifier and state-of-the-art academic work SimCoTest, HSTCG achieves an average improvement of 55% and 103% on Decision Coverage, 53% and 62% on Condition Coverage and 192% and 201% on Modified Condition Decision Coverage, respectively. We also validated the significant improvement of the heuristic strategy, which can improve the efficiency of test case generation by 62.2% on average. },
keywords={Task analysis;Software packages;Computational modeling;Data models;Testing;Logic;Codes},
doi={10.1109/TSE.2024.3428528},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3428528},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=dec}

@ARTICLE{10682606,
author={Silva, Denini and Gruber, Martin and Gokhale, Satyajit and Arteca, Ellen and Turcotte, Alexi and dAmorim, Marcelo and Lam, Wing and Winter, Stefan and Bell, Jonathan},
journal={ IEEE Transactions on Software Engineering },
title={{ The Effects of Computational Resources on Flaky Tests }},
year={2024},
volume={50},
number={12},
ISSN={1939-3520},
pages={3104-3121},
abstract={ Flaky tests are tests that non-deterministically pass and fail in unchanged code. These tests can be detrimental to developers’ productivity. Particularly when tests run in continuous integration environments, the tests may be competing for access to limited computational resources (CPUs, memory etc.), and we hypothesize that resource (un)-availability may be a significant factor in the failure rate of flaky tests. We present the first assessment of the impact that computational resources have on flaky tests, including a total of 52 projects written in Java, JavaScript and Python, and 27 different resource configurations. Using a rigorous statistical methodology, we determine which tests are RAFTs (Resource-Affected Flaky Tests). We find that 46.5% of the flaky tests in our dataset are RAFTs, indicating that a substantial proportion of flaky-test failures happen depending on the resources available when running tests. We report RAFTs and configurations to avoid them to developers, and received interest to either fix the RAFTs or to improve the specifications of the projects so that tests would be run only in configurations that are unlikely to encounter RAFT failures. Although most test suites in our dataset are executed quite quickly (under one minute) in a baseline configuration, our results highlight the possibility of using this methodology to detect RAFT to reduce the cost of cloud infrastructure for reliably running larger test suites. },
keywords={Servers;Costs;Codes;Java;Prevention and mitigation;Debugging;Python},
doi={10.1109/TSE.2024.3462251},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3462251},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=dec}

@ARTICLE{10682975,
author={Munoz, Paula and Wimmer, Manuel and Troya, Javier and Vallecillo, Antonio},
journal={ IEEE Transactions on Software Engineering },
title={{ Measuring the Fidelity of a Physical and a Digital Twin Using Trace Alignments }},
year={2024},
volume={50},
number={12},
ISSN={1939-3520},
pages={3122-3145},
abstract={ Digital twins are gaining relevance in many domains to improve the operation and maintenance of complex systems. Despite their importance, most efforts are currently focused on their design, development, and deployment but do not fully address their validation. In this paper, we are interested in assessing the fidelity of physical and digital twins and, more specifically, whether they exhibit twinned behaviors. This will allow engineers to check the suitability of the digital twin for its intended purpose. Our approach assesses their fidelity by comparing the behavioral traces of the two twins. Our contribution is threefold. First, we define a measure of equivalence between individual snapshots capable of deciding whether two snapshots are sufficiently similar. Second, we use a trace alignment algorithm to align the corresponding equivalent states reached by the two twins. Finally, we measure the fidelity of the behavior of the two twins using the level of alignment achieved in terms of the percentage of matched snapshots and the distance between the aligned traces. Our proposal has been validated with the digital twins of four cyber-physical systems: an elevator, an incubator, a robotic arm, and a programmable robotic car. We were able to determine which systems were sufficiently faithful and which parts of their behavior failed to emulate their counterparts. Finally, we compared our proposal with similar approaches from the literature, highlighting their respective strengths and weaknesses related to our own. },
keywords={Proposals;Cranes;Digital twins;Measurement;Trajectory;Accuracy;Computational modeling},
doi={10.1109/TSE.2024.3462978},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3462978},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=dec}

@ARTICLE{10704582,
author={Fatima, Sakina and Hemmati, Hadi and C. Briand, Lionel},
journal={ IEEE Transactions on Software Engineering },
title={{ FlakyFix: Using Large Language Models for Predicting Flaky Test Fix Categories and Test Code Repair }},
year={2024},
volume={50},
number={12},
ISSN={1939-3520},
pages={3146-3171},
abstract={ Flaky tests are problematic because they non-deterministically pass or fail for the same software version under test, causing confusion and wasting development effort. While machine learning models have been used to predict flakiness and its root causes, there is much less work on providing support to fix the problem. To address this gap, in this paper, we focus on predicting the type of fix that is required to remove flakiness and then repair the test code on that basis. We do this for a subset of flaky tests where the root cause of flakiness is in the test itself and not in the production code. One key idea is to guide the repair process with additional knowledge about the test's flakiness in the form of its predicted fix category. Thus, we first propose a framework that automatically generates labeled datasets for 13 fix categories and trains models to predict the fix category of a flaky test by analyzing the test code only. Our experimental results using code models and few-shot learning show that we can correctly predict most of the fix categories. To show the usefulness of such fix category labels for automatically repairing flakiness, we augment the prompts of GPT 3.5 Turbo, a Large Language Model (LLM), with such extra knowledge to request repair suggestions. The results show that our suggested fix category labels, complemented with in-context learning, significantly enhance the capability of GPT 3.5 Turbo in generating fixes for flaky tests. Based on the execution and analysis of a sample of GPT-repaired flaky tests, we estimate that a large percentage of such repairs, (roughly between 51% and 83%) can be expected to pass. For the failing repaired tests, on average, 16% of the test code needs to be further changed for them to pass. },
keywords={Codes;Predictive models;Maintenance engineering;Analytical models;Production;Large language models;Java;Few shot learning;Python;Manuals},
doi={10.1109/TSE.2024.3472476},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3472476},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=dec}

@ARTICLE{10705351,
author={Melegati, Jorge and Conboy, Kieran and Graziotin, Daniel},
journal={ IEEE Transactions on Software Engineering },
title={{ Qualitative Surveys in Software Engineering Research: Definition, Critical Review, and Guidelines }},
year={2024},
volume={50},
number={12},
ISSN={1939-3520},
pages={3172-3187},
abstract={ Qualitative surveys are emerging as a popular research method in software engineering (SE), particularly as many aspects of the field are increasingly socio-technical and thus concerned with the subtle, social, and often ambiguous issues that are not amenable to a simple quantitative survey. While many argue that qualitative surveys play a vital role amongst the diverse range of methods employed in SE there are a number of shortcomings that inhibits its use and value. First there is a lack of clarity as to what defines a qualitative survey and what features differentiate it from other methods. There is an absence of a clear set of principles and guidelines for its execution, and what does exist is very inconsistent and sometimes contradictory. These issues undermine the perceived reliability and rigour of this method. Researchers are unsure about how to ensure reliability and rigour when designing qualitative surveys and reviewers are unsure how these should be evaluated. In this paper, we present a systematic mapping study to identify how qualitative surveys have been employed in SE research to date. This paper proposes a set of principles, based on a multidisciplinary review of qualitative surveys and capturing some of the commonalities of the diffuse approaches found. These principles can be used by researchers when choosing whether to do a qualitative survey or not. They can then be used to design their study. The principles can also be used by editors and reviewers to judge the quality and rigour of qualitative surveys. It is hoped that this will result in more widespread use of the method and also more effective and evidence-based reviews of studies that use these methods in the future. },
keywords={Surveys;Software engineering;Guidelines;Reviews;Interviews;Systematics;Instruments;Standards;Software;Ontologies},
doi={10.1109/TSE.2024.3474173},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3474173},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=dec}

@ARTICLE{10707668,
author={Li, Yichen and Huo, Yintong and Jiang, Zhihan and Zhong, Renyi and He, Pinjia and Su, Yuxin and Briand, Lionel C. and Lyu, Michael R.},
journal={ IEEE Transactions on Software Engineering },
title={{ Exploring the Effectiveness of LLMs in Automated Logging Statement Generation: An Empirical Study }},
year={2024},
volume={50},
number={12},
ISSN={1939-3520},
pages={3188-3207},
abstract={ Automated logging statement generation supports developers in documenting critical software runtime behavior. While substantial recent research has focused on retrieval-based and learning-based methods, results suggest they fail to provide appropriate logging statements in real-world complex software. Given the great success in natural language generation and programming language comprehension, large language models (LLMs) might help developers generate logging statements, but this has not yet been investigated. To fill the gap, this paper performs the first study on exploring LLMs for logging statement generation. We first build a logging statement generation dataset, LogBench, with two parts: (1) LogBench-O: 3,870 methods with 6,849 logging statements collected from GitHub repositories, and (2) LogBench-T: the transformed unseen code from LogBench-O. Then, we leverage LogBench to evaluate the effectiveness and generalization capabilities (using LogBench-T) of 13 top-performing LLMs, from 60M to 405B parameters. In addition, we examine the performance of these LLMs against classical retrieval-based and machine learning-based logging methods from the era preceding LLMs. Specifically, we evaluate the logging effectiveness of LLMs by studying their ability to determine logging ingredients and the impact of prompts and external program information. We further evaluate LLM's logging generalization capabilities using unseen data (LogBench-T) derived from code transformation techniques. While existing LLMs deliver decent predictions on logging levels and logging variables, our study indicates that they only achieve a maximum BLEU score of 0.249, thus calling for improvements. The paper also highlights the importance of prompt constructions and external factors (e.g., programming contexts and code comments) for LLMs’ logging performance. In addition, we observed that existing LLMs show a significant performance drop (8.2%-16.2% decrease) when dealing with logging unseen code, revealing their unsatisfactory generalization capabilities. Based on these findings, we identify five implications and provide practical advice for future logging research. Our empirical analysis discloses the limitations of current logging approaches while showcasing the potential of LLM-based logging tools, and provides actionable guidance for building more practical models. },
keywords={Codes;Software;Natural languages;Runtime;Generators;Training data;Software development management;Measurement;Large language models;Analytical models},
doi={10.1109/TSE.2024.3475375},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3475375},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=dec}

@ARTICLE{10713474,
author={Xue, Pengyu and Wu, Linhao and Yu, Zhongxing and Jin, Zhi and Yang, Zhen and Li, Xinyi and Yang, Zhenyu and Tan, Yue},
journal={ IEEE Transactions on Software Engineering },
title={{ Automated Commit Message Generation With Large Language Models: An Empirical Study and Beyond }},
year={2024},
volume={50},
number={12},
ISSN={1939-3520},
pages={3208-3224},
abstract={ Commit Message Generation (CMG) approaches aim to automatically generate commit messages based on given code diffs, which facilitate collaboration among developers and play a critical role in Open-Source Software (OSS). Very recently, Large Language Models (LLMs) have been applied in diverse code-related tasks owing to their powerful generality. Yet, in the CMG field, few studies systematically explored their effectiveness. This paper conducts the first comprehensive experiment to investigate how far we have been in applying LLM to generate high-quality commit messages and how to go further beyond in this field. Motivated by a pilot analysis, we first construct a multi-lingual high-quality CMG test set following practitioners’ criteria. Afterward, we re-evaluate diverse CMG approaches and make comparisons with recent LLMs. To delve deeper into LLMs’ ability, we further propose four manual metrics following the practice of OSS, including Accuracy, Integrity, Readability, and Applicability for assessment. Results reveal that LLMs have outperformed existing CMG approaches overall, and different LLMs carry different advantages, where GPT-3.5 performs best. To further boost LLMs’ performance in the CMG task, we propose an Efficient Retrieval-based In-Context Learning (ICL) framework, namely ERICommiter, which leverages a two-step filtering to accelerate the retrieval efficiency and introduces semantic/lexical-based retrieval algorithm to construct the ICL examples, thereby guiding the generation of high-quality commit messages with LLMs. Extensive experiments demonstrate the substantial performance improvement of ERICommiter on various LLMs across different programming languages. Meanwhile, ERICommiter also significantly reduces the retrieval time while keeping almost the same performance. Our research contributes to the understanding of LLMs’ capabilities in the CMG field and provides valuable insights for practitioners seeking to leverage these tools in their workflows. },
keywords={Codes;Measurement;Manuals;Collaboration;Systematics;Solid modeling;Data models;Python;Large language models;Java},
doi={10.1109/TSE.2024.3478317},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3478317},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=dec}

@ARTICLE{10715677,
author={Wang, Yin and Fan, Ming and Liu, Junfeng and Tao, Junjie and Jin, Wuxia and Wang, Haijun and Xiong, Qi and Liu, Ting},
journal={ IEEE Transactions on Software Engineering },
title={{ Do as You Say: Consistency Detection of Data Practice in Program Code and Privacy Policy in Mini-App }},
year={2024},
volume={50},
number={12},
ISSN={1939-3520},
pages={3225-3248},
abstract={ Mini-app is an emerging form of mobile application that combines web technology with native capabilities. Its features, e.g., no need to download and no installation, have made it popular rapidly. However, privacy issues that violate the laws or regulations are breeding in the swiftly expanding mini-app ecosystem. Ensuring consistency between the mini-app's data practices embedded in its program code behavior and privacy policy description is crucial. But no work has systematically investigated the privacy problem of the mini-app before. To achieve this purpose, there are two main challenges. Firstly, the mini-app represents a novel application form, and a deficiency exists in information-sensitive code analysis tools capable of accurately discerning data practices from the code. Secondly, previous studies focusing on consistency have exhibited granularity issues related to data types and consistency patterns. This paper introduces MiniDetector, a novel approach for identifying consistency issues in mini-apps. MiniDetector employs data flow analysis to pinpoint data practices within the program code and utilizes a two-stage prompt engineering process to extract data practices from privacy policies. The results from both analyses are then compared to establish a consistency match. The proposed method undergoes sufficiency evaluations on a dataset comprising 70 mini-apps. Additionally, we conduct a comprehensive analysis of 100,000 mini-apps on the WeChat client in the wild, extracting 3,369 with privacy policies. Astonishingly, only 11 of these meet the consistency requirements, while 3,358 exhibit inconsistencies, resulting in an alarming inconsistency rate of 99.7%. },
keywords={Privacy;Codes;Social networking (online);Message services;Data privacy;Regulation;Operating systems;Smart phones;Prompt engineering;Protection},
doi={10.1109/TSE.2024.3479288},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3479288},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=dec}

@ARTICLE{10715683,
author={Wang, Xinyi and Ali, Shaukat and Yue, Tao and Arcaini, Paolo},
journal={ IEEE Transactions on Software Engineering },
title={{ Quantum Approximate Optimization Algorithm for Test Case Optimization }},
year={2024},
volume={50},
number={12},
ISSN={1939-3520},
pages={3249-3264},
abstract={ Test case optimization (TCO) reduces the software testing cost while preserving its effectiveness. However, to solve TCO problems for large-scale and complex software systems, substantial computational resources are required. Quantum approximate optimization algorithms (QAOAs) are promising combinatorial optimization algorithms that rely on quantum computational resources, with the potential to offer increased efficiency compared to classical approaches. Several proof-of-concept applications of QAOAs for solving combinatorial problems, such as portfolio optimization, energy optimization in power systems, and job scheduling, have been proposed. Given the lack of investigation into QAOA's application for TCO problems, and motivated by the computational challenges of TCO problems and the potential of QAOAs, we present IGDec-QAOA to formulate a TCO problem as a QAOA problem and solve it on both ideal and noisy quantum computer simulators, as well as on a real quantum computer. To solve bigger TCO problems that require many qubits, which are unavailable these days, we integrate a problem decomposition strategy with the QAOA. We performed an empirical evaluation with five TCO problems and four publicly available industrial datasets from ABB, Google, and Orona to compare various configurations of IGDec-QAOA, assess its decomposition strategy of handling large datasets, and compare its performance with classical algorithms (i.e., Genetic Algorithm (GA) and Random Search). Based on the evaluation results achieved on an ideal simulator, we recommend the best configuration of our approach for TCO problems. Also, we demonstrate that our approach can reach the same effectiveness as GA and outperform GA in two out of five test case optimization problems we conducted. In addition, we observe that, on the noisy simulator, IGDec-QAOA achieved similar performance to that from the ideal simulator. Finally, we also demonstrate the feasibility of IGDec-QAOA on a real quantum computer in the presence of noise. },
keywords={Quantum computing;Optimization;Qubit;Logic gates;Approximation algorithms;Noise measurement;Software algorithms;Software systems;Noise;Genetic algorithms},
doi={10.1109/TSE.2024.3479421},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3479421},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=dec}

@ARTICLE{10720528,
author={Xie, Huan and Lei, Yan and Yan, Meng and Li, Shanshan and Mao, Xiaoguang and Yu, Yue and Lo, David},
journal={ IEEE Transactions on Software Engineering },
title={{ Towards More Precise Coincidental Correctness Detection With Deep Semantic Learning }},
year={2024},
volume={50},
number={12},
ISSN={1939-3520},
pages={3265-3289},
abstract={ Coincidental correctness (CC) is a situation during the execution of a test case, the buggy entity is executed, but the program behaves correctly as expected. Many automated fault localization (FL) techniques use runtime information to discover the underlying connection between the executed buggy entity and the failing test result. The existence of CC will weaken such connection, mislead the FL algorithms to build inaccurate models, and consequently, decrease the localization accuracy. To alleviate the adverse effect of CC on FL, CC detection techniques have been proposed to identify the possible CC tests via heuristic or machine learning algorithms. However, their performance on precision is not satisfactory since they overestimate the possible CC tests and are insufficient in learning the deep semantic features. In this work, we propose a novel Triplet network-based Coincidental Correctness detection technique (i.e., TriCoCo) to overcome the limitations of the prior works. TriCoCo narrows the possible CC tests by designing three features to identify genuine passing tests. Instead of using all tests as inputs by existing techniques, TriCoCo takes the identified genuine passing tests and failing ones to train a triplet model that can evaluate their relative distance. Finally, TriCoCo infers the probability of being a CC test of the test in the rest of the passing tests by using the trained triplet model. We conduct large-scale experiments to evaluate TriCoCo based on the widely-used Defects4J benchmark. The results demonstrate that TriCoCo can improve not only the precision of CC detection but also the effectiveness of FL techniques, e.g., the precision of TriCoCo is 80.33$\%$% on average, and TriCoCo boosts the efficacy of DStar by 18$\%$%–74$\%$% in terms of MFR metric when compared to seven state-of-the-art CC detection baselines. },
keywords={Feature extraction;Semantics;Machine learning algorithms;Location awareness;Deep learning;Clustering algorithms;Prediction algorithms;Benchmark testing;Accuracy;Partitioning algorithms},
doi={10.1109/TSE.2024.3481893},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3481893},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=dec}

@ARTICLE{10720834,
author={Abbasishahkoo, Amin and Dadkhah, Mahboubeh and Briand, Lionel and Lin, Dayi},
journal={ IEEE Transactions on Software Engineering },
title={{ TEASMA: A Practical Methodology for Test Adequacy Assessment of Deep Neural Networks }},
year={2024},
volume={50},
number={12},
ISSN={1939-3520},
pages={3307-3329},
abstract={ Successful deployment of Deep Neural Networks (DNNs), particularly in safety-critical systems, requires their validation with an adequate test set to ensure a sufficient degree of confidence in test outcomes. Although well-established test adequacy assessment techniques from traditional software, such as mutation analysis and coverage criteria, have been adapted to DNNs in recent years, we still need to investigate their application within a comprehensive methodology for accurately predicting the fault detection ability of test sets and thus assessing their adequacy. In this paper, we propose and evaluate TEASMA, a comprehensive and practical methodology designed to accurately assess the adequacy of test sets for DNNs. In practice, TEASMA allows engineers to decide whether they can trust high-accuracy test results and thus validate the DNN before its deployment. Based on a DNN model's training set, TEASMA provides a procedure to build accurate DNN-specific prediction models of the Fault Detection Rate (FDR) of a test set using an existing adequacy metric, thus enabling its assessment. We evaluated TEASMA with four state-of-the-art test adequacy metrics: Distance-based Surprise Coverage (DSC), Likelihood-based Surprise Coverage (LSC), Input Distribution Coverage (IDC), and Mutation Score (MS). We calculated MS based on mutation operators that directly modify the trained DNN model (i.e., post-training operators) due to their significant computational advantage compared to the operators that modify the DNN's training set or program (i.e., pre-training operators). Our extensive empirical evaluation, conducted across multiple DNN models and input sets, including large input sets such as ImageNet, reveals a strong linear correlation between the predicted and actual FDR values derived from MS, DSC, and IDC, with minimum $R^{2}$R2 values of 0.94 for MS and 0.90 for DSC and IDC. Furthermore, a low average Root Mean Square Error (RMSE) of 9% between actual and predicted FDR values across all subjects, when relying on regression analysis and MS, demonstrates the latter's superior accuracy when compared to DSC and IDC, with RMSE values of 0.17 and 0.18, respectively. Overall, these results suggest that TEASMA provides a reliable basis for confidently deciding whether to trust test results for DNN models. },
keywords={Measurement;Training;Artificial neural networks;Predictive models;Accuracy;Testing;Analytical models;Fault detection;Correlation;Computational modeling},
doi={10.1109/TSE.2024.3482984},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3482984},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=dec}

@ARTICLE{10726920,
author={Hasan, Mohammed Tayeeb and Tsantalis, Nikolaos and Alikhanifard, Pouria},
journal={ IEEE Transactions on Software Engineering },
title={{ Refactoring-Aware Block Tracking in Commit History }},
year={2024},
volume={50},
number={12},
ISSN={1939-3520},
pages={3330-3350},
abstract={ Tracking the change history of statements in the commits of a project repository is in many cases useful for supporting various software maintenance, comprehension, and evolution tasks. A high level of accuracy can facilitate the adoption of code tracking tools by developers and researchers. To this end, we propose CodeTracker, a refactoring-aware tool that can generate the commit change history for code blocks. To evaluate its accuracy, we created an oracle with the change history of 1,280 code blocks found within 200 methods from 20 popular open-source project repositories. Moreover, we created a baseline based on the current state-of-the-art Abstract Syntax Tree diff tool, namely GumTree 3.0, in order to compare the accuracy and execution time. Our experiments have shown that CodeTracker has a considerably higher precision/recall and faster execution time than the GumTree-based baseline, and can extract the complete change history of a code block with a precision and recall of 99.5% within 3.6 seconds on average. },
keywords={Codes;History;Software;Accuracy;Containers;Computer bugs;Source coding;Software engineering;Java;Target tracking},
doi={10.1109/TSE.2024.3484586},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3484586},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=dec}

@ARTICLE{10732009,
author={Zhang, Mengxi and Liu, Huaxiao and Zhou, Yuheng and Chen, Chunyang and Huang, Pei and Zhao, Jian},
journal={ IEEE Transactions on Software Engineering },
title={{ Don’t Confuse! Redrawing GUI Navigation Flow in Mobile Apps for Visually Impaired Users }},
year={2024},
volume={50},
number={12},
ISSN={1939-3520},
pages={3351-3368},
abstract={ Mobile applications (apps) are integral to our daily lives, offering diverse services and functionalities. They enable sighted users to access information coherently in an extremely convenient manner. However, it remains unclear if visually impaired users, who rely solely on the screen readers (e.g., Talkback) to navigate and access app information, can do so in the correct and reasonable order. This may result in significant information bias and operational errors. Furthermore, in our preliminary exploration, we explained and clarified that the navigation sequence-related issues encountered by visually impaired users could be categorized into two types: unintuitive navigation sequence and unapparent focus switching. Considering these issues, in this work, we proposed a method named RGNF (Re-draw GUI Navigation Flow). It aimed to enhance the understandability and coherence of accessing the content of each component within the Graphical User Interface (GUI), together with assisting developers in creating well-designed GUI navigation flow (GNF). This method was inspired by the characteristics identified in our preliminary study, where visually impaired users expected navigation to be associated with close position and similar shape of GUI components that were read consecutively. Thus, our method relied on the principles derived from the Gestalt psychological model, aiming to group GUI components into different regions according to the laws of proximity and similarity, thereby redrawing the GNFs. To evaluate the effectiveness of our method, we calculated sequence similarity values before and after redrawing the GNF, and further employed the tools proposed by Alotaibi et al. to measure the reachability of GUI components. Our results demonstrated a substantial improvement in similarity (0.921) compared to the baseline (0.624), together with the reachability (90.31%) compared to the baseline GNF (74.35%). Furthermore, a qualitative user study revealed that our method had a positive effect on providing visually impaired users with an improved user experience. },
keywords={Navigation;Graphical user interfaces;Recruitment;Psychology;Guidelines;Computer science;Visualization;Shape;Mobile applications;Filtering},
doi={10.1109/TSE.2024.3485225},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3485225},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=dec}

@ARTICLE{10734067,
author={Liao, Dianshu and Pan, Shidong and Sun, Xiaoyu and Ren, Xiaoxue and Huang, Qing and Xing, Zhenchang and Jin, Huan and Li, Qinying},
journal={ IEEE Transactions on Software Engineering },
title={{ $\mathbf{A^{3}}$A3-CodGen: A Repository-Level Code Generation Framework for Code Reuse With Local-Aware, Global-Aware, and Third-Party-Library-Aware }},
year={2024},
volume={50},
number={12},
ISSN={1939-3520},
pages={3369-3384},
abstract={ LLM-based code generation tools are essential to help developers in the software development process. Existing tools often disconnect with the working context, i.e., the code repository, causing the generated code to be not similar to human developers. In this paper, we propose a novel code generation framework, dubbed $A^{3}$A3-CodGen, to harness information within the code repository to generate code with fewer potential logical errors, code redundancy, and library-induced compatibility issues. We identify three types of representative information for the code repository: local-aware information from the current code file, global-aware information from other code files, and third-party-library information. Results demonstrate that by adopting the $A^{3}$A3-CodGen framework, we successfully extract, fuse, and feed code repository information into the LLM, generating more accurate, efficient, and highly reusable code. The effectiveness of our framework is further underscored by generating code with a higher reuse rate, compared to human developers. This research contributes significantly to the field of code generation, providing developers with a more powerful tool to address the evolving demands in software development in practice. },
keywords={Codes;Libraries;Software development management;Data mining;Benchmark testing;Transformers;Software engineering;Context modeling;Chatbots;Australia},
doi={10.1109/TSE.2024.3486195},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3486195},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=dec}

@ARTICLE{10735776,
author={Yang, Zhou and Zhao, Zhipeng and Wang, Chenyu and Shi, Jieke and Kim, Dongsun and Han, DongGyun and Lo, David},
journal={ IEEE Transactions on Software Engineering },
title={{ Gotcha! This Model Uses My Code! Evaluating Membership Leakage Risks in Code Models }},
year={2024},
volume={50},
number={12},
ISSN={1939-3520},
pages={3290-3306},
abstract={ Leveraging large-scale datasets from open-source projects and advances in large language models, recent progress has led to sophisticated code models for key software engineering tasks, such as program repair and code completion. These models are trained on data from various sources, including public open-source projects like GitHub and private, confidential code from companies, raising significant privacy concerns. This paper investigates a crucial but unexplored question: What is the risk of membership information leakage in code models? Membership leakage refers to the vulnerability where an attacker can infer whether a specific data point was part of the training dataset. We present Gotcha, a novel membership inference attack method designed for code models, and evaluate its effectiveness on Java-based datasets. Gotcha simultaneously considers three key factors: model input, model output, and ground truth. Our ablation study confirms that each factor significantly enhances attack performance. Our ablation study confirms that each factor significantly enhances attack performance. Our investigation reveals a troubling finding: membership leakage risk is significantly elevated. While previous methods had accuracy close to random guessing, Gotcha achieves high precision, with a true positive rate of 0.95 and a low false positive rate of 0.10. We also demonstrate that the attacker's knowledge of the victim model (e.g., model architecture and pre-training data) affects attack success. Additionally, modifying decoding strategies can help reduce membership leakage risks. This research highlights the urgent need to better understand the privacy vulnerabilities of code models and develop strong countermeasures against these threats. },
keywords={Codes;Data models;Training data;Training;Information leakage;Software development management;Data privacy;Privacy;Decoding;Source coding},
doi={10.1109/TSE.2024.3482719},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3482719},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=dec}

@ARTICLE{10738434,
author={Chaleshtari, Nazanin Bayati and Marquer, Yoann and Pastore, Fabrizio and Briand, Lionel C.},
journal={ IEEE Transactions on Software Engineering },
title={{ AIM: Automated Input Set Minimization for Metamorphic Security Testing }},
year={2024},
volume={50},
number={12},
ISSN={1939-3520},
pages={3403-3434},
abstract={ Although the security testing of Web systems can be automated by generating crafted inputs, solutions to automate the test oracle, i.e., vulnerability detection, remain difficult to apply in practice. Specifically, though previous work has demonstrated the potential of metamorphic testing—security failures can be determined by metamorphic relations that turn valid inputs into malicious inputs—metamorphic relations are typically executed on a large set of inputs, which is time-consuming and thus makes metamorphic testing impractical. We propose AIM, an approach that automatically selects inputs to reduce testing costs while preserving vulnerability detection capabilities. AIM includes a clustering-based black-box approach, to identify similar inputs based on their security properties. It also relies on a novel genetic algorithm to efficiently select diverse inputs while minimizing their total cost. Further, it contains a problem-reduction component to reduce the search space and speed up the minimization process. We evaluated the effectiveness of AIM on two well-known Web systems, Jenkins and Joomla, with documented vulnerabilities. We compared AIM's results with four baselines involving standard search approaches. Overall, AIM reduced metamorphic testing time by 84% for Jenkins and 82% for Joomla, while preserving the same level of vulnerability detection. Furthermore, AIM significantly outperformed all the considered baselines regarding vulnerability coverage. },
keywords={Testing;Security;Software;Uniform resource locators;Minimization;Costs;Reactive power;Software reliability;Search problems;Scalability},
doi={10.1109/TSE.2024.3488525},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3488525},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=dec}

@ARTICLE{10738442,
author={Zhu, Jingyun and Li, Kaixuan and Chen, Sen and Fan, Lingling and Wang, Junjie and Xie, Xiaofei},
journal={ IEEE Transactions on Software Engineering },
title={{ A Comprehensive Study on Static Application Security Testing (SAST) Tools for Android }},
year={2024},
volume={50},
number={12},
ISSN={1939-3520},
pages={3385-3402},
abstract={ To identify security vulnerabilities in Android applications, numerous static application security testing (SAST) tools have been proposed. However, it poses significant challenges to assess their overall performance on diverse vulnerability types. The task is non-trivial and poses considerable challenges. Firstly, the absence of a unified evaluation platform for defining and describing tools’ supported vulnerability types, coupled with the lack of normalization for the intricate and varied reports generated by different tools, significantly adds to the complexity. Secondly, there is a scarcity of adequate benchmarks, particularly those derived from real-world scenarios. To address these problems, we are the first to propose a unified platform named VulsTotal, supporting various vulnerability types, enabling comprehensive and versatile analysis across diverse SAST tools. Specifically, we begin by meticulously selecting 11 free and open-sourced SAST tools from a pool of 97 existing options, adhering to clearly defined criteria. After that, we invest significant efforts in comprehending the detection rules of each tool, subsequently unifying 67 general/common vulnerability types for Android SAST tools. We also redefine and implement a standardized reporting format, ensuring uniformity in presenting results across all tools. Additionally, to mitigate the problem of benchmarks, we conducted a manual analysis of huge amounts of CVEs to construct a new CVE-based benchmark based on our comprehension of Android app vulnerabilities. Leveraging the evaluation platform, which integrates both existing synthetic benchmarks and newly constructed CVE-based benchmarks from this study, we conducted a comprehensive analysis to evaluate and compare these selected tools from various perspectives, such as general vulnerability type coverage, type consistency, tool effectiveness, and time performance. Our observations yielded impressive findings, like the technical reasons underlying the performance, which provide insights for different stakeholders. },
keywords={Benchmark testing;Operating systems;Stars;Software development management;Python;Stakeholders;Source coding;Mobile applications;Documentation;Codes},
doi={10.1109/TSE.2024.3488041},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3488041},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=dec}

@ARTICLE{10745266,
author={Yu, Xiao and Liu, Lei and Hu, Xing and Keung, Jacky Wai and Liu, Jin and Xia, Xin},
journal={ IEEE Transactions on Software Engineering },
title={{ Fight Fire With Fire: How Much Can We Trust ChatGPT on Source Code-Related Tasks? }},
year={2024},
volume={50},
number={12},
ISSN={1939-3520},
pages={3435-3453},
abstract={ With the increasing utilization of large language models such as ChatGPT during software development, it has become crucial to verify the quality of code content it generates. Recent studies proposed utilizing ChatGPT as both a developer and tester for multi-agent collaborative software development. The multi-agent collaboration empowers ChatGPT to produce test reports for its generated code, enabling it to self-verify the code content and fix bugs based on these reports. However, these studies did not assess the effectiveness of the generated test reports in validating the code. Therefore, we conduct a comprehensive empirical investigation to evaluate ChatGPT's self-verification capability in code generation, code completion, and program repair. We request ChatGPT to (1) generate correct code and then self-verify its correctness; (2) complete code without vulnerabilities and then self-verify for the presence of vulnerabilities; and (3) repair buggy code and then self-verify whether the bugs are resolved. Our findings on two code generation datasets, one code completion dataset, and two program repair datasets reveal the following observations: (1) ChatGPT often erroneously predicts its generated incorrect code as correct, its vulnerable completed code as non-vulnerable, and its failed program repairs as successful during its self-verification. (2) The self-contradictory hallucinations in ChatGPT's behavior arise: (a) ChatGPT initially generates code that it believes to be correct but later predicts it to be incorrect; (b) ChatGPT initially generates code completions that it deems secure but later predicts them to be vulnerable; (c) ChatGPT initially outputs code that it considers successfully repaired but later predicts it to be buggy during its self-verification. (3) The self-verification capability of ChatGPT can be enhanced by asking the guiding question, which queries whether ChatGPT agrees with assertions about incorrectly generated or repaired code and vulnerabilities in completed code. (4) Using test reports generated by ChatGPT can identify more vulnerabilities in completed code, but the explanations for incorrectly generated code and failed repairs are mostly inaccurate in the test reports. Based on these findings, we provide implications for further research or development using ChatGPT. },
keywords={Codes;Chatbots;Maintenance engineering;Software development management;Software;Computer bugs;Urban areas;Computer science;Accuracy;Source coding},
doi={10.1109/TSE.2024.3492204},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3492204},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=dec}

@ARTICLE{10746847,
author={Jiang, Yuan and Zhang, Yujian and Su, Xiaohong and Treude, Christoph and Wang, Tiantian},
journal={ IEEE Transactions on Software Engineering },
title={{ StagedVulBERT: Multigranular Vulnerability Detection With a Novel Pretrained Code Model }},
year={2024},
volume={50},
number={12},
ISSN={1939-3520},
pages={3454-3471},
abstract={ The emergence of pre-trained model-based vulnerability detection methods has significantly advanced the field of automated vulnerability detection. However, these methods still face several challenges, such as difficulty in learning effective feature representations of statements for fine-grained predictions and struggling to process overly long code sequences. To address these issues, this study introduces StagedVulBERT, a novel vulnerability detection framework that leverages a pre-trained code language model and employs a coarse-to-fine strategy. The key innovation and contribution of our research lies in the development of the CodeBERT-HLS component within our framework, specialized in hierarchical, layered, and semantic encoding. This component is designed to capture semantics at both the token and statement levels simultaneously, which is crucial for achieving more accurate multi-granular vulnerability detection. Additionally, CodeBERT-HLS efficiently processes longer code token sequences, making it more suited to real-world vulnerability detection. Comprehensive experiments demonstrate that our method enhances the performance of vulnerability detection at both coarse- and fine-grained levels. Specifically, in coarse-grained vulnerability detection, StagedVulBERT achieves an F1 score of 92.26%, marking a 6.58% improvement over the best-performing methods. At the fine-grained level, our method achieves a Top-5% accuracy of 65.69%, which outperforms the state-of-the-art methods by up to 75.17%. },
keywords={Codes;Transformers;Feature extraction;Semantics;Training;Accuracy;Technological innovation;Predictive models;Heating systems;Computer architecture},
doi={10.1109/TSE.2024.3493245},
url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3493245},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=dec}
