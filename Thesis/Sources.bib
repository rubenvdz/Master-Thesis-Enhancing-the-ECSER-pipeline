@article{arvidsson2023,
  title={Prompt engineering guidelines for LLMs in Requirements Engineering},
  author={Arvidsson, Simon and Axell, Johan},
  year={2023}
}

@misc{ronanki2023,
      title={Requirements Engineering using Generative AI: Prompts and Prompting Patterns}, 
      author={Krishna Ronanki and Beatriz Cabrero-Daniel and Jennifer Horkoff and Christian Berger},
      year={2023},
      eprint={2311.03832},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2311.03832}, 
}

@article{ekin2023,
  author = {Ekin, S.},
  title = {Prompt engineering for chatgpt: a quick guide to techniques, tips, and best practices},
  year = {2023},
  doi = {10.36227/techrxiv.22683919.v2}
}


@inproceedings{kitchenham2004,
author = {Kitchenham, Barbara and Dybå, Tore and Jorgensen, M.},
year = {2004},
month = {06},
pages = {273-281},
title = {Evidence-based software engineering},
isbn = {0-7695-2163-0},
doi = {10.1109/ICSE.2004.1317449}
}

@ARTICLE{kitchenham2002,
  author={Kitchenham, B.A. and Pfleeger, S.L. and Pickard, L.M. and Jones, P.W. and Hoaglin, D.C. and El Emam, K. and Rosenberg, J.},
  journal={IEEE Transactions on Software Engineering}, 
  title={Preliminary guidelines for empirical research in software engineering}, 
  year={2002},
  volume={28},
  number={8},
  pages={721-734},
  keywords={Guidelines;Software engineering;Computer Society;Statistics;Computer science;Data analysis;Software standards;Laboratories;Software performance;Surgery},
  doi={10.1109/TSE.2002.1027796}}

@article{kitchenham2011,
title = {Using mapping studies as the basis for further research – A participant-observer case study},
journal = {Information and Software Technology},
volume = {53},
number = {6},
pages = {638-651},
year = {2011},
note = {Special Section: Best papers from the APSEC},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2010.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0950584910002272},
author = {Barbara A. Kitchenham and David Budgen and O. {Pearl Brereton}},
keywords = {Case study, Systematic literature review, Mapping studies, Software engineering},
abstract = {Context
We are strong advocates of evidence-based software engineering (EBSE) in general and systematic literature reviews (SLRs) in particular. We believe it is essential that the SLR methodology is used constructively to support software engineering research.
Objective
This study aims to assess the value of mapping studies which are a form of SLR that aims to identify and categorise the available research on a broad software engineering topic.
Method
We used a multi-case, participant-observer case study using five examples of studies that were based on preceding mapping studies. We also validated our results by contacting two other researchers who had undertaken studies based on preceding mapping studies and by assessing review comments related to our follow-on studies.
Results
Our original case study identified 11 unique benefits that can accrue from basing research on a preceding mapping study of which only two were case specific. We also identified nine problems associated with using preceding mapping studies of which two were case specific. These results were consistent with the information obtained from the validation activities. We did not find an example of an independent research group making use of a mapping study produced by other researchers.
Conclusion
Mapping studies can save time and effort for researchers and provide baselines to assist new research efforts. However, they must be of high quality in terms of completeness and rigour if they are to be a reliable basis for follow-on research.}
}

@article{kitchenham2007,
author = {Kitchenham, Barbara and Charters, Stuart},
year = {2007},
month = {01},
pages = {},
title = {Guidelines for performing Systematic Literature Reviews in Software Engineering},
volume = {2}
}

@article{Dellanna2022,
author = {Dell’Anna, Davide and Aydemir, Fatma Ba\c{s}ak and Dalpiaz, Fabiano},
title = {Evaluating classifiers in SE research: the ECSER pipeline and two replication studies},
year = {2022},
issue_date = {Jan 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-022-10243-1},
doi = {10.1007/s10664-022-10243-1},
journal = {Empirical Softw. Engg.},
month = nov,
numpages = {40},
keywords = {Replication study, Software engineering, Machine learning, Automated classification}
}

@inproceedings{Zhang2002,
author = {Zhang, Du and Tsai, J.J.P.},
year = {2002},
month = {02},
pages = {22 - 29},
title = {Machine learning and software engineering},
volume = {11},
isbn = {0-7695-1849-4},
journal = {Software Quality Journal - SQJ},
doi = {10.1109/TAI.2002.1180784}
}

@article{Menzies2012,
  title = {Special issue on repeatable results in software engineering prediction},
  volume = {17},
  ISSN = {1573-7616},
  url = {http://dx.doi.org/10.1007/s10664-011-9193-5},
  DOI = {10.1007/s10664-011-9193-5},
  number = {1–2},
  journal = {Empirical Software Engineering},
  publisher = {Springer Science and Business Media LLC},
  author = {Menzies,  Tim and Shepperd,  Martin},
  year = {2012},
  month = jan,
  pages = {1–17}
}

@INPROCEEDINGS{fan2023,
  author={Fan, Angela and Gokkaya, Beliz and Harman, Mark and Lyubarskiy, Mitya and Sengupta, Shubho and Yoo, Shin and Zhang, Jie M.},
  booktitle={2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE)}, 
  title={Large Language Models for Software Engineering: Survey and Open Problems}, 
  year={2023},
  volume={},
  number={},
  pages={31-53},
  keywords={Surveys;Maintenance engineering;Reliability engineering;Software;Software reliability;Software engineering;Testing;Automated Program Repair;Documentation generation;Generative AI;Genetic Improvement;Human-Computer Interaction;Large Language Models;Refactoring;Requirements engineering;Search Based Software Engineering (SBSE);Software Analytics;Software Engineering Education;Software Processes;Software Maintenance and Evolution;Software Testing},
  doi={10.1109/ICSE-FoSE59343.2023.00008}
}

@misc{zhang2024,
      title={A Survey on Large Language Models for Software Engineering}, 
      author={Quanjun Zhang and Chunrong Fang and Yang Xie and Yaxin Zhang and Yun Yang and Weisong Sun and Shengcheng Yu and Zhenyu Chen},
      year={2024},
      eprint={2312.15223},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2312.15223}, 
}

@InProceedings{Belzner2024,
author="Belzner, Lenz
and Gabor, Thomas
and Wirsing, Martin",
editor="Steffen, Bernhard",
title="Large Language Model Assisted Software Engineering: Prospects, Challenges, and a Case Study",
booktitle="Bridging the Gap Between AI and Reality",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="355--374",
abstract="Large language models such as OpenAI's GPT and Google's Bard offer new opportunities for supporting software engineering processes. Large language model assisted software engineering promises to support developers in a conversational way with expert knowledge over the whole software lifecycle. Current applications range from requirements extraction, ambiguity resolution, code and test case generation, code review and translation to verification and repair of software vulnerabilities. In this paper we present our position on the potential benefits and challenges associated with the adoption of language models in software engineering. In particular, we focus on the possible applications of large language models for requirements engineering, system design, code and test generation, code quality reviews, and software process management. We also give a short review of the state-of-the-art of large language model support for software construction and illustrate our position by a case study on the object-oriented development of a simple ``search and rescue'' scenario.",
isbn="978-3-031-46002-9"
}

@article{Zheng2024,
  title = {Towards an understanding of large language models in software engineering tasks},
  volume = {30},
  ISSN = {1573-7616},
  url = {http://dx.doi.org/10.1007/s10664-024-10602-0},
  DOI = {10.1007/s10664-024-10602-0},
  number = {2},
  journal = {Empirical Software Engineering},
  publisher = {Springer Science and Business Media LLC},
  author = {Zheng,  Zibin and Ning,  Kaiwen and Zhong,  Qingyuan and Chen,  Jiachi and Chen,  Wenqing and Guo,  Lianghong and Wang,  Weicheng and Wang,  Yanlin},
  year = {2024},
  month = dec 
}

@ARTICLE{Ozkaya2023,
  author={Ozkaya, Ipek},
  journal={IEEE Software}, 
  title={Application of Large Language Models to Software Engineering Tasks: Opportunities, Risks, and Implications}, 
  year={2023},
  volume={40},
  number={3},
  pages={4-8},
  keywords={},
  doi={10.1109/MS.2023.3248401}
}

@misc{wang2024,
      title={Software Testing with Large Language Models: Survey, Landscape, and Vision}, 
      author={Junjie Wang and Yuchao Huang and Chunyang Chen and Zhe Liu and Song Wang and Qing Wang},
      year={2024},
      eprint={2307.07221},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2307.07221}, 
}

@article{wang2023plm,
title = {Pre-Trained Language Models and Their Applications},
journal = {Engineering},
volume = {25},
pages = {51-65},
year = {2023},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2022.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S2095809922006324},
author = {Haifeng Wang and Jiwei Li and Hua Wu and Eduard Hovy and Yu Sun},
keywords = {Pre-trained models, Natural language processing},
abstract = {Pre-trained language models have achieved striking success in natural language processing (NLP), leading to a paradigm shift from supervised learning to pre-training followed by fine-tuning. The NLP community has witnessed a surge of research interest in improving pre-trained models. This article presents a comprehensive review of representative work and recent progress in the NLP field and introduces the taxonomy of pre-trained models. We first give a brief introduction of pre-trained models, followed by characteristic methods and frameworks. We then introduce and analyze the impact and challenges of pre-trained models and their downstream applications. Finally, we briefly conclude and address future research directions in this field.}
}

@inproceedings{mikolov2012rnn,
  title={Context dependent recurrent neural network language model},
  author={Mikolov, Tomas and Zweig, Geoffrey},
  booktitle={2012 IEEE Spoken Language Technology Workshop (SLT)},
  pages={234--239},
  year={2012},
  organization={IEEE}
}

@article{hochreiter1997lstm,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT press}
}

@misc{hou2024,
      title={Large Language Models for Software Engineering: A Systematic Literature Review}, 
      author={Xinyi Hou and Yanjie Zhao and Yue Liu and Zhou Yang and Kailong Wang and Li Li and Xiapu Luo and David Lo and John Grundy and Haoyu Wang},
      year={2024},
      eprint={2308.10620},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2308.10620}, 
}

@ARTICLE{fields2024,
  author={Fields, John and Chovanec, Kevin and Madiraju, Praveen},
  journal={IEEE Access}, 
  title={A Survey of Text Classification With Transformers: How Wide? How Large? How Long? How Accurate? How Expensive? How Safe?}, 
  year={2024},
  volume={12},
  number={},
  pages={6518-6531},
  keywords={Text categorization;Transformers;Surveys;Task analysis;Taxonomy;Data models;Chatbots;NLP;text classification;transformers;survey},
  doi={10.1109/ACCESS.2024.3349952}
}


@article{Guo2024health,
    title={Evaluating large language models for health-related text classification tasks with public social media data},
    author={Yuting Guo and Anthony Ovadje and M. Al-garadi and Abeed Sarker},
    journal={Journal of the American Medical Informatics Association : JAMIA},
    year={2024},
    volume={31},
    pages={2181 - 2189},
    doi={10.1093/jamia/ocae210}
}

@misc{guo2023survey,
      title={Evaluating Large Language Models: A Comprehensive Survey}, 
      author={Zishan Guo and Renren Jin and Chuang Liu and Yufei Huang and Dan Shi and Supryadi and Linhao Yu and Yan Liu and Jiaxuan Li and Bojian Xiong and Deyi Xiong},
      year={2023},
      eprint={2310.19736},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.19736}, 
}

@article{Chen2024,
  title={ClinicalBench: Can LLMs Beat Traditional ML Models in Clinical Prediction?},
  author={Canyu Chen and Jian Yu and Shan Chen and Che Liu and Zhongwei Wan and Danielle S. Bitterman and Fei Wang and Kai Shu},
  journal={ArXiv},
  year={2024},
  volume={abs/2411.06469},
  url={https://api.semanticscholar.org/CorpusID:273963111}
}

@misc{vajjala2025,
      title={Text Classification in the LLM Era -- Where do we stand?}, 
      author={Sowmya Vajjala and Shwetali Shimangaud},
      year={2025},
      eprint={2502.11830},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.11830}, 
}

@InProceedings{Marvin2024,
author="Marvin, Ggaliwango
and Hellen, Nakayiza
and Jjingo, Daudi
and Nakatumba-Nabende, Joyce",
editor="Jacob, I. Jeena
and Piramuthu, Selwyn
and Falkowski-Gilski, Przemyslaw",
title="Prompt Engineering in Large Language Models",
booktitle="Data Intelligence and Cognitive Informatics",
year="2024",
publisher="Springer Nature Singapore",
address="Singapore",
pages="387--402",
abstract="With the undeniable rapid development of Conversational Artificial Intelligence (AI) particularly Large Language Models (LLMs), prompt engineering has become an obligatory skill for effective communication and interaction with language driven tools like ChatGPT. It can be leveraged in enforcing rules and process automation for ensuring good quality and quantity of output from LLMs. Moreover, the order of providing examples within prompts, automatic instruction generation, and selection methods has been proven to significantly impact the performance of LLMs. Prompts can be optimized to maximize a chosen score function by searching a pool of instruction candidates within LLMs. No wonder automatically generated instructions give better or similar performance than human annotated instructions and outperform baselines of LLMs, this makes prompt engineering a programming procedure for customizing outputs and interactions of LLMs. In this chapter, we provide thorough understanding of prompt engineering, latest prompt engineering techniques with relevant exercises for putting the techniques in practice. We also discuss current and future trends of LLMs and prompt engineering research, including the rise of automatic instruction generation and selection methods. These are very important for prompt and NLP engineers, conversational AI researchers, and all information seekers or users of LLMs and prompt engineering tools in sensitive domains like health care, security, education among others. The chapter provides indepth understanding of prompt engineering principles and techniques for responsible coversational AI.",
isbn="978-981-99-7962-2"
}

@misc{white2023,
      title={A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT}, 
      author={Jules White and Quchen Fu and Sam Hays and Michael Sandborn and Carlos Olea and Henry Gilbert and Ashraf Elnashar and Jesse Spencer-Smith and Douglas C. Schmidt},
      year={2023},
      eprint={2302.11382},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2302.11382}, 
}

@article{Chicco2020,
  title = {The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation},
  volume = {21},
  ISSN = {1471-2164},
  url = {http://dx.doi.org/10.1186/s12864-019-6413-7},
  DOI = {10.1186/s12864-019-6413-7},
  number = {1},
  journal = {BMC Genomics},
  publisher = {Springer Science and Business Media LLC},
  author = {Chicco,  Davide and Jurman,  Giuseppe},
  year = {2020},
  month = jan 
}

@article{Foody2023,
  title={Challenges in the real world use of classification accuracy metrics: From recall and precision to the Matthews correlation coefficient},
  author={Giles M. Foody},
  journal={PLOS ONE},
  year={2023},
  volume={18},
  url={https://api.semanticscholar.org/CorpusID:263668955}
}

@article{Yao2020,
  title={Assessing software defection prediction performance: why using the Matthews correlation coefficient matters},
  author={Jingxiu Yao and Martin J. Shepperd},
  journal={Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:211818318}
}

@misc{liu2024,
      title={Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment}, 
      author={Yang Liu and Yuanshun Yao and Jean-Francois Ton and Xiaoying Zhang and Ruocheng Guo and Hao Cheng and Yegor Klochkov and Muhammad Faaiz Taufiq and Hang Li},
      year={2024},
      eprint={2308.05374},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2308.05374}, 
}

@misc{chang2023,
      title={A Survey on Evaluation of Large Language Models}, 
      author={Yupeng Chang and Xu Wang and Jindong Wang and Yuan Wu and Linyi Yang and Kaijie Zhu and Hao Chen and Xiaoyuan Yi and Cunxiang Wang and Yidong Wang and Wei Ye and Yue Zhang and Yi Chang and Philip S. Yu and Qiang Yang and Xing Xie},
      year={2023},
      eprint={2307.03109},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.03109}, 
}

@inproceedings{zadenoori2025,
  title={Automatic Prompt Engineering: The Case of Requirements Classification},
  author={Zadenoori, Mohammad Amin and Zhao, Liping and Alhoshan, Waad and Ferrari, Alessio},
  booktitle={International Working Conference on Requirements Engineering: Foundation for Software Quality},
  pages={217--225},
  year={2025},
  organization={Springer}
}

@book{Wieringa2014,
  title = {Design Science Methodology for Information Systems and Software Engineering},
  ISBN = {9783662438398},
  url = {http://dx.doi.org/10.1007/978-3-662-43839-8},
  DOI = {10.1007/978-3-662-43839-8},
  publisher = {Springer Berlin Heidelberg},
  author = {Wieringa,  Roel J.},
  year = {2014}
}

@article{zhang2011,
title = {Identifying relevant studies in software engineering},
journal = {Information and Software Technology},
volume = {53},
number = {6},
pages = {625-637},
year = {2011},
note = {Special Section: Best papers from the APSEC},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2010.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950584910002260},
author = {He Zhang and Muhammad Ali Babar and Paolo Tell},
keywords = {Search strategy, Quasi-gold standard, Systematic literature review, Evidence-based software engineering},
abstract = {Context
Systematic literature review (SLR) has become an important research methodology in software engineering since the introduction of evidence-based software engineering (EBSE) in 2004. One critical step in applying this methodology is to design and execute appropriate and effective search strategy. This is a time-consuming and error-prone step, which needs to be carefully planned and implemented. There is an apparent need for a systematic approach to designing, executing, and evaluating a suitable search strategy for optimally retrieving the target literature from digital libraries.
Objective
The main objective of the research reported in this paper is to improve the search step of undertaking SLRs in software engineering (SE) by devising and evaluating systematic and practical approaches to identifying relevant studies in SE.
Method
We have systematically selected and analytically studied a large number of papers (SLRs) to understand the state-of-the-practice of search strategies in EBSE. Having identified the limitations of the current ad-hoc nature of search strategies used by SE researchers for SLRs, we have devised a systematic and evidence-based approach to developing and executing optimal search strategies in SLRs. The proposed approach incorporates the concept of ‘quasi-gold standard’ (QGS), which consists of collection of known studies, and corresponding ‘quasi-sensitivity’ into the search process for evaluating search performance.
Results
We conducted two participant–observer case studies to demonstrate and evaluate the adoption of the proposed QGS-based systematic search approach in support of SLRs in SE research.
Conclusion
We report their findings based on the case studies that the approach is able to improve the rigor of search process in an SLR, as well as it can serve as a supplement to the guidelines for SLRs in EBSE. We plan to further evaluate the proposed approach using a series of case studies on varying research topics in SE.}
}

@article{Abualhaija2024,
author = {Abualhaija, Sallam and Aydemir, F. Basak and Dalpiaz, Fabiano and Dell'Anna, Davide and Ferrari, Alessio and Franch, Xavier and Fucci, Davide},
title = {Replication in Requirements Engineering: The NLP for RE Case},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3658669},
doi = {10.1145/3658669},
abstract = {Natural language processing (NLP) techniques have been widely applied in the requirements engineering (RE) field to support tasks such as classification and ambiguity detection. Despite its empirical vocation, RE research has given limited attention to replication of NLP for RE studies. Replication is hampered by several factors, including the context specificity of the studies, the heterogeneity of the tasks involving NLP, the tasks’ inherent hairiness, and, in turn, the heterogeneous reporting structure. To address these issues, we propose a new artifact, referred to as ID-Card, whose goal is to provide a structured summary of research papers emphasizing replication-relevant information. We construct the ID-Card through a structured, iterative process based on design science. In this article: (i) we report on hands-on experiences of replication; (ii) we review the state-of-the-art and extract replication-relevant information: (iii) we identify, through focus groups, challenges across two typical dimensions of replication: data annotation and tool reconstruction; and (iv) we present the concept and structure of the ID-Card to mitigate the identified challenges. This study aims to create awareness of replication in NLP for RE. We propose an ID-Card that is intended to foster study replication but can also be used in other contexts, e.g., for educational purposes.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {151},
numpages = {33},
keywords = {Requirements Engineering (RE), Natural Language Processing (NLP), replication, tool reconstruction, annotation, ID card}
}

@ARTICLE{Basili1999,
  author={Basili, V.R. and Shull, F. and Lanubile, F.},
  journal={IEEE Transactions on Software Engineering}, 
  title={Building knowledge through families of experiments}, 
  year={1999},
  volume={25},
  number={4},
  pages={456-473},
  keywords={Software engineering;Buildings;Design for experiments;Testing;Computer science;Mathematical model;Computer Society;Context modeling;Organizing;Software measurement},
  doi={10.1109/32.799939}}

@inproceedings{Vegas2006,
author = {Vegas, Sira and Juristo, Natalia and Moreno, Ana and Solari, Mart\'{\i}n and Letelier, Patricio},
title = {Analysis of the influence of communication between researchers on experiment replication},
year = {2006},
isbn = {1595932186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1159733.1159741},
doi = {10.1145/1159733.1159741},
abstract = {The replication of experiments is a key undertaking in SE. Successful replications enable a discipline's body of knowledge to grow, as the results are added to those of earlier replications. However, replication is extremely difficult in SE, primarily because it is difficult to get a setting that is exactly the same as in the original experiment. Consequently, changes have to be made to the experiment to adapt it to the new site. To be able to replicate an experiment, information also has to be transmitted (usually orally and in writing) between the researchers who ran the experiment earlier and the ones who are going to replicate the experiment. This article examines the influence of the type of communication there is between experimenters on how successful a replication is. We have studied three replications of the same experiment in which different types of communication were used.},
booktitle = {Proceedings of the 2006 ACM/IEEE International Symposium on Empirical Software Engineering},
pages = {28–37},
numpages = {10},
keywords = {replication, experimentation, empirical studies},
location = {Rio de Janeiro, Brazil},
series = {ISESE '06}
}


@article{Shrout2018,
   author = "Shrout, Patrick E. and Rodgers, Joseph L.",
   title = "Psychology, Science, and Knowledge Construction: Broadening Perspectives from the Replication Crisis", 
   journal= "Annual Review of Psychology",
   year = "2018",
   volume = "69",
   number = "Volume 69, 2018",
   pages = "487-510",
   doi = "https://doi.org/10.1146/annurev-psych-122216-011845",
   url = "https://www.annualreviews.org/content/journals/10.1146/annurev-psych-122216-011845",
   publisher = "Annual Reviews",
   issn = "1545-2085",
   type = "Journal Article",
   keywords = "statistics",
   keywords = "methodology",
   keywords = "replication",
   abstract = "Psychology advances knowledge by testing statistical hypotheses using empirical observations and data. The expectation is that most statistically significant findings can be replicated in new data and in new laboratories, but in practice many findings have replicated less often than expected, leading to claims of a replication crisis. We review recent methodological literature on questionable research practices, meta-analysis, and power analysis to explain the apparently high rates of failure to replicate. Psychologists can improve research practices to advance knowledge in ways that improve replicability. We recommend that researchers adopt open science conventions of preregi-stration and full disclosure and that replication efforts be based on multiple studies rather than on a single replication attempt. We call for more sophisticated power analyses, careful consideration of the various influences on effect sizes, and more complete disclosure of nonsignificant as well as statistically significant findings.",
  }

@article{Maxwell2015,
  title={Is psychology suffering from a replication crisis? What does "failure to replicate" really mean?},
  author={Scott E. Maxwell and Michael Yan-Kiat Lau and George S. Howard},
  journal={The American psychologist},
  year={2015},
  volume={70 6},
  pages={
          487-98
        },
  url={https://api.semanticscholar.org/CorpusID:11690879}
}

@article{Amrhein2018,
  title={Inferential Statistics as Descriptive Statistics: There Is No Replication Crisis if We Don’t Expect Replication},
  author={Valentin Amrhein and David Trafimow and Sander Greenland},
  journal={The American Statistician},
  year={2018},
  volume={73},
  pages={262 - 270},
  url={https://api.semanticscholar.org/CorpusID:56130495}
}

@article{Shull2008,
  title = {The role of replications in Empirical Software Engineering},
  volume = {13},
  ISSN = {1573-7616},
  url = {http://dx.doi.org/10.1007/s10664-008-9060-1},
  DOI = {10.1007/s10664-008-9060-1},
  number = {2},
  journal = {Empirical Software Engineering},
  publisher = {Springer Science and Business Media LLC},
  author = {Shull,  Forrest J. and Carver,  Jeffrey C. and Vegas,  Sira and Juristo,  Natalia},
  year = {2008},
  month = jan,
  pages = {211–218}
}

@article{Juristo2010,
  title = {The role of non-exact replications in software engineering experiments},
  volume = {16},
  ISSN = {1573-7616},
  url = {http://dx.doi.org/10.1007/s10664-010-9141-9},
  DOI = {10.1007/s10664-010-9141-9},
  number = {3},
  journal = {Empirical Software Engineering},
  publisher = {Springer Science and Business Media LLC},
  author = {Juristo,  Natalia and Vegas,  Sira},
  year = {2010},
  month = aug,
  pages = {295–324}
}

@article{Shepperd2018,
title = {The role and value of replication in empirical software engineering results},
journal = {Information and Software Technology},
volume = {99},
pages = {120-132},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917304305},
author = {Martin Shepperd and Nemitari Ajienka and Steve Counsell},
keywords = {Software engineering, Experiment, Reliability, Replication, Meta-analysis},
abstract = {Context
Concerns have been raised from many quarters regarding the reliability of empirical research findings and this includes software engineering. Replication has been proposed as an important means of increasing confidence.
Objective
We aim to better understand the value of replication studies, the level of confirmation between replication and original studies, what confirmation means in a statistical sense and what factors modify this relationship.
Method
We perform a systematic review to identify relevant replication experimental studies in the areas of (i) software project effort prediction and (ii) pair programming. Where sufficient details are provided we compute prediction intervals.
Results
Our review locates 28 unique articles that describe replications of 35 original studies that address 75 research questions. Of these 10 are external, 15 internal and 3 internal-same-article replications. The odds ratio of internal to external (conducted by independent researchers) replications of obtaining a ‘confirmatory’ result is 8.64. We also found incomplete reporting hampered our ability to extract estimates of effect sizes. Where we are able to compute replication prediction intervals these were surprisingly large.
Conclusion
We show that there is substantial evidence to suggest that current approaches to empirical replications are highly problematic. There is a consensus that replications are important, but there is a need for better reporting of both original and replicated studies. Given the low power and incomplete reporting of many original studies, it can be unclear the extent to which a replication is confirmatory and to what extent it yields additional knowledge to the software engineering community. We recommend attention is switched from replication research to meta-analysis.}
}

@inproceedings{carver2010,
  author    = {Carver, Jeffrey C.},
  title     = {Towards Reporting Guidelines for Experimental Replications: A Proposal},
  booktitle = {Proceedings of the 1st International Workshop on Replication in Empirical Software Engineering Research (RESER)},
  year      = {2010},
  month     = {May},
  day       = {4},
  address   = {Cape Town, South Africa},
  note      = {Held during ICSE 2010}
}

@article{Cruz2020,
  title={Replication of Studies in Empirical Software Engineering: A Systematic Mapping Study, From 2013 to 2018},
  author={Margarita Cruz and Beatriz Bern{\'a}rdez and Amador Dur{\'a}n and Jos{\'e} A. Galindo and Antonio Ruiz-Cort{\'e}s},
  journal={IEEE Access},
  year={2020},
  volume={8},
  pages={26773-26791},
  url={https://api.semanticscholar.org/CorpusID:209073520}
}

@INPROCEEDINGS{Siegmund2015,
  author={Siegmund, Janet and Siegmund, Norbert and Apel, Sven},
  booktitle={2015 IEEE/ACM 37th IEEE International Conference on Software Engineering}, 
  title={Views on Internal and External Validity in Empirical Software Engineering}, 
  year={2015},
  volume={1},
  number={},
  pages={9-19},
  keywords={Software engineering;Guidelines;Bibliographies;Computer languages;Context;Standards;History},
  doi={10.1109/ICSE.2015.24}}


@article{Brandt2014,
title = {The Replication Recipe: What makes for a convincing replication?},
journal = {Journal of Experimental Social Psychology},
volume = {50},
pages = {217-224},
year = {2014},
issn = {0022-1031},
doi = {https://doi.org/10.1016/j.jesp.2013.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0022103113001819},
author = {Mark J. Brandt and Hans IJzerman and Ap Dijksterhuis and Frank J. Farach and Jason Geller and Roger Giner-Sorolla and James A. Grange and Marco Perugini and Jeffrey R. Spies and Anna {van 't Veer}},
keywords = {Replication, Statistical power, Research method, Pre-registration, Solid Science},
abstract = {Psychological scientists have recently started to reconsider the importance of close replications in building a cumulative knowledge base; however, there is no consensus about what constitutes a convincing close replication study. To facilitate convincing close replication attempts we have developed a Replication Recipe, outlining standard criteria for a convincing close replication. Our Replication Recipe can be used by researchers, teachers, and students to conduct meaningful replication studies and integrate replications into their scholarly habits.}
}

@inproceedings{Shepperd2018b,
author = {Shepperd, Martin},
title = {Replication studies considered harmful},
year = {2018},
isbn = {9781450356626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183399.3183423},
doi = {10.1145/3183399.3183423},
abstract = {Context: There is growing interest in establishing software engineering as an evidence-based discipline. To that end, replication is often used to gain confidence in empirical findings, as opposed to reproduction where the goal is showing the correctness, or validity of the published results.Objective: To consider what is required for a replication study to confirm the original experiment and apply this understanding in software engineering.Method: Simulation is used to demonstrate why the prediction interval for confirmation can be surprisingly wide. This analysis is applied to three recent replications.Results: It is shown that because the prediction intervals are wide, almost all replications are confirmatory, so in that sense there is no `replication crisis', however, the contributions to knowledge are negligible.Conclusion: Replicating empirical software engineering experiments, particularly if they are under-powered or under-reported, is a waste of scientific resources. By contrast, meta-analysis is strongly advocated so that all relevant experiments are combined to estimate the population effect.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {73–76},
numpages = {4},
keywords = {software engineering, replication, evidence, empirical study},
location = {Gothenburg, Sweden},
series = {ICSE-NIER '18}
}

@article{Gomez2014,
title = {Understanding replication of experiments in software engineering: A classification},
journal = {Information and Software Technology},
volume = {56},
number = {8},
pages = {1033-1048},
year = {2014},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914000858},
author = {Omar S. Gómez and Natalia Juristo and Sira Vegas},
keywords = {Software engineering, Experimental software engineering, Experimentation, Replication},
abstract = {Context
Replication plays an important role in experimental disciplines. There are still many uncertainties about how to proceed with replications of SE experiments. Should replicators reuse the baseline experiment materials? How much liaison should there be among the original and replicating experimenters, if any? What elements of the experimental configuration can be changed for the experiment to be considered a replication rather than a new experiment?
Objective
To improve our understanding of SE experiment replication, in this work we propose a classification which is intend to provide experimenters with guidance about what types of replication they can perform.
Method
The research approach followed is structured according to the following activities: (1) a literature review of experiment replication in SE and in other disciplines, (2) identification of typical elements that compose an experimental configuration, (3) identification of different replications purposes and (4) development of a classification of experiment replications for SE.
Results
We propose a classification of replications which provides experimenters in SE with guidance about what changes can they make in a replication and, based on these, what verification purposes such a replication can serve. The proposed classification helped to accommodate opposing views within a broader framework, it is capable of accounting for less similar replications to more similar ones regarding the baseline experiment.
Conclusion
The aim of replication is to verify results, but different types of replication serve special verification purposes and afford different degrees of change. Each replication type helps to discover particular experimental conditions that might influence the results. The proposed classification can be used to identify changes in a replication and, based on these, understand the level of verification.}
}

@misc{peeperkorn2024,
      title={Is Temperature the Creativity Parameter of Large Language Models?}, 
      author={Max Peeperkorn and Tom Kouwenhoven and Dan Brown and Anna Jordanous},
      year={2024},
      eprint={2405.00492},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.00492}, 
}

@inproceedings{Yu2024,
  series = {South American ’24},
  title = {Can LLMs Have a Fever? Investigating the Effects of Temperature on LLM Security},
  url = {http://dx.doi.org/10.46254/SA05.20240024},
  DOI = {10.46254/sa05.20240024},
  booktitle = {Proceedings of the International Conference on Industrial Engineering and Operations Management},
  publisher = {IEOM Society International},
  author = {Yu,  Chan Xing and James,  James and Si Yu,  Chan and David,  David and Hoe,  Chow Ban and Hui-Li Phyllis,  Poh},
  year = {2024},
  month = may,
  collection = {South American ’24}
}

@article{Pakdaman2015, title={Obtaining Well Calibrated Probabilities Using Bayesian Binning}, volume={29}, url={https://ojs.aaai.org/index.php/AAAI/article/view/9602}, DOI={10.1609/aaai.v29i1.9602}, abstractNote={ &lt;p&gt; Learning probabilistic predictive models that are well calibrated is critical for many prediction and decision-making tasks in artificial intelligence. In this paper we present a new non-parametric calibration method called Bayesian Binning into Quantiles (BBQ) which addresses key limitations of existing calibration methods. The method post processes the output of a binary classification algorithm; thus, it can be readily combined with many existing classification algorithms. The method is computationally tractable, and empirically accurate, as evidenced by the set of experiments reported here on both real and simulated datasets. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Pakdaman Naeini, Mahdi and Cooper, Gregory and Hauskrecht, Milos}, year={2015}, month={Feb.} }


@InProceedings{Woodworth2017,
  title = 	 {Learning Non-Discriminatory Predictors},
  author = 	 {Woodworth, Blake and Gunasekar, Suriya and Ohannessian, Mesrob I. and Srebro, Nathan},
  booktitle = 	 {Proceedings of the 2017 Conference on Learning Theory},
  pages = 	 {1920--1953},
  year = 	 {2017},
  editor = 	 {Kale, Satyen and Shamir, Ohad},
  volume = 	 {65},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {07--10 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v65/woodworth17a/woodworth17a.pdf},
  url = 	 {https://proceedings.mlr.press/v65/woodworth17a.html},
  abstract = 	 {We consider learning a predictor which is non-discriminatory with respect to a “protected attribute” according to the notion of “equalized odds” proposed by Hardt et al. (2016).  We study the problem of learning such a non-discriminatory predictor from a finite training set, both statistically and computationally.  We show that a post-hoc correction approach, as suggested by Hardt et al, can be highly suboptimal, present a nearly-optimal statistical procedure, argue that the associated computational problem is intractable, and suggest a second moment relaxation of the non-discrimination definition for which learning is tractable.}
}

@misc{zhu2024,
      title={PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts}, 
      author={Kaijie Zhu and Jindong Wang and Jiaheng Zhou and Zichen Wang and Hao Chen and Yidong Wang and Linyi Yang and Wei Ye and Yue Zhang and Neil Zhenqiang Gong and Xing Xie},
      year={2024},
      eprint={2306.04528},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.04528}, 
}

@unpublished{jurafsky2025,
  author =       "Daniel Jurafsky and James H. Martin",
  title =        "Speech and Language Processing: An Introduction to
                 Natural Language Processing, Computational Linguistics,
                 and Speech Recognition with Language Models",
  year =         "2025",
  url = {https://web.stanford.edu/~jurafsky/slp3/},
  note = "Online manuscript released January 12, 2025",
  edition =         "3rd",
  }

@misc{zhao2025,
      title={A Survey of Large Language Models}, 
      author={Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
      year={2025},
      eprint={2303.18223},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.18223}, 
}

@ARTICLE{shannon1948,
  author={Shannon, C. E.},
  journal={The Bell System Technical Journal}, 
  title={A mathematical theory of communication}, 
  year={1948},
  volume={27},
  number={3},
  pages={379-423},
  keywords={},
  doi={10.1002/j.1538-7305.1948.tb01338.x}}

@misc{Vaswani2017transformer,
  doi = {10.48550/ARXIV.1706.03762},
  url = {https://arxiv.org/abs/1706.03762},
  author = {Vaswani,  Ashish and Shazeer,  Noam and Parmar,  Niki and Uszkoreit,  Jakob and Jones,  Llion and Gomez,  Aidan N. and Kaiser,  Lukasz and Polosukhin,  Illia},
  keywords = {Computation and Language (cs.CL),  Machine Learning (cs.LG),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Attention Is All You Need},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@misc{kaplan2020scaling,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}

@misc{wei2022emergent,
      title={Emergent Abilities of Large Language Models}, 
      author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
      year={2022},
      eprint={2206.07682},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2206.07682}, 
}

@misc{bahdanau2016attention,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1409.0473}, 
}

@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@misc{lewis2019bart,
      title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}, 
      author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
      year={2019},
      eprint={1910.13461},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1910.13461}, 
}

@article{roberts2022t5x,
  url = {https://arxiv.org/abs/2203.17189},
  author = {Roberts, Adam and Chung, Hyung Won and Levskaya, Anselm and Mishra, Gaurav and Bradbury, James and Andor, Daniel and Narang, Sharan and Lester, Brian and Gaffney, Colin and Mohiuddin, Afroz and Hawthorne, Curtis and Lewkowycz, Aitor and Salcianu, Alex and van Zee, Marc and Austin, Jacob and Goodman, Sebastian and Soares, Livio Baldini and Hu, Haitang and Tsvyashchenko, Sasha and Chowdhery, Aakanksha and Bastings, Jasmijn and Bulian, Jannis and Garcia, Xavier and Ni, Jianmo and Chen, Andrew and Kenealy, Kathleen and Clark, Jonathan H. and Lee, Stephan and Garrette, Dan and Lee-Thorp, James and Raffel, Colin and Shazeer, Noam and Ritter, Marvin and Bosma, Maarten and Passos, Alexandre and Maitin-Shepard, Jeremy and Fiedel, Noah and Omernick, Mark and Saeta, Brennan and Sepassi, Ryan and Spiridonov, Alexander and Newlan, Joshua and Gesmundo, Andrea},
  title = {Scaling Up Models and Data with $\texttt{t5x}$ and $\texttt{seqio}$},
  journal={arXiv preprint arXiv:2203.17189},
  year = {2022},
}

@misc{cho2014encoderdecoder,
      title={On the Properties of Neural Machine Translation: Encoder-Decoder Approaches}, 
      author={Kyunghyun Cho and Bart van Merrienboer and Dzmitry Bahdanau and Yoshua Bengio},
      year={2014},
      eprint={1409.1259},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1409.1259}, 
}

@Inbook{Asadi2020encoderdecoder,
author="Asadi, Ahmad
and Safabakhsh, Reza",
editor="Pedrycz, Witold
and Chen, Shyi-Ming",
title="The Encoder-Decoder Framework and Its Applications",
bookTitle="Deep Learning: Concepts and Architectures",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="133--167",
abstract="The neural encoder-decoder frameworkEncoder-decoder framework has advanced the state-of-the-art in machine translationMachine translation significantly. Many researchers in recent years have employed the encoder-decoder based models to solve sophisticated tasks such as image/video captioning, textual/visual question answeringQuestion answering, and text summarization. In this work we study the baseline encoder-decoder frameworkEncoder-decoder framework in machine translation and take a brief look at the encoder structures proposed to cope with the difficulties of feature extraction. Furthermore, an empirical study of solutions to enable decoders to generate richer fine-grained output sentences is provided. Finally, the attention mechanismAttentionmechanism which is a technique to cope with long-term dependenciesLong-term dependencies and to improve the encoder-decoder performance on sophisticated tasks is studied.",
isbn="978-3-030-31756-0",
doi="10.1007/978-3-030-31756-0_5",
url="https://doi.org/10.1007/978-3-030-31756-0_5"
}

@misc{liu2019roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1907.11692}, 
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@misc{lan2020albert,
      title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations}, 
      author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
      year={2020},
      eprint={1909.11942},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.11942}, 
}

@misc{koroteev2021encoder,
      title={BERT: A Review of Applications in Natural Language Processing and Understanding}, 
      author={M. V. Koroteev},
      year={2021},
      eprint={2103.11943},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2103.11943}, 
}

@article{radford2018gpt,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={San Francisco, CA, USA}
}

@misc{openai2024gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{brown2020gpt3,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@misc{OpenAI2023chatgpt,
  author       = {OpenAI},
  title        = {ChatGPT},
  year         = {2023},
  note         = {Available at https://chat.openai.com/},
  howpublished = {\url{https://chat.openai.com/}}
}

@misc{touvron2023llama2,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.09288}, 
}

@misc{poldrack2023,
      title={AI-assisted coding: Experiments with GPT-4}, 
      author={Russell A Poldrack and Thomas Lu and Gašper Beguš},
      year={2023},
      eprint={2304.13187},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2304.13187}, 
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{brown2020fewshot,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@misc{ye2024ape,
      title={Prompt Engineering a Prompt Engineer}, 
      author={Qinyuan Ye and Maxamed Axmed and Reid Pryzant and Fereshte Khani},
      year={2024},
      eprint={2311.05661},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.05661}, 
}

@misc{zhou2023ape,
      title={Large Language Models Are Human-Level Prompt Engineers}, 
      author={Yongchao Zhou and Andrei Ioan Muresanu and Ziwen Han and Keiran Paster and Silviu Pitis and Harris Chan and Jimmy Ba},
      year={2023},
      eprint={2211.01910},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.01910}, 
}

@inproceedings{tam2024format,
    title = "Let Me Speak Freely? A Study On The Impact Of Format Restrictions On Large Language Model Performance.",
    author = "Tam, Zhi Rui  and
      Wu, Cheng-Kuang  and
      Tsai, Yi-Lin  and
      Lin, Chieh-Yen  and
      Lee, Hung-yi  and
      Chen, Yun-Nung",
    editor = "Dernoncourt, Franck  and
      Preo{\c{t}}iuc-Pietro, Daniel  and
      Shimorina, Anastasia",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track",
    month = nov,
    year = "2024",
    address = "Miami, Florida, US",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-industry.91/",
    doi = "10.18653/v1/2024.emnlp-industry.91",
    pages = "1218--1236",
    abstract = "Structured generation, the process of producing content in standardized formats like JSON and XML, is widely utilized in real-world applications to extract key output information from large language models (LLMs).This study investigates whether such constraints on generation space impact LLMs' abilities, including reasoning and domain knowledge comprehension. Specifically, we evaluate LLMs' performance when restricted to adhere to structured formats versus generating free-form responses across various common tasks. Surprisingly, we observe a significant decline in LLMs' reasoning abilities under format restrictions. Furthermore, we find that stricter format constraints generally lead to greater performance degradation in reasoning tasks."
}

@misc{long2025format,
      title={LLMs Are Biased Towards Output Formats! Systematically Evaluating and Mitigating Output Format Bias of LLMs}, 
      author={Do Xuan Long and Hai Nguyen Ngoc and Tiviatis Sim and Hieu Dao and Shafiq Joty and Kenji Kawaguchi and Nancy F. Chen and Min-Yen Kan},
      year={2025},
      eprint={2408.08656},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.08656}, 
}

@Article{Li2024format,
AUTHOR = {Li, Diya and Zhao, Yue and Wang, Zhifang and Jung, Calvin and Zhang, Zhe},
TITLE = {Large Language Model-Driven Structured Output: A Comprehensive Benchmark and Spatial Data Generation Framework},
JOURNAL = {ISPRS International Journal of Geo-Information},
VOLUME = {13},
YEAR = {2024},
NUMBER = {11},
ARTICLE-NUMBER = {405},
URL = {https://www.mdpi.com/2220-9964/13/11/405},
ISSN = {2220-9964},
ABSTRACT = {Large language models (LLMs) have demonstrated remarkable capabilities in document processing, data analysis, and code generation. However, the generation of spatial information in a structured and unified format remains a challenge, limiting their integration into production environments. In this paper, we introduce a benchmark for generating structured and formatted spatial outputs from LLMs with a focus on enhancing spatial information generation. We present a multi-step workflow designed to improve the accuracy and efficiency of spatial data generation. The steps include generating spatial data (e.g., GeoJSON) and implementing a novel method for indexing R-tree structures. In addition, we explore and compare a series of methods commonly used by developers and researchers to enable LLMs to produce structured outputs, including fine-tuning, prompt engineering, and retrieval-augmented generation (RAG). We propose new metrics and datasets along with a new method for evaluating the quality and consistency of these outputs. Our findings offer valuable insights into the strengths and limitations of each approach, guiding practitioners in selecting the most suitable method for their specific use cases. This work advances the field of LLM-based structured spatial data output generation and supports the seamless integration of LLMs into real-world applications.},
DOI = {10.3390/ijgi13110405}
}

@inproceedings{xia2024fofo,
    title = "{FOFO}: A Benchmark to Evaluate {LLM}s' Format-Following Capability",
    author = "Xia, Congying  and
      Xing, Chen  and
      Du, Jiangshu  and
      Yang, Xinyi  and
      Feng, Yihao  and
      Xu, Ran  and
      Yin, Wenpeng  and
      Xiong, Caiming",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.40/",
    doi = "10.18653/v1/2024.acl-long.40",
    pages = "680--699",
    abstract = "This paper presents FoFo, a pioneering benchmark for evaluating large language models' (LLMs) ability to follow complex, domain-specific formats, a crucial yet under-examined capability for their application as AI agents. Despite LLMs' advancements, existing benchmarks fail to assess their format-following proficiency adequately. FoFo fills this gap with a diverse range of real-world formats and instructions, developed through an AI-Human collaborative method. Our evaluation across both open-source (e.g., Llama 2, WizardLM) and closed-source (e.g., GPT-4, PALM2, Gemini) LLMs highlights three key findings: open-source models significantly lag behind closed-source ones in format adherence; LLMs' format-following performance is independent of their content generation quality; and LLMs' format proficiency varies across different domains. These insights suggest the need for specialized tuning for format-following skills and highlight FoFo{'}s role in guiding the selection of domain-specific AI agents. FoFo will be publicly released, contributing a critical tool for advancing LLM evaluation and application."
}

@article{petersen2008map,
	author = {Petersen, Kai and Feldt, Robert and Mujtaba, Shahid and Mattsson, Michael},
	year = {2008},
	month = {06},
	pages = {},
	title = {Systematic Mapping Studies in Software Engineering},
	volume = {17},
	journal = {Proceedings of the 12th International Conference on Evaluation and Assessment in Software Engineering}
}

@article{meyes2019ablation,
	title={Ablation studies in artificial neural networks},
	author={Meyes, Richard and Lu, Melanie and De Puiseau, Constantin Waubert and Meisen, Tobias},
	journal={arXiv preprint arXiv:1901.08644},
	year={2019}
}

@article{huang2005auc,
	title={Using AUC and accuracy in evaluating learning algorithms},
	author={Huang, Jin and Ling, Charles X},
	journal={IEEE Transactions on knowledge and Data Engineering},
	volume={17},
	number={3},
	pages={299--310},
	year={2005},
	publisher={IEEE}
}

@inproceedings{ling2003auc,
	title={AUC: a better measure than accuracy in comparing learning algorithms},
	author={Ling, Charles X and Huang, Jin and Zhang, Harry},
	booktitle={Conference of the canadian society for computational studies of intelligence},
	pages={329--341},
	year={2003},
	organization={Springer}
}

@article{lobo2008auc_mis,
	title={AUC: a misleading measure of the performance of predictive distribution models},
	author={Lobo, Jorge M and Jim{\'e}nez-Valverde, Alberto and Real, Raimundo},
	journal={Global ecology and Biogeography},
	volume={17},
	number={2},
	pages={145--151},
	year={2008},
	publisher={Wiley Online Library}
}

@article{Nahm2022roc,
	title = {Receiver operating characteristic curve: overview and practical use for clinicians},
	volume = {75},
	ISSN = {2005-7563},
	url = {http://dx.doi.org/10.4097/kja.21209},
	DOI = {10.4097/kja.21209},
	number = {1},
	journal = {Korean Journal of Anesthesiology},
	publisher = {The Korean Society of Anesthesiologists},
	author = {Nahm,  Francis Sahngun},
	year = {2022},
	month = feb,
	pages = {25–36}
}

@inproceedings{nixon2019calibration,
	title={Measuring calibration in deep learning.},
	author={Nixon, Jeremy and Dusenberry, Michael W and Zhang, Linchuan and Jerfel, Ghassen and Tran, Dustin},
	booktitle={CVPR workshops},
	volume={2},
	number={7},
	year={2019}
}

@inproceedings{guo2017calibration,
	title={On calibration of modern neural networks},
	author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
	booktitle={International conference on machine learning},
	pages={1321--1330},
	year={2017},
	organization={PMLR}
}

@misc{sclar2024quantifyinglanguagemodelssensitivity,
	title={Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting}, 
	author={Melanie Sclar and Yejin Choi and Yulia Tsvetkov and Alane Suhr},
	year={2024},
	eprint={2310.11324},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2310.11324}, 
}

@misc{mizrahi2024stateartmultipromptllm,
	title={State of What Art? A Call for Multi-Prompt LLM Evaluation}, 
	author={Moran Mizrahi and Guy Kaplan and Dan Malkin and Rotem Dror and Dafna Shahaf and Gabriel Stanovsky},
	year={2024},
	eprint={2401.00595},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2401.00595}, 
}

@misc{polo2024multiprompt,
	author = {Maia Polo, Felipe and Xu, Ronald and Weber, Lucas and Silva, Mírian and Bhardwaj, Onkar and Choshen, Leshem and Oliveira, Allysson and Sun, Yuekai and Yurochkin, Mikhail},
	year = {2024},
	month = {05},
	pages = {},
	title = {Efficient multi-prompt evaluation of LLMs},
	doi = {10.48550/arXiv.2405.17202}
}