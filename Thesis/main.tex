\documentclass[a4paper]{article}

\usepackage{graphicx,float} % Required for inserting images
\usepackage[skins]{tcolorbox}
%\usepackage{apacite}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{parskip} 
\usepackage{tabto}
\usepackage{adjustbox} % Scaling
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{hhline}
\graphicspath{ {figures/} }

% Fonts
\usepackage{times} 

\usepackage{caption}  % fine control over caption appearance
\captionsetup{labelfont=bf,singlelinecheck=false,labelsep=space,skip=2pt}

% Citation style
\usepackage[bibstyle=ieee,citestyle=numeric-comp]{biblatex}
\addbibresource{Sources.bib}
\addbibresource{MappingStudy.bib}

% Macros
%\newcommand{\emphasize}[1]{\begin{quote} \textbf{#1} \end{quote}}
\newcommand{\emphasize}[1]{\textbf{#1}}
\newcommand{\newecser}{\textit{ECSER-LLM} }

\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage[toc,page]{appendix}
%\newcommand{\comment}[1]{}
\makeatletter
\newcommand*{\centerfloat}{%
  \parindent \z@
  \leftskip \z@ \@plus 1fil \@minus \textwidth
  \rightskip\leftskip
  \parfillskip \z@skip}
\makeatother

\newcommand{\infobox}[2]{
	\begin{tcolorbox}[
		skin=widget,
		boxrule=1mm,
		coltitle=black,
		colframe=gray,
		colback=lightgray,
		width=(\linewidth),
		before=\hfill,
		after=\hfill,
		before skip=1pt,
		adjusted title={#1}
		]
		#2
	\end{tcolorbox}
}

\newcommand\MyBox[2]{
	\fbox{\lower0.75cm
		\vbox to 1.7cm{\vfil
			\hbox to 1.7cm{\hfil\parbox{1.4cm}{#1\\#2}\hfil}
			\vfil}%
	}%
}

% Start content
\title{\includegraphics[width=0.2\textwidth]{figures/UUlogo.png} 
\vspace{0.5cm}
\hrule
\vspace{0.5cm}
\textbf{Enhancing the ECSER pipeline: Evaluating Large Language Model Classifiers in SE Research}\\
\vspace{0.5cm}
\hrule
}
\author{Ruben van der Zeijden (7081111) \\ \\
\textit{Supervisor:} Dr. F.B. Aydemir 
\\ \textit{Second Reader:} Dr. D. Dell'Anna \\
}
\begin{document}
\date{}
\maketitle
\begin{center}
\begin{abstract}
The various research papers in the field of Software Engineering (SE) that use classification algorithms, LLMs, or other machine learning methods to obtain their results differ in how many and which evaluation metrics are reported, whether or not significance tests are performed, and which steps are taken to aid reproducibility. The ECSER pipeline was designed to mitigate this issue by providing a step-by-step pipeline that researchers can use to report their results when classification algorithms are used, and the pipeline was empirically shown to be effective in replicating the findings of several studies, as well as producing additional findings and occasionally contradicting the findings of the original papers. However, the ECSER pipeline is designed for evaluating simple classifiers and gives no specific recommendations for LLM classifiers, despite LLMs being an increasingly popular choice for classification tasks. 
The goal of this thesis is to expand the ECSER pipeline for the use of LLMs by adding recommendations from LLM4SE research and related fields. First, an exploratory mapping study will be done of SE studies released in 2024 that use LLM classifiers, summarising which steps are taken and which metrics are (or aren't) reported, in order to get an overview of the current state of LLM4SE research. Subsequently, we design the \newecser pipeline, an enhanced version of the ECSER pipeline for the use of LLMs, including recommendations for prompt engineering, evaluating the fairness and robustness of models, and more. Lastly, in order to evaluate the comprehensiveness and ease-of-use of the new pipeline, two replication studies were conducted using the pipeline to test its ability to strengthen or contradict the findings of the original papers.
\end{abstract}
\vfill 
\textbf{MSc. Artificial Intelligence}\\
\textbf{30EC} \\ 
\footnotesize{The Ethics and Privacy Quick Scan of the Utrecht University Research Institute of Information and Computing Sciences classified this research as low-risk with no fuller ethics review or privacy assessment required.}
\vspace{0.5cm}
\end{center}

\newpage
\tableofcontents

\newpage

\section{Introduction}
% Introduction of the topic, talk about ECSER pipeline (sources: aydemir, kitchenham)
In recent years, large language models (LLMs) have become an increasingly relevant tool in software engineering (SE), allowing researchers to accomplish complex tasks usually reserved for humans or other machine learning (ML) methods, such as code generation, summarization and classification. However, few guidelines exist for conducting and reporting on LLM research, leaving researchers on their own to decide what choices to make when designing prompts, what evaluation metrics to report in their results and the level of statistical validation needed for their comparisons. Because of these lack of guidelines, researchers using LLMs often omit important details, such as the F1 score or the significance of their results \cite{guo2023survey}. This problem is not new or unique to LLMs; many researchers have reported that there is a poor standard of empirical software engineering, which inhibits the usefulness of results and impairs reproducibility \cite{kitchenham2002,Menzies2012}.

To treat the observed poor standard of empirical SE, some researchers have called for a more systematic approach to conducting and evaluating experiments. The ECSER (Evaluating Classifiers in Software Engineering) pipeline, introduced by \textcite{Dellanna2022}, provides a systemic pipeline for training, validating and testing ML classifiers and analysing their results. ECSER is based on recommendations from the field of machine learning for software engineering (ML4SE) and is designed to be easy to use for SE researchers, regardless of expertise with machine learning methods. The ECSER pipeline has seen some empirical validation in the form of two replication studies, in requirements engineering and software testing, where \citeauthor{Dellanna2022} were able to confirm and strengthen some findings, as well as discover additional findings or findings that contradict the original ones. Figure \ref{fig:ECSER} shows the steps of the ECSER pipeline.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/ECSER.png}
    \caption{The ECSER pipeline. Reproduced from \textcite{Dellanna2022}. Steps with a dashed border are optional. The $\leftrightarrow$ arrows indicate feedback loops between phases}
    \label{fig:ECSER}
\end{figure}


% What is the ECSER pipeline missing (LLMs) / Problems
However, while ECSER is tailored to traditional ML classifiers (such as logistic regression, SVMs and decision trees), it includes no specific recommendations for LLM-based classifiers, despite LLMs showing promise for various classification tasks \cite{Guo2024health,fields2024}. Thus, the ECSER pipeline is not sufficient for LLM research, since using LLMs for classification comes with unique considerations that are not present for traditional classifiers. For one, it is common to use pre-trained, generalised LLMs and give task-specific instructions to get the desired classification on the input data. This means that designing these instructions, also called prompts, becomes a crucial part to conducting LLM research \cite{Marvin2024}. There are also unique considerations when it comes to evaluating the fairness and robustness of the results: does the model behave in accordance with human values and are the results resilient to adversarial prompts, such as those containing typos \cite{Woodworth2017,zhu2024}?


% Contributions that will be made
The goal of this thesis is to address the lack of LLM-specific guidelines in the ECSER pipeline, by expanding it to include specific recommendations for conducting LLM classifier research. In doing so, we made the following contributions:
\begin{itemize}
    \item We conducted a mapping study to assess the current state of reporting in LLM4SE classification research.
    \item We developed the \newecser pipeline, an expanded version of the ECSER pipeline that includes specific recommendations for conducting LLM classifier research, such as prompt engineering and the evaluation of model fairness and robustness.
    \item We conducted two replication studies of existing LLM classifier papers using the \newecser pipeline, in order to evaluate the pipeline's applicability and comprehensiveness.
\end{itemize}

Supplemental materials, including all code that was used, the full intermediary and final results and the source for this document can be found at:\\ \url{https://github.com/rubenvdz/Master-Thesis-Enhancing-the-ECSER-pipeline}.

% Structure of the thesis
The remainder of this thesis is structured as follows. In Section \ref{Background}, we provide background information about LLMs. Section \ref{Research approach} describes the research approach and research questions. Lastly, Section \ref{Related Work} discusses related work.

\section{Background}\label{Background}
% LLMs (what are LLMs, types of LLMs, prompt engineering)
\textbf{Language Models (LMs)} are computational models designed to model the generative likelihood of word sequences \cite{zhao2025}. In other words, given a sequence of input text, they predict the next word in the sequence based on what is deemed most likely by the model. One of the oldest examples of language models are N-grams, which model the likelihood of a word given the previous N tokens \cite{shannon1948}. Early statistical models like N-gram models suffer from several problems, including limited context and difficulty with unseen words.

\textbf{Pre-trained Language Models (PLMs)} are LMs that are designed to take in vast amounts of unannotated text in order to learn knowledge about language and the world \cite{jurafsky2025}. The knowledge that is contained in these models can then be used on new tasks by way of \textbf{transfer learning} \cite{wang2023plm}. PLMs require much more information to be contained within than statistical language models such as N-grams could provide, which has led researchers to design more sophisticated models that rely on neural networks to model language, starting with recurrent neural networks (RNNs) and later Long Short-Term Memory (LSTM) models \cite{mikolov2012rnn,hochreiter1997lstm}. The biggest recent breakthrough came with the invention of \textbf{transformers}, which performed better than previous models, were easier to train and allowed for the parallel processing of inputs \cite{Vaswani2017transformer}. Transformers typically consist of an \textbf{encoder}, which embeds the input sequence into a hidden (latent) space and a \textbf{decoder}, which translates the abstract representation of the hidden space to the target text. Both the encoder and decoder use a self-attention mechanism to weigh the contribution of each token to the output \cite{Vaswani2017transformer,bahdanau2016attention}.

\textbf{Large Language Models (LLMs)} were introduced as a way to refer to PLMs with massive parameter sizes, since it was found that larger parameter sizes lead to better performance and emergent abilities \cite{kaplan2020scaling,wei2022emergent}. Almost all LLMs use transformer models with the self-attention mechanism, but not all of them use both the encoder and decoder component \cite{hou2024}:
\begin{itemize}
    \item \textbf{Encoder-decoder LLMs}, such as BART \cite{lewis2019bart} and T5X \cite{roberts2022t5x}, are good at both understanding and generating language \cite{hou2024}. As such they are useful for tasks like summarization and translation \cite{cho2014encoderdecoder, Asadi2020encoderdecoder}.
    \item \textbf{Encoder-only LLMs}, such as BERT and its variants \cite{devlin2019bert,liu2019roberta,sanh2019distilbert,lan2020albert}, are tailored towards language understanding \cite{hou2024}. As such they are good at tasks like classification and inference \cite{koroteev2021encoder}. 
    \item \textbf{Decoder-only LLMs}, such as the GPT-series \cite{radford2018gpt,brown2020gpt3,openai2024gpt4}, are tailored towards language generation \cite{hou2024}. As such they are good at all generation tasks, including code generation \cite{poldrack2023} and creating conversational agents like ChatGPT \cite{OpenAI2023chatgpt} and LLama 2 \cite{touvron2023llama2}.
\end{itemize}




\textbf{Prompt Engineering} is the process of systematically designing natural language instructions (or \textbf{prompts}) to guide LLMs towards a desired output \cite{ronanki2023}. Prompts condition the outputs of the LLM on the given instructions, so that the model generates the next tokens with both the prompt and previously generated tokens in mind \cite{jurafsky2025}. The most common approaches to prompt engineering are:
\begin{itemize}
    \item \textbf{Zero-shot Prompting}. The model is given no examples for a task, forcing it to rely entirely on the prompt \cite{radford2019language}.
    \item \textbf{Few-shot Prompting}. A limited number of examples for a task are given, which the model can learn from \cite{brown2020fewshot}.
    \item \textbf{Chain-of-Thought Prompting}. The prompt instructs the model to follow coherent reasoning steps \cite{wei2022chain}.
\end{itemize}
Another less common approach is automatic prompt engineering, where instructions are automatically generated and selected \cite{zhou2023ape}.

\section{Research Questions} \label{Research approach} 

% Research questions
The usage of LLMs in SE research has increased massively over the past few years, but the research of how to best conduct studies and report results in this field remains very limited. In order to approach the problem of LLM4SE reporting standards and design meaningful guidelines, we follow \citeauthor{Wieringa2014}'s design cycle of \textit{problem investigation}, \textit{treatment design} and \textit{treatment validation} \cite{Wieringa2014}. This leads us to the following research questions:

\emphasize{RQ1: What is the current state of reporting in LLM4SE classification research?} \\
RQ1 follows the step of \textit{problem investigation} and is both descriptive and evaluative: what do researchers report in their studies and does this level of reporting contribute or detract from the accuracy and reproducibility of the results. The aim of this question is to provide insights into the potential shortcomings of existing research by conducting a mapping study of the reporting practices of LLM4SE research, in order to provide a basis for additional guidelines that help researchers avoid these shortcomings. 


\emphasize{RQ2: What resource can be developed to assist researchers in conducting and reporting on LLM4SE classification research?}\\
Following the problem investigation of RQ1, RQ2 fulfils the step of \textit{treatment design}. The purpose of RQ2 is to design a tangible artefact that is comprehensive and widely applicable to the field of LLM4SE, with the intent of helping researchers avoid any problems identified in RQ1. This research question was designed to guide the modification of the existing ECSER pipeline \cite{Dellanna2022}, taking into consideration recommendations from the fields of LLM4SE, ML4SE and Design Science.


\emphasize{RQ3: How applicable is the proposed pipeline in assisting LLM4SE classification research?}\\
The purpose of RQ3 is \textit{treatment validation}: does the designed pipeline in RQ2 achieve its goals of applicability and comprehensiveness. To answer this question we will conduct two replication studies in different subfields of SE that employ LLM classification using the guidelines by \textcite{carver2010}. The original studies will be chosen based on the following criteria:
\begin{enumerate}
    \item The authors provide a ready-to-use replication package.
    \item The study is recent (past five years) and represents current practices in LLM4SE well.
    \item The study is peer-reviewed and published in a reputable journal.
    \item The study does not already follow all the proposed guidelines, so that the added benefit of the pipeline can be evaluated.
\end{enumerate}

By conducting these replication studies, we can evaluate the comprehensiveness of the pipeline and identify potential shortcomings. After the first replication study, the findings from RQ3 were taken into account to make changes to the guidelines from RQ2 where needed, creating a feedback-loop between RQ2 and RQ3. The second replication was conducted using the updated pipeline.

\section{Related Work} \label{Related Work}
The main focus of this thesis is to study the usage and evaluation of LLM Classifiers in Software Engineering, but in doing so we touch on several related research fields, including Empirical Software Engineering, Machine Learning for Software Engineering (ML4SE), Large Language Models for Software Engineering (LLM4SE) and Prompt Engineering. Additionally, each of the three research questions has related work that is relevant to their methodologies. Hence, we have split the related work into several sections based on different areas of study to aid readability. 

First, we discuss challenges in empirical software engineering which motivated the creation of the ECSER pipeline and further motivated our mapping study of LLM4SE. Second, we discuss the topic of replication in SE, which is relevant to our replication studies and the field of empirical SE. Third, we discuss how Large Language Models have been used in SE, with a focus on classification tasks. Fourth, we discuss related work in Prompt Engineering, as it has become a vital part to using LLMs and informed our guidelines for prompt engineering in the \newecser pipeline. Finally, we discuss related work on the evaluation of LLMs and classifiers informed our guidelines for the evaluation of LLM Classifiers.

\subsection{Challenges in Empirical Software Engineering}
% Related work on evaluating "simple" classifiers in SE (Dell'ana 2022, Kitchenham 2004, Kitchenhham 2002, Menzies 2012, Zhang 2002)
Over the past couple decades, the use of machine learning methods has become ubiquitous in every field of science, among them software engineering. \textcite{Zhang2002} summarized seven main activities that use machine learning in SE: prediction, property/model discovery, transformation, generation, library construction and maintenance, acquisition of specifications and development knowledge management. In addition, they list specific tasks belonging to each activity, such as \textit{predicting} software quality and \textit{generating} test cases. The field of classification research falls almost exclusively in the prediction category.

With the increased popularity of machine learning methods, many software packages have been developed to allow anyone to use these methods without having to write any code or even understand how these methods work. This leads to many researchers using machine learning without fully understanding the underlying assumptions of the methods they are using and assuming the results are correct without proper (statistical) analysis. This problem is not new: \textcite{kitchenham2002} reported at the start of this century that there was a poor standard of empirical software engineering, mostly caused by a lack of statistical expertise from both the authors and reviewers. They go on to provide extensive guidelines for designing and conducting experiments as well as analysing, presenting and interpreting results. In another paper, \textcite{kitchenham2004} discuss the benefits of evidence-based software engineering approach by drawing an analogy to evidence-based medicine, but note that the infrastructure needed for widespread adoption of evidence-based SE isn't there yet, which means that SE experiments are vulnerable to subject and experimenter bias. 

Methodological problems in SE research can result in problems with the reproducibility of results, which severely reduces the real-world usefulness of said results. \textcite{Menzies2012} argue that the main goal of science is \textit{conclusion stability}, which means that when you discover an effect, it holds true irrespective of the situation. However, they note that in the software engineering literature there is often as much evidence \textit{for} a given effect, as \textit{against}. This lack of conclusion stability is caused by sources of bias and variance that are introduced at various steps: sampling, pre-processing, training, algorithm selection and the analysis of results. If these sources of bias and variance are accurately analysed and reported, that would make it easier for the authors and other researchers to interpret the conclusions.

Despite being a known issue, the problems of improper analysis and reporting have persisted over the years. In a mapping study of the proceedings of the International Conference of Software Engineering (ICSE) from 2019 to 2021, \textcite{Dellanna2022} analysed 60 SE papers related to classification and found that most papers (36) reported on the precision and recall of their experiment, fewer reported the F-score (27) and accuracy (24), but only 14 papers explicitly justified why they chose the reported metrics. The full confusion matrix was only included in six papers. Other metrics like the receiver operator characteristic (ROC) curve and the area under the curve (AUC) were mostly absent with 3 and 9 respective mentions. Of course, as we know from statistics, it is not enough to report a metric and claim it's better than that of other models, since it could be coincidental, so statistical significance testing is required to make such claims. However, only six papers analysed the statistical significance of their results. Evidently, the situation is dire, which is why \citeauthor{Dellanna2022} developed the ECSER pipeline to give SE researchers a systematic approach to conducting classification experiments and reporting and analysing the results. 

\subsection{Replications in SE}
Replication studies are a vital part of empirical software engineering, allowing researchers to investigate the effects of alternative values for important attributes, vary the strategy with which hypotheses are investigated and make up for certain threats to validity \cite{Basili1999}. The importance of replication is not unique to software engineering, after all the so-called "replication crisis" was a massive topic of debate in psychology research in recent years, since it was found that many previous results in the field could not be replicated, even if the original findings were statistically significant \cite{Shrout2018,Maxwell2015,Amrhein2018}. \textcite{Cruz2020} conducted a systematic literature review of replication studies in SE between 2013 and 2018 and found that the number of published replication studies has been steadily increasing, though there are research gaps in certain fields, suggesting that the SE community is taking an increased interest in replicating previous studies.

\textcite{Shull2008} identify two types of replication studies: \textit{exact replications}, in which the procedures of the original study are strictly followed; and \textit{conceptual replications}, in which the research problem of the original study is evaluated using a different procedure. Additionally, exact replications are subdivided into \textit{dependent} and \textit{independent} replications. Dependent replications keep most of the conditions of the experiment (close to) the same, which allows one to test the effect of specific variables on the results; whereas independent replications vary the original conditions of the experiment significantly, which allows one to test the robustness of the original results under different conditions. \textcite{Juristo2010} underline the value of non-exact replications, which they define similarly to how \citeauthor*{Shull2008} define independent exact replications, by conducting a multiple-case replication study using a newly proposed four-phase process, to show that non-identical replications can be used to learn new information about the original results.

While there is a consensus in SE that replication is important, \textcite{Shepperd2018} show that replication studies often suffer from the same problems as original studies, such as incomplete reporting, low power or potential researcher bias. Notably, many replication studies in their survey did not report any details on the dispersion (e.g., variance) of the response variables. In another paper, \textcite{Shepperd2018b} shows that because of wide prediction intervals, almost all replications are confirmatory, which means the added knowledge is negligible. Because of these problems, they suggest that researchers should focus more on broader meta-analyses instead of replication studies. Other common problems in conducting replication research include difficulty in getting them published, lack of guidelines and the unavailability of replication packages \cite{Cruz2020,Siegmund2015}.

Communication can also play a factor in successful replication: \textcite{Vegas2006} investigated the role of communication in successful replication of previous experiments and found that there should be some level of communication, orally or in writing, between the previous authors and authors of the replication study, to avoid unnecessary changes to the experiments. 

Given these challenges, guidelines for conducting replications are clearly needed. Despite this, the number of existing guidelines remains low. Guidelines were proposed by \textcite{carver2010}, which are still commonly used in replications, but these seem to be the only replication guidelines for software engineering and were intended as a starting point for future guidelines. Other artifacts do exist to aid replications: \textcite{Gomez2014} proposed a classification of replications in SE, differentiating between literal, operational and conceptual replications, that can be used to identify which changes can be made in each type of replication and understand the level or verification needed for that type. 

\textcite{Abualhaija2024} proposed an artifact, referred to as the ID-Card, for summarizing papers in RE research that use NLP techniques. The intention behind the ID-Card is that all the relevant information needed for replication is presented in an easy to read format and can be used for both replication and educational purposes. Lastly, \textcite{Brandt2014} developed the "Replication Recipe" for psychology research, which lists 36 questions that should be addressed when conducting a replication, most of which are also directly applicable to empirical SE. 

\subsection{Large Language Models for SE}
% Related work on LLMs in SE in general (Wang 2024, Fan 2023, find more for specific subdomains?)
A recent development in SE research is the increased relevance and use of LLMs. \textcite{fan2023} looked at all preprints on arXiv categorised under computer science whose title included "LLM", "Large Language Model" or "GPT" and found the number of SE papers that mentioned LLMs grew exponentially from 0 in 2019, 5 in 2020, to 181 in 2023. Naturally, this is only an approximation of the total number of preprints on arXiv that use LLMs, but it undoubtedly signifies a substantial increase. \citeauthor{fan2023} also conducted an extensive survey of how LLMs have been applied to various software development activities (such as code generation and testing) and research domains (such as human-computer interaction and education), but the survey is almost exclusively focused on the generative use of LLMs and does not go into depth on the use of LLMs in classification research. Many other good surveys and analyses of the use of LLMs in Software Engineering have been written, but most of them similarly do not mention any applications to classification tasks and instead exclusively focus on generation \cite{Belzner2024,Zheng2024,Ozkaya2023}.

Surveys have also been written about specific domains of software engineering: \textcite{wang2024} did a comprehensive review of 102 studies that use LLMs for software testing and found that LLMs have commonly been applied to various tasks, including test case generation, test input generation, debugging, program repair and more. They also found that while most papers used LLMs to address the entire task, many others combined LLMs with additional techniques to optimize the outputs of the LLM, including mutation testing, differential testing, syntactic checking, program analysis, statistical analysis and other techniques. With these extra techniques, researchers were able to generate more diverse and complex code and overcome some of the limitations of LLMs. 

% Related work on LLMs in CLASSIFICATION (Hou 2024, Zhang 2024, Fields 2024)
While LLMs are most frequently used to generate text (and code), they have seen considerable success in classification tasks. \textcite{Guo2024health} have shown that LLMs can outperform other methods like SVMs for health-related text classification. \textcite{fields2024} present an in-depth survey of text classification using transformers across domains and found that LLMs can perform remarkably well on many (but not all) classification tasks. However, \textcite{Chen2024} compared the performance of various LLMs to traditional ML methods in clinical prediction and found that LLMs could \textit{not} beat traditional methods in this case. \textcite{vajjala2025} also show that there are large performance disparities between languages in classification tasks. Thus, LLMs might not always be a better choice than traditional ML methods, but they certainly show potential. 

Software engineers have also started using LLMs for classification. \textcite{hou2024} conducted a systematic literature review of 395 software engineering studies that use LLMs and found that around 21.61\% involve classification tasks. The most common classification tasks where LLMs have been used include vulnerability detection, requirements classification, bug prediction and review/commit/code classification, with many other classification tasks having been attempted using LLMs. \textcite{zhang2024} conducted a systematic survey of 947 SE studies and summarized 62 unique LLMs of code, including six LLMs specifically fine-tuned for the tasks of code classification, clone detection and defect detection.

% Related work on GUIDELINES
Recently, \textcite{baltes2025guidelinesempiricalstudiessoftware} presented a list of community-driven guidelines for using and reporting on LLMs, providing eight detailed guidelines. These guidelines include: (1) declaring LLM usage and role; (2) reporting model versions, configurations and fine-tuning; (3) documenting tool architectures; (4) disclosing prompts and interaction logs; (5) using human validation; (6) employing an open LLM as baseline; (7) using suitable baselines, benchmarks and metrics; and (8) openly articulating limitations and mitigations. These guidelines were designed to increase the transparency and reproducibility of LLM research and are actively maintained on \url{https://llm-guidelines.org/}. The guidelines in this thesis serve a similar purpose of helping researchers conduct LLM4SE research; however, their guidelines have a broader focus, since they include all LLM applications, whereas our guidelines are specifically tailored to classification research. Furthermore, unlike their paper, this thesis also includes a mapping study to assess the state of LLM4SE classification reporting and provides a replication study to evaluate the guidelines. Despite the difference in scope, many of their recommendations directly apply to our research and were taken into account for the \newecser pipeline where relevant.

\subsection{Prompt Engineering} \label{Prompt Engineering}
% Prompt engineering (Hou 2024, Arvidsson 2023, Ronanki 2023, Ekin 2023, White 2023, Marvin 2024)
Designing a good prompt for a particular task is vital to ensuring the adequate performance of LLMs, which is why much research has gone into the systematic designing of prompts, or prompt engineering. \textcite{Marvin2024} provide an overview of prompt engineering principles and techniques and outline the main steps involved in the process of prompt engineering: 1) define the goal of the prompt; 2) understand the model's capabilities; 3) choose the right prompting format; 4) provide context to the LLM and 5) test and refine the prompt based on the goal. Several resources exist to make it easier to choose the right prompting approach: \textcite{white2023} introduce prompt patterns, which offer reusable solutions to common prompting challenges, and provide a framework for designing and documenting these patterns in terms of the intent, motivation, structure and consequences of the patterns. For example, the persona pattern can be used to make the LLM take the point of view of an expert in the field, which is especially useful if the user itself is not an expert. The paper provides a catalogue of this pattern and other successfully applied patterns with example implementations. Further, \textcite{ekin2023} provides an accessible (AI-generated) guide to prompt engineering.

Prompt engineering is an important consideration when using LLMs: \textcite{sclar2024quantifyinglanguagemodelssensitivity} show that choices in prompt design, even if minor, can have a very large impact on the performance of several popular LLMs, and this sensitivity is present regardless of the size of the model or the number of examples provided. Because of this, they argue that researchers should forego using only a single prompt and switch to reporting the performance of their models using multiple plausible prompt patterns. They present a novel algorithm, FormatSpread, to evaluate multiple prompts at the same time and measure the sensitivity of the LLM. \textcite{mizrahi2024stateartmultipromptllm} have found a similar sensitivity of LLMs to prompt design and discuss metrics to evaluate multiple prompts. Lastly, \textcite{polo2024multiprompt} propose PromptEval, an efficient method for evaluating a large number of prompts, and show it can accurately estimate the performance distribution of the prompts. 

Prompt engineering research has also been conducted specifically for software engineering: \textcite{hou2024} looked at what prompt engineering techniques are commonly applied in SE tasks and found the most common techniques involve few-shot prompting and zero-shot prompting, but the third largest group of studies had no explicit mention of prompting techniques or proposed their own strategies. Other techniques included chain-of-thought, automatic prompt engineering, chain-of-code, automatic chain-of-thought, modular-of-thought and structured chain-of-thought.

\textcite{arvidsson2023} collected prompt engineering recommendations for requirements engineering (RE) in a systematic literature review and classified the guidelines into various themes, after which they interviewed three RE experts to evaluate the guidelines. \textcite{ronanki2023} evaluated the effectiveness of 5 prompting patterns (from the catalogue by \textcite{white2023}) on two RE tasks and proposed a framework for evaluating the effectiveness of prompting patterns for any RE task, providing five steps for conducting a comparison of patterns.

A recent development in prompt engineering is automatic prompt engineering (APE), which has shown good performance compared to baseline models, while avoiding the need for manually designing prompts \cite{zhou2023ape,ye2024ape}. \textcite{zadenoori2025} investigated the use of automatic prompt engineering in RE and found that, on average, APE outperforms traditional prompt engineering techniques. However, research is still limited.

\subsection{Evaluating LLM Classifiers} \label{Evaluation of LLM Classifiers}
% Evaluation of classifiers (ECSER, papers mentioned in ECSER)
Few papers have been written specifically about the evaluation of LLM-based classifiers, but we can look at the evaluation of classifiers and LLMs separately to get a picture. The ECSER pipeline is the most relevant for evaluating classifiers, since it gives in-depth recommendations about the conducting and reporting of classifier research \cite{Dellanna2022}. Specific recommendations include the reporting of the full confusion matrix, reporting other metrics relevant to the domain such as specificity (true negative rate), analysing overfitting and degradation, visualising the ROC and applying statistical significance tests. Some researchers also recommend reporting the Matthews correlation coefficient (MCC), which combines all four metrics of the confusion matrix and can be more informative than the F1 score \cite{Chicco2020,Foody2023,Yao2020}. 

\textcite{hou2024} looked at whether these metrics were reported in LLM-based classification research and found that out of 147 papers that use LLMs for classification, the most frequently reported metric is precision with 35 papers, followed by recall (34), F1-score (33) and accuracy (23), but the AUC was reported 9 times, the ROC only 4 times and the MCC only twice. Unfortunately, they did not count how many studies did significance testing. These results are very similar to \textcite{Dellanna2022}'s aforementioned mapping study of classifier research as a whole and suggest that many important metrics are under-reported in LLM-based classification research. 

% Evaluation of LLMs/LLM classifiers (guo 2023, chang 2023) 
Using and evaluating LLMs comes with more considerations than just the accuracy of the results. \textcite{guo2023survey} categorize the evaluation of LLMs into three types: knowledge and capability evaluation, which assesses the fundamental knowledge and reasoning capabilities of the LLM; alignment evaluation, which refers to evaluating ethical and moral considerations like bias; and safety evaluation, which focuses on the robustness of LLMs and risk evaluation. \textcite{liu2024} have created guidelines specifically for evaluating LLM alignment in terms of reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms and robustness. 

Another consideration is how well the LLM outputs adhere to the format specified in the prompt. For example, it is common to ask an LLM to format its answer as a valid JSON object, allowing for easier processing of the output. Limiting responses to a specified format can be beneficial: \textcite{tam2024format} compared the performance of several LLMs between free-form response and formatted responses and found that, while reasoning ability was weakened for formatted responses, classification accuracy was increased. However, LLMs do not always follow the format accurately. \textcite{long2025format} define several measures to analyse the level of adherence to the output format: SysE, which measures the performance of the LLM for answers that \textit{meet the constraint}; TrueE, which measures the performance of all answers \textit{regardless of whether the format is satisfied}; and BiasF, which measures the (mean squared) difference between SysE and TrueE. \textcite{Li2024format} introduce the JScore measure, which measures the similarity between JSON objects and can be used to compare LLM generated JSON objects with the target JSON objects. Lastly, \textcite{xia2024fofo} introduce the FoFo benchmark for evaluating LLMs based on their ability to follow complex, domain-specific formats. 

\textcite{chang2023} conducted another survey of the evaluation of LLMs which focuses on downstream applications, resulting in a classification of four aspects of LLM evaluation: the accuracy, containing measures of correctness such as the F1 score; the calibration, which contains measures for the degree of alignment between the predicted probabilities and actual probabilities, such as the expected calibration error (ECE); the fairness, with measures pertaining to the equal treatment of different groups, such as the equalized odds difference (EOD); and robustness, with measures pertaining to the performance of a model in the face of challenging inputs and noise, such as performance drop rate (PDR). Further, they mention several ways in which human evaluation can be used to evaluate LLMs, such as the degree to which the model outputs align with human values.

Lastly, the model temperature, which regulates the randomness of the model, can also affect its performance. \textcite{peeperkorn2024} looked at the effect of model temperature on the level of creativity of the outputs and found that LLMs generate slightly more novel outputs with higher temperature, but the temperature was also correlated with incoherence. No relationship was found between temperature and cohesion or typicality. Model temperature might also affect a model's susceptibility to security attacks: \textcite{Yu2024} found that some LLMs became more susceptible to jailbreaking attacks as temperature increased, whereas others had decreased susceptibility to jailbreaking with increased temperature, possibly due to the fact that the former models had a lower susceptibility at 0 temperature and the latter models had a higher susceptibility at 0 temperature.


\section{RQ1: What is the current state of reporting in LLM4SE classification research?}
\label{RQ1}

In this section, we first discuss the methodology used to conduct a systematic mapping study on the state of reporting of LLM4SE classification research. Subsequently, we discuss the results from the mapping study and draw conclusions from these results.

\subsection{Research Approach}
To answer the research question, we conducted a mapping study of SE papers published in 2024 that use LLM classifiers using the guidelines by \citeauthor{kitchenham2011} \cite{kitchenham2011,kitchenham2007} and \textcite{petersen2008map}, consisting of five steps:
\begin{enumerate}
	\item Define the research question.
	\item Conduct a search for primary studies.
	\item Screen papers based on inclusion/exclusion criteria.
	\item Classify the papers.
	\item Extract data. 
\end{enumerate}

What follows is a detailed summary of the search strategy (steps 2 and 3) and the information that was obtained from the list of relevant papers (steps 4 and 5). 

\textbf{Search Strategy}\\
To get a complete image of the state of LLM4SE classifier evaluation, we collected all 528 papers published in 2024 in three of the top SE conferences and journals (Table \ref{table:venues}). These venues were narrowed down from the list of venues included in \cite{hou2024} based on their impact and relevance. Using the full list of 528 papers, an automatic keyword search was conducted on the title and abstract of each paper to identify the papers that were potentially relevant to LLM4SE.

The list of keywords was based on previous research (\cite{zhang2024,hou2024}) and designed with the intent of high recall, since it is easier to remove false positives than to retrieve false negatives. Keyword matching was done on the lowercased versions of the title and abstract and allowed for partial matches (e.g., "llm" matches "LLM", "LLMs", "LLM4SE", etc.). The complete list of keywords is as follows:
\begin{itemize}
	\item \textit{llm, large language model, language model, code generation model, plm, pre-trained, pretrained, pre-training, pretraining, chatgpt, gpt, bert, t5, llama, bart.}
\end{itemize}

The keyword search resulted in 129 potentially relevant papers. After manually removing false positives, mostly caused by the authors comparing their non-LLM model to previous LLM models, the total number of identified LLM4SE papers was 116, representing 21.97\% of total SE papers. 

To narrow down the list from all LLM-related papers to those pertaining to SE classification research, the title and abstract of the remaining 116 papers were manually screened for the inclusion and exclusion criteria (Table \ref{table:criteria}). These criteria were designed to exclude any paper not relevant for answering the research question, such as meta studies and those not related to classification. During the manual search, 18 papers were identified that use LLMs for SE classification research and satisfy all other criteria. 

\begin{table}[H]
	\raggedright
	\caption{Publication venues included in the mapping study.}
	\begin{tabularx}{\textwidth}{|c|X|} \hline
		\textbf{Acronym} & \textbf{Venue} \\ \hline
		ICSE  & International Conference on Software Engineering \\ \hline
		FSE   & International Conference on the Foundations of Software Engineering \\ \hline
		TSE   & Transactions on Software Engineering \\ \hline
	\end{tabularx}
	\label{table:venues}
\end{table}

\begin{table}[H]
	\caption{Inclusion/Exclusion Criteria}
	\begin{tabularx}{\textwidth}{|X|} \hline
		\textbf{Inclusion Criteria} \\ \hline
		The paper uses an LLM.  \\
		The paper focuses on a downstream application of the LLM. \\
		The downstream application is a SE classification task. \\
		The full text of the paper is accessible. \\
		The paper was published in 2024. \\ \hline
		\textbf{Exclusion Criteria} \\ \hline
		The paper focuses on LLM architecture with no downstream application. \\
		The paper or task is not relevant to SE. \\
		The full text is inaccessible. \\
		The paper is a survey or other meta-analysis. \\
		The paper focuses on research methodology. \\
		The paper is a reprint or different version of another paper. \\ \hline
	\end{tabularx}
	
	\label{table:criteria}
\end{table}

While only classification papers were included in the final analysis, all the papers that applied LLMs to SE tasks (a total of 83 papers) were additionally classified based on the type of LLM application (classification, generation or recommendation) to get a view of the relative academic interest in each type (Figure \ref{fig:LLMTypes}). As can be seen, LLMs are most often used for generation, with 73.5\% of papers using them in this way, including some that use generation alongside classification. However, classification represents the second largest application of LLMs, with 22.9\% in total. Appendix \ref{appendix:ref} contains references for Figure \ref{fig:LLMTypes} and other pie charts contained in this thesis.

\begin{figure}[h]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=0.5\linewidth]{figures/LLMTypes.png}
	\caption{Distribution of LLM application types in collected studies.}
	\label{fig:LLMTypes}
\end{figure}


\textbf{Information Extraction}\\
After completing the manual search and obtaining 18 relevant LLM4SE classification papers, the evaluation and reporting practices of each paper were manually analysed and recorded. The information that was extracted from the papers was determined based on what is recommended to be included by the ECSER pipeline \cite{Dellanna2022}, metrics that are recommended by other authors (such as the Matthews correlation coefficient \cite{Chicco2020,Foody2023,Yao2020}) and recommendations from the field of LLM4SE. For the latter, we recorded which metrics (if any) were reported for each of \textcite{chang2023}'s classification of general metrics: the accuracy, calibration, fairness \& robustness. Since the accuracy was already covered by the ECSER pipeline, this resulted only in the addition of calibration, fairness \& robustness to the final list of extracted information. 

The calibration of an LLM refers to how well the predicted probabilities of the model align with actual probabilities \cite{nixon2019calibration,guo2017calibration}. Evaluating the fairness of an LLM is done to ensure the alignment of LLM results with human values and prevent a difference in performance across groups of people \cite{liu2024}. The robustness refers to the performance of a model in the face of challenging inputs and can be analysed in several ways, including by making changes in the data distribution, the addition of noise to the data, and adversarial attacks \cite{chang2023}.

In total, the following list of evaluation metrics or evaluation metric types were added: \textit{precision, recall, accuracy, F-score, Area under curve (AUC), Matthews correlation coefficient (MCC), Confusion matrix, Receiver Operation Characteristic (ROC) plot, Calibration, Fairness} and \textit{Robustness}.

Further, we looked at several factors related to the reproducibility and generalisation of results, particularly whether justification was given for why the chosen evaluation metrics were used, whether results were obtained for multiple datasets, whether the results were compared to external baselines and whether the statistical significance was analysed and the choice of significance test justified. 

Lastly, for the papers that used a prompt-based approach for classification, we analysed whether the prompt approach and final prompt text were reported and justified, whether the results were evaluated over multiple prompts and whether the temperatures of the models were reported.

\subsection{Mapping Study Results}
\begin{figure}[h]
	\hfill
	\begin{minipage}{0.45\textwidth}
		\includegraphics[width=1\textwidth]{figures/Venues.png}
		\caption{Distribution of venues in mapping study.}
		\label{fig:Venues}
	\end{minipage}
	\hfill
	\begin{minipage}{0.45\textwidth}
		\includegraphics[width=1\textwidth]{figures/Prompts.png}
		\caption{Distribution of prompt usage in mapping study.}
		\label{fig:Prompts}
	\end{minipage}
	\hfill
\end{figure}

During the search process, 18 papers were identified as relevant for answering the research question. Each venue included in the search strategy is represented in the final selection, with eight ICSE papers, eight TSE papers and two FSE papers (Figure \ref{fig:Venues}). By including papers from three of the most influential Software Engineering venues, we increase the likelihood that the results are representative of the field. 
	
One of the first things we analysed was whether the papers used a prompt-based approach to LLM classification or another approach such as fine-tuning, since additional information about the prompt engineering approach was extracted from the papers where prompts were used. Figure \ref{fig:Prompts} shows that 44.4\% or 8 papers used a prompt-based approach, while 55.6\% or 10 papers used a different approach. Note that additional references for the figures and the subsequent tables are included in Appendix \ref{appendix:ref}.


\begin{table}[H]
	\centering
	\captionsetup{justification=centering}
	\caption{Summary of mapping study results.}
	\begin{tabularx}{0.7\textwidth}{|X|c|}
		\hline
		\textbf{Category} & \textbf{Total} \\
		\hline
		Total mapping study papers & 18 \\
		\hline
		\textbf{Evaluation metrics} & \\
		\hspace{3mm} Precision & 17 (94.4\%)  \\
		\hspace{3mm} Recall & 17 (94.4\%) \\
		\hspace{3mm} Accuracy & 11 (61.1\%) \\
		\hspace{3mm} F-Score & 17 (94.4\%)\\
		\hspace{3mm} AUC & 3 (16.7\%) \\
		\hspace{3mm} ROC plots & 0 (0.0\%) \\
		\hspace{3mm} MCC & 0 (0.0\%) \\
		\textbf{Calibration} & 0 (0.0\%) \\
		\textbf{Fairness} &  0 (0.0\%) \\
		\textbf{Robustness} &  \\
		\hspace{3mm} Data distribution \& Adversarial Attacks & 1 (5.5\%)  \\
		\hspace{3mm} Noisy data & 1 (5.5\%) \\
		\textbf{Metrics justification} &\\
		\hspace{3mm} No & 9 (50.0\%) \\
		\hspace{3mm} Implicit & 0 (0.0\%) \\
		\hspace{3mm} Previous work & 5 (27.8\%) \\
		\hspace{3mm} Yes & 4 (22.2\%) \\
		Confusion matrix & 4 (22.2\%) \\
		Evaluation over multiple data sets & 5 (27.8\%) \\
		\textbf{Type of baseline} &\\
		\hspace{3mm} None & 2 (11.1\%) \\
		\hspace{3mm} Own & 3 (16.7\%) \\
		\hspace{3mm} External & 4 (22.2\%)\\
		\hspace{3mm} External and Own & 9 (50.0\%) \\
		Analysis of statistical significance & 5 (27.8\%) \\
		\textbf{Statistical significance justification}&\\
		\hspace{3mm} No & 4 (80.0\%)  \\
		\hspace{3mm} Implicit & 0 (0.0\%) \\
		\hspace{3mm} Previous work & 1 (20.0\%) \\
		\hspace{3mm} Yes & 0 (0.0\%)  \\
		\hline
	\end{tabularx}
	\label{table:mapping_results}
\end{table}

Table \ref{table:mapping_results} shows a summary of the information that was extracted from each paper and how many papers reported each piece of information. This includes which evaluation metrics were reported; whether the calibration, fairness and robustness of the models were analysed; whether the confusion matrix was reported; the level of justification for the reported metrics; whether the models were evaluated over multiple data sets; the type of baseline used; and whether the statistical significance of the results was reported and justified.

Starting with the evaluation metrics that are recommended by the ECSER pipeline \cite{Dellanna2022}, 17 papers reported the precision, recall and F-score of the results, while only 11 papers reported the accuracy and only four papers reported the full confusion matrix. Although less known, some authors also recommend reporting the Matthews Correlation Coefficient (MCC), since it takes the full confusion matrix into account and might therefore give a more balanced representation of the results than the F-score \cite{Chicco2020}, but it was not reported in any of the papers. Further, the AUC, which some researchers consider to be a theoretically and empirically better metric for comparing classification models than the accuracy \cite{huang2005auc,ling2003auc}, was reported in only three papers, while none provided a plot of the full ROC curve.
% While the precision, recall and F-score are often more informative for the usefulness of a model compared to the accuracy, by not providing the accuracy or the full confusion matrix, relevant information is lost that cannot be obtained from just three metrics, which prevents further comparisons between models that do provide the full matrix.
% Since the AUC is a summary of the ROC curve, the full curve should also be provided for a more detailed view of the discriminatory performance of the model \cite{Nahm2022roc}.

Apart from metrics related to the correctness of the classification results, we extracted three other types of LLM evaluation that are frequently used: the calibration, fairness and robustness \cite{chang2023}. None of the papers reported measures of the calibration or fairness of the LLMs.  Only two papers tested the robustness of their models in one or more of these ways; one looked at both altering the data distribution as well as adversarial attacks and the other looked at the effect of noisy data. However, some papers tested something similar to robustness by performing ablation studies, which work by removing parts of the model to see the impact on the performance, which allows one to assess whether the model exhibits graceful degradation \cite{meyes2019ablation}. Ablation studies were not initially included in the mapping study but after seeing their successful implementation were added to the \newecser pipeline as an optional part of evaluating the robustness.
%The presence or omission of evaluation metrics should be justified in some way, as well as the relative importance for answering the research question. For example, including or omitting the accuracy could be motivated by referring to previous work or an explanation as to why it is or isn't important for the given task. Additionally, the precision or recall can be argued to be more important for the given task than the other.

We also looked at whether justifications were provided in the papers for the selection of metrics they chose to include or exclude from the results and whether these justifications went beyond referring to previous research, which runs the risk of bad habits propagating through the field \cite{Dellanna2022}. Only nine papers included a justification for their choice of metrics, with five of those exclusively referring to previous research as a justification, while four provided a more elaborate explanation.

Lastly, we looked at methodological information about the evaluation and comparison of models and found that only five papers evaluated their models over multiple data sets. Further, 13 papers compared their results to external baselines (nine of which compared to both external and their own baselines), though three papers only compared the results to their own baselines and two papers made no comparisons. 

Despite almost all papers comparing their results to their own baselines or other research and usually making explicit claims about the superior performance of their models, only five papers did any statistical significance testing on their results. Some papers even claimed their results show a \textit{significant} improvement despite not doing or not reporting on the significance testing. Further, of the five papers that did a statistical analysis of the results, only one justified the statistical test that was chosen, by referring to previous research. 
%However, since there are often multiple statistical tests that can be applied with their own positives and negatives, it is better to explicitly justify the choice. It is further important to justify the significance threshold instead of relying on commonly used values, since the consequences of errors depend on the application. For example, the consequences of false positives might be worse in a medical context compared to a software engineering context.

\begin{table}[H]
	\centering
	\captionsetup{justification=centering}
	\caption{Summary of prompt reporting results in the mapping study.}
	\begin{tabularx}{0.7\textwidth}{|X|c|}
		\hline
		\textbf{Category} & \textbf{Total} \\
		\hline
		Total prompt-based classification papers & 8 \\
		\hline
		\textbf{Prompt approach} & \\
		\hspace{3mm} Chain-of-verification & 1 (12.5\%)  \\
		\hspace{3mm} Few-shot & 2 (25.0\%)  \\
		\hspace{3mm} Mimic-in-the-background & 1 (12.5\%) \\
		\hspace{3mm} Not reported & 1 (12.5\%) \\
		\hspace{3mm} Zero-shot & 3 (37.5\%)  \\
		\textbf{Final prompt} &\\
		\hspace{3mm} Not reported & 2 (25.0\%)\\
		\hspace{3mm} Own & 6 (75.0\%) \\
		\textbf{Prompt approach justification} &\\
		\hspace{3mm} No & 5 (62.5\%)  \\
		\hspace{3mm} Yes & 3 (37.5\%)  \\
		\textbf{Final prompt justification} & \\
		\hspace{3mm} No & 4 (50.0\%)  \\
		\hspace{3mm} Previous Work & 1 (12.5\%) \\
		\hspace{3mm} Yes & 3 (37.5\%)  \\
		\textbf{Evaluation over multiple prompts} &\\
		\hspace{3mm} No & 8 (100.0\%) \\
		\textbf{Temperature} & \\
		\hspace{3mm} No & 7 (87.5\%) \\
		\hspace{3mm} Yes & 1 (12.5\%) \\
		\hline
	\end{tabularx}
	\label{table:prompt_results}
\end{table}


Eight of the papers used prompting to obtain results from pre-trained LLMs, which comes with additional considerations and challenges, such as what prompt approach to use (few-shot, zero-shot, etc.) and how to effectively write the text of the prompt. Table \ref{table:prompt_results} shows a summary of the reported prompting information of the papers that used prompts in the mapping study. 

Firstly, papers used various prompt design approaches; one used chain-of-verification, two used few-shot, one used a novel approach called mimic-in-the-background, three used zero-shot prompting, and one paper failed to report the prompt approach used or didn't use a systematic approach. However, only three papers provided justification for the chosen prompt approach. 

Secondly, the final prompt, which refers to the actual prompt text used with possible placeholder values for data insertion, was only reported in six out of eight papers. Further, the authors of the six papers that provided the prompt wrote the prompts themselves without consulting established patterns, which means there is no prior evidence of the efficacy of the prompt pattern. Only four papers provided a justification for how the prompt was designed, one of which referred to previous work.

The performance of LLMs can be heavily impacted by minor changes in the prompt design, which is why \textcite{sclar2024quantifyinglanguagemodelssensitivity} recommend comparing multiple plausible prompts. However, none of the papers evaluated the performance of their models using multiple prompts, which leaves the vulnerability of the models to prompt variability unknown. Lastly, only one paper reported the temperature used for the LLM, despite this parameter having a potential impact on the performance of LLMs \cite{peeperkorn2024}.

\textbf{Conclusion}\\
Evidently, the results confirm a pattern of incomplete reporting and unsubstantiated claims similar to that found previously in the fields of ML4SE \cite{Dellanna2022} and LLM4SE \cite{hou2024}. Metrics such as the AUC, ROC plots, the MCC, the full confusion matrix and metrics related to the calibration, fairness and robustness of LLMs go completely or almost completely unreported.

Since none of the included papers seemingly involve human participants or sensitive applications, this could be a valid reason for not evaluating the fairness, but these considerations are not discussed in any of the papers, nor do the papers explain the absence of calibration and robustness metrics. 

Even when metrics \textit{are} reported, there is often no explanation as to the why the metrics were chosen for the specific tasks. Further, many papers only use one data set and sometimes make no comparisons to external baselines. Further, many papers make unsubstantiated claims about the relative performance of their models without significant statistical backing and when significance testing is performed, little explanation is given to the choice of test and significance threshold. 

For papers that use prompts for LLM classification, there is often little to no explanation for the chosen prompt approach and the design of the final prompt. Further, the prompt texts are generally not based on established patterns with empirical backing, but designed without a systematic approach. Lastly, none of the papers evaluated multiple prompts to avoid the potential sensitivity of LLMs to minor changes in wording, and the temperature of LLMs goes largely unreported as well. 

Naturally, although multiple venues were considered and all papers from 2024 were considered from these venues, these results are still based on a limited set of papers. However, the results align with findings from a large base of previous research \cite{Dellanna2022,hou2024,guo2023survey,kitchenham2002,Menzies2012}, so it is not unexpected to find similar problems here, especially since there are still very few guidelines for conducting LLM research.

In conclusion, there is a clear need for easy to use and comprehensive guidelines to aid researchers in conducting LLM4SE classification research, especially for statistical significance testing and prompt engineering. We designed the \newecser pipeline (section \ref{RQ2}) to take into account the mentioned common shortcomings of existing research and provide extra attention to these topics. 

\section{RQ2: What resource can be developed to assist researchers in conducting and reporting on LLM4SE classification research?}
\label{RQ2}
In this section, we first discuss the approach that was used to modify the ECSER pipeline with recommendations tailored to the conducting and reporting of LLM4SE classification research. Subsequently, we present the \newecser pipeline, which features a step by step guide to the implementation and results reporting of LLM-based classification research. 

\subsection{Research Approach}
To answer the research question, we developed the \newecser pipeline by enhancing the existing ECSER pipeline to include the use of LLMs, taking into consideration recommendations from the fields of LLM4SE and ML4SE, as well as relevant statistical or methodological research in other fields (see section \ref{Related Work}). 

The ECSER pipeline was chosen as the basis for the development of a new LLM-specific resource, since most ECSER recommendations apply to all classification research, including LLM-based classification \cite{Dellanna2022}. However, using LLMs comes with additional challenges that weren't considered during the development of the original pipeline, such as the use of prompts to guide pre-trained LLMs or the alignment of LLM behaviour with human values \cite{liu2024}. 

To enhance the ECSER pipeline with LLM-specific recommendations, we consulted various existing guidelines and surveys for using and evaluating LLMs, including \textcite{hou2024}'s systematic literature review of LLM4SE, \textcite{chang2023}'s survey on the evaluation of LLMs, \textcite{Marvin2024}'s chapter on prompt engineering and \textcite{baltes2025guidelinesempiricalstudiessoftware}'s guidelines for using LLMs in SE, among many others. These higher level sources lead to more specific sources and recommendations that were included in the pipeline.

Additionally, the results of RQ1 were taken into account to ensure that common issues encountered in the mapping study received additional care in the new pipeline, including issues such as the lack of reporting for common evaluation metrics, the lack of significance testing and the lack of consistent reporting for prompt usage and development. The results of RQ1 also lead to the inclusion of ablation studies as part of the the pipeline (S8), which were overlooked during the initial literature review but were shown to be useful in assessing the graceful degradation of the models in the papers that used them \cite{meyes2019ablation}.

In Table \ref{table:additions}, we list each step of the original ECSER pipeline along with the new version of the step and the major changes that were made in the new \newecser pipeline, along with the primary source(s) that motivated these changes. Minor changes are not included in the table.

\begin{table}[H]
	\centering
	\caption{Summary of changes and additions to the ECSER pipeline.}
	\begin{tabularx}{\textwidth}{|p{3cm}|p{3.5cm}|X|}
		\hline
		\textbf{\textit{ECSER} Step} & \textbf{\newecser Step} & \textbf{Changes from \textit{ECSER} to \newecser} \\ \hline
		
		S1. Select an evaluation method and split the data.
		& S1. Select an evaluation method and split the data.
		& Add specification of desired level of model alignment and safety \cite{guo2023survey}. \\ \hline
		
		\multirow[c]{2}{*}{\parbox{3cm}{S2. Train the model.}}
		& \multirow[c]{2}{*}{\parbox{3.5cm}{S2. Fine-tune the model}} 
		& Change focus to fine-tuning LLMs. \\ 
		& & Add reporting of LLM configuration and interaction \cite{baltes2025guidelinesempiricalstudiessoftware}. \\ \hline
		
		\multirow[c]{2}{*}{\parbox{3cm}{-}}
		& \multirow[c]{2}{*}{\parbox{3.5cm}{S3. Design Prompts.}}
		& Add prompt engineering guidelines \cite{arvidsson2023,Marvin2024}.\\
		& & Add reporting of LLM configuration and interaction \cite{baltes2025guidelinesempiricalstudiessoftware}. \\ \hline
		
		\multirow[c]{1}{*}{\parbox{3cm}{S3. Hyper-parameter tuning \& validation.}}
		& \multirow[c]{1}{*}{\parbox{3.5cm}{S4a. Hyper-parameter tuning}}
		& Tune LLM model temperature and other hyper-parameters \cite{peeperkorn2024, baltes2025guidelinesempiricalstudiessoftware}. \\ \hline
		
		\multirow[c]{2}{*}{\parbox{3cm}{-}}
		& \multirow[c]{2}{*}{\parbox{3.5cm}{S4b. Prompt comparison.}}
		& Compare similar prompts \cite{sclar2024quantifyinglanguagemodelssensitivity,ronanki2023}. \\ 
		& & Report prompt revisions and LLM interaction logs \cite{baltes2025guidelinesempiricalstudiessoftware} \\ \hline
		
		S4. Re-train with optimized parameters. 
		& -
		& Merged into S4a.\\ \hline
		
		S5. Test the models.
		& S5. Test the models.
		& - \\ \hline
		
		S6. Report confusion matrix.
		& S6. Report confusion matrix.
		& - \\ \hline
		
		S7. Report Metrics.
		& S7. Report Metrics.
		&  Report Matthews Correlation Coefficient (MCC) \cite{Chicco2020,Foody2023,Yao2020} \\ \hline

		\multirow[c]{2}{*}{\parbox{3cm}{-}}
		& \multirow[c]{2}{*}{\parbox{3.5cm}{S8. Evaluate calibration, fairness, robustness \& sustainability.}}
		& New step that includes guidelines on how to evaluate the calibration, fairness, robustness and sustainability of LLMs  \cite{chang2023,liu2024}. \\ & & Conduct ablation studies \cite{meyes2019ablation}. \\ \hline
		
		S8. Analyse overfitting and degradation.
		& S9. Analyse overfitting and degradation.
		& - \\ \hline
		
		S9. Visualize ROC.
		& S10. Visualize ROC.
		& - \\ \hline
		
		S10. Apply statistical significance tests.
		& S11. Apply statistical significance tests.
		& Justify the significance test and significance threshold \cite{benjamin2018significance,amrhein2017significance}. \\ \hline
	\end{tabularx}
	\label{table:additions}
\end{table}


%		\multirow[c]{4}{*}{\parbox{3.5cm}{S7. Report Metrics}}
%		& \multirow[c]{4}{*}{\parbox{4cm}{S7. Report Metrics}}
%		& Report Matthews correlation coefficient (MCC) \cite{Chicco2020,Foody2023,Yao2020} \\ 
%		& & Report calibration (e.g., expected calibration error \cite{Pakdaman2015}) \\ 
%		& & Report fairness (e.g., equalized odds difference \cite{Woodworth2017}) \\ 
%		& & Report robustness (e.g., performance drop rate \cite{zhu2024}) \\ \hline

\subsection{The \newecser Pipeline}
\label{ECSER-LLM}
In this section we present the \newecser pipeline, a step-by-step guideline for SE researchers to conduct classification experiments using LLMs and evaluate the results, which serves to provide an answer to the research question. The pipeline encompasses the \textit{treatment validation} step of \textcite{Wieringa2014}'s design cycle, which happens after the \textit{treatment design} step. Steps belonging to the \textit{treatment design}, such as feature engineering and algorithms selection, are out of scope of the pipeline. 

Figure \ref{fig:ECSER-LLM} shows the full pipeline, consisting of 11 steps divided into two macro activities: (1) training, validation and testing and (2) results analysis. The steps are shown consecutively, but it is always possible to return to previous steps. For example, after finding optimal hyper-parameters in S4, one will likely return to S2 to retrain the models with these parameters. Further, there is a possible feedback loop between macro activities, including the \textit{treatment design}. For example, when it is found that the model shows signs of overfitting (S9), one can reconsider the algorithms used (\textit{treatment design}) or increase regularization (S2).

Some steps are marked as optional with a dashed border, since they depend on the \textit{treatment design} or other considerations such as time and space constraints. In particular, when using a pre-trained LLM without fine-tuning, S2 (Fine-tuning) becomes obsolete since no training is performed, and when \textit{not} using prompts to guide LLM outputs, S3 (Design prompts) and S4b (Prompt comparison) become obsolete since they provide guidelines for using prompts. Lastly, S4 is split into two steps: (a) hyper-parameter tuning and (b) prompt comparison. This is done to indicate that these steps can be done in parallel to find the best combination of prompt and/or hyper-parameters, but since both are marked as optional it is possible to do only one of the steps or neither. 

\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=1\linewidth]{figures/ECSER-LLM.png}
	\caption{The \newecser pipeline. Dashed borders indicate optional steps. Double arrows show possible feedback loops between macro activities. The steps in S4. can be done in parallel.}
	\label{fig:ECSER-LLM}
\end{figure}

\subsubsection{Training, Validation, Testing}
\textbf{S1. Select an evaluation method and split the data.}\\
In experimental software engineering, the treatment that is designed to answer a research question generally consists of a model that must be trained using a data set, but how well the trained model fits the training data is not a good indicator of the performance of the model on unseen data, since the training data is only a sample of the true data distribution. Therefore, some of the available data must be set aside to test whether the trained model generalises well to data that wasn't encountered in the process of training the model; and in the case of hyper-parameter tuning, some of the data must be used to evaluate the performance of models with different parameters. 

When using a pre-trained LLM with no additional training, it is not needed to set aside data for training, but the data should still be split into validation and test sets. The validation set should be used to select hyper-parameters for the LLM, such as the temperature, and during the development and comparison of prompts \cite{Homiar2025promptdev}. The test set should only be used when the final prompt is developed (along with any other part of the model) to ensure the prompt design was not influenced by the test set. 

Several methods exists to partition data into training, validation and test sets. The simplest is the \textit{holdout method}, where the data is split into training, validation and test sets. The training set is used for fine-tuning (S2), the validation set is used for prompt development and hyper-parameter tuning (S3 \& S4), and the test set is used for evaluating the model (S5). Generally, at least 50-70\% of the data is used for training and validation, with the rest being used for testing \cite{Bichri2024traintest,xu2018splitting}. The holdout method does have a major issue: since only a portion of the training data is used for validation, the test error rate can be highly variable, leading to unreliable results \cite{kim2009holdout,James2023isl5}. 

A more robust alternative is \textit{resampling}, where models are fitted on multiple samples drawn from the training data. One example is \textit{k-fold cross-validation}, where the training set is split into \textit{k} groups (or folds) of equal size, chosen randomly from the data. The model is trained $k$ times, once for each fold, but each time a different fold is used as the validation set, with the remaining $k-1$ folds being used for training. The result of this process is $k$ estimates of the test error, the average of which can be used for hyper-parameter selection \cite{James2023isl5}. A special case of k-fold cross-validation is \textit{leave-one-out cross-validation} (LOOCV), where $k$ is set to $N$, with $N$ being the number of observations in the training set. This means that the model is trained $N$ times on $N-1$ observations, with a different observation being used as the validation set each time. LOOCV is more computationally expensive than k-fold cross-validation, although efficient algorithms exist for some applications that don't require training the model $N$ times \cite{Cawley2003,wang2018approximateloocv}. In terms of performance, LOOCV has a lower bias than k-fold cross-validation, at the cost of a higher variance \cite{Cawley2003,Wong2015}.

It is also possible to use projects to partition the data using \textit{leave-one-project-out cross-validation} (LOPO), where the training data is not split randomly, but grouped with all the data belonging to the same SE project. Each project is set aside in turn to serve as the validation set, while the other projects are used for training \cite{Herbold2020,Dalpiaz2019}. The advantage of LOPO is that a different project is used for validation than what is used for training, which possibly enhances the generalisability of the results \cite{scikit-learn-cv2025}.

Which of these methods is most appropriate might depend on the available computational power and time, with methods like LOOCV requiring much more training than the holdout method; and whether the data is obtained from distinct projects. In all cases, 10-30\% of the data should be set aside for testing and not touched until all parameters are tuned and prompts are finalised.

Finally, when using a pre-trained LLM, if the researcher uses open source data, there is a legitimate concern that the validation and testing sets were also used in training the LLM, leading to data leakage. LLMs have been shown to sometimes repeat training data verbatim  \cite{carlini2021extractingtrainingdatalarge}. If possible, this can be avoided by using testing data that was released after the creation of the LLM or proprietary data. \\

\textbf{S2. Fine-tune models.} \\ 
\infobox{Optional}{This step does not apply when using a pre-trained LLM without fine-tuning or additional training, such as research that only uses prompts to guide the output of the LLM.}
After splitting the data and deciding on the evaluation method, the training set is used to fine-tune the LLM or train a classification model on the LLM encodings. Training or fine-tuning a model entails changing the \textit{parameters} of the model to produce outputs that are most similar to the supervised labels in the training data. The training process is usually influenced by \textit{hyper-parameters} that are set before training and don't change during training. For example, when fine-tuning an LLM, the learning rate, batch size and epochs are examples of hyper-parameters that influence the training process, but the weights of the model are the parameters that are actually changed during training. 

Since LLMs are often updated after release, the exact LLM version, configuration and experiment dates should be reported to make it easier for other researchers to replicate findings \cite{baltes2025guidelinesempiricalstudiessoftware}. Additionally, when using fine-tuning, the fine-tuning data and weights should be shared whenever possible.

It should be noted that training a traditional classification model on LLM encodings also falls under the scope of the original ECSER pipeline \cite{Dellanna2022}, which deals exclusively with traditional classification models. However, additional information should still be included about the way the LLM is used, its version, experiment dates and configuration (including seed); and the additional evaluation of calibration, fairness, robustness \& sustainability should still be considered when using LLM encodings.

See \textcite{parthasarathy2024hyperUltimate} for a detailed guide to fine-tuning LLMs.\\


\textbf{S3. Design prompts.} \\ 
\infobox{Optional}{This step only applies when using prompts to guide LLM outputs, instead of using only fine-tuning or using LLM embeddings with traditional classifiers.}
Prompt engineering is an important part of interacting with LLMs, since the prompt design can have a substantial effect on the performance of the LLM. We split the process of designing a prompt into five steps, based on \textcite{Marvin2024}.

1. \textit{Define the goal of the prompt}: the first step of prompt engineering is to clearly define the goal of a prompt. Setting a goal for the type of output that you desire the LLM to produce helps inform the content of the prompt \cite{Marvin2024}. For example, having the goal of producing requirement classification results that resemble those of a human expert might inform the choice to add a stipulation to the prompt: "act like an expert in requirements engineering". 

2. \textit{Choose the right prompt engineering approach}: there are many approaches to prompt engineering, but the most notable are zero-shot prompting, few-shot prompting and chain-of-thought prompting. Of these, the most popular are few-shot and zero-shot in that order. When providing examples in the prompts, these examples should be removed from the validation set before testing and refining the prompt or comparing different prompts (S4b). A relatively novel approach to prompt engineering is automatic prompt engineering (APE), which avoids the problem of manually having to write, test and refine prompts by automatically generating prompts \cite{zhou2023ape,zadenoori2025}. 

3. \textit{Design the prompt}: writing a high quality prompt is an often overlooked aspect of prompt engineering, with most researchers writing prompts from scratch without potentially putting much thought into the exact wording, despite small changes in wording sometimes having a large effect on the performance \cite{sclar2024quantifyinglanguagemodelssensitivity}. Some resources exist that make it easier to write high quality prompts, such as \textcite{white2023}'s catalogue of prompt patterns. These patterns represent techniques that have been successfully used to solve common problems, put into reusable formats. For example, the Persona pattern tells the LLM to take up a certain role (e.g., "act as a software engineering researcher"). Some suitable patterns for binary classification are Question Refinement, Cognitive Verifier, Persona, Template and Context Manager \cite{ronanki2023}. Another approach is adapting prompts from related research, to more consistently compare results and provide empirical backing to the prompt design. 

4. \textit{Provide context}: providing context related to the specific domain can be a useful way to improve the performance of a prompt and avoid irrelevant responses \cite{Marvin2024}. Context can include examples like in few-shot prompting, but also information such as the desired content of the output, the format of the output or additional information about the topic. 

5. \textit{Test and refine}: it is vital to test and refine the designed prompt, with respect to the defined goal. The validation set should be used to fill in any placeholder data, after which the output of the LLM with respect to the prompt can be used to evaluate the performance and whether the LLM adheres to the desired output format. Any changes made to the prompt or insights gained from the evaluation of the prompt, as well as the full interaction logs of the prompts and responses, should be made available, since this information can be useful for reproductions and replications \cite{baltes2025guidelinesempiricalstudiessoftware}.

Lastly, whenever an impactful prompt engineering decision is made, such as choosing to use few-shot prompting instead of another method, the choice should be motivated in the paper. This can be done by explaining the choice based on the specific nature of the task or by referencing previous research which made the same choice. If certain information, such as the prompt or interaction log, cannot be provided for privacy or confidentiality reasons, this should be clearly stated and the researcher should still provide a summary of this information \cite{baltes2025guidelinesempiricalstudiessoftware}.\\

\textbf{S4a. Hyper-parameter tuning.}\\
\infobox{Optional}{This step only applies when fine-tuning or when using other configurable parameters that influence LLM outputs, like the temperature.}
Hyper-parameters are parameters that are set before training, as opposed to parameters that are optimized by training, such as the weights of an LLM. Some hyper-parameters are used during fine-tuning, such as the number of epochs, batch size and learning rate; whereas others influence the outputs of a pre-trained model, such as the temperature \cite{halfon2024hyper,peeperkorn2024}. Although hyper-parameters are not learned, they can have a substantial effect on the performance of an LLM \cite{halfon2024hyper}. For this reason, it is important to carefully select the hyper-parameters. 

Several methods exist to choose hyper-parameters. The simplest is manual search, where the researcher manually compares hyper-parameter values and develops an intuition for choosing the parameters, but this method makes it difficult to reproduce results \cite{bergstra2012hyperRandomSearch}. Grid search automates the process by testing every combination of different subsets of the hyper-parameter space. For example, the researcher picks the subset \{1e-5,1e-4,1e-3\} for the learning rate and \{5,10,20\} for the number of epochs and then fine-tunes with all nine combinations of parameters and continues with the parameters that have the highest performance on the validation set. Since grid search brute-forces all combinations of the chosen subsets, it can become very computationally expensive \cite{liashchynskyi2019hyperGridsearch}. 

A more efficient alternative to grid search is \textit{random search}. Random search works by drawing multiple samples of the hyper-parameters from a discrete or continuous distribution of the hyper-parameter space and fine-tuning using these samples \cite{bergstra2012hyperRandomSearch}. Another advantage of random search is that it can be stopped after any amount of trials, instead of only after the full grid has been explored. Lastly, some other less common methods for tuning are genetic algorithms \cite{Loussaief2018} and DODGE \cite{agrawal2022hyper}.

After the optimal hyper-parameters are determined, the model is retrained using the full training data, including any data that was set aside for validation. This can also be done using nested cross-validation, where the final model is trained at the same time as tuning the hyper-parameters.\\

\textbf{S4b. Prompt comparison.}\\
\infobox{Optional}{This step only applies when using prompts to guide LLM outputs, instead of using only fine-tuning or using LLM embeddings with traditional classifiers.}
The specific wording of a prompt can have a large impact on the performance of an LLM, even when the prompt retains its semantic content and intent \cite{sclar2024quantifyinglanguagemodelssensitivity, he2024doespromptformattingimpact,mizrahi2024stateartmultipromptllm}. This includes seemingly innocuous changes such as capitalisation. Thus, relying on a single human-written prompt could lead to unreliable results. One way to mitigate this problem is by writing multiple prompts that use different prompt patterns (such as Question Refinement, Cognitive Verifier and Persona) and compare the results using the validation set \cite{ronanki2023}. The comparison of different prompts can be done efficiently using PromptEval, a technique for estimating the performance of a large set of prompts, allowing one to pick the best prompt or evaluate the average performance over multiple prompts \cite{polo2024multiprompt}. However, these techniques require manual prompt writing, which still leaves the risk of specific word choices or syntax affecting the performance of the prompts. 

Alternatively, \textcite{sclar2024quantifyinglanguagemodelssensitivity} developed the algorithm FormatSpread, which is able to generate and evaluate a large set of plausible prompt formats, reporting the range of performances over these prompt formats. Some advantages of this technique include that it provides insight into the sensitivity of the original prompt design and that it gives a lower bound on the model's performance. A similar technique is Mixture of Formats (MOF), where the style of each few-shot example is modified by asking the LLM to change the style, leading to a mix of different styles being used, reinforcing model understanding \cite{ngweta2025MOFrobustness}.

The approach to prompt comparison that was used should be reported and justified in the paper, along with prompt revisions and interaction logs \cite{baltes2025guidelinesempiricalstudiessoftware}. The result of prompt comparison could be a single prompt which has shown good performance, which will then be used for the final evaluation; or a set of plausible prompts, in which case the evaluation will be done over this set of prompts. If no prompt comparison is done, the reason should be explained in the paper.\\

\textbf{S5. Test the models.}\\
Finally, after fine-tuning and determining the optimal hyper-parameters and/or prompts, the performance of the LLM is evaluated on the test set. It is recommended to include multiple data sets into the test set, to assess the generalisability of the model and to run statistical tests across these multiple data sets (S11) \cite{Dellanna2022}. \\

\subsubsection{Results Analysis}
 
\textbf{S6. Report the Confusion Matrix.}

 \begin{figure}[H]
	\noindent
	\renewcommand\arraystretch{1.5}
	\setlength\tabcolsep{0pt}
	\begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c c }
		\multirow{10}{*}{\parbox{1.1cm}{\bfseries\raggedleft \rotatebox[origin=c]{90}{Actual}}} &
		& \multicolumn{2}{c}{\bfseries Predicted} \\
		& & \bfseries Positive & \bfseries Negative \\
		& Positive& \MyBox{True}{Positive} & \MyBox{False}{Negative} \\
		& Negative & \MyBox{False}{Positive} & \MyBox{True}{Negative} \\
	\end{tabular}
	\caption{Confusion Matrix. All values add up to the number of data points.}
	\label{fig:confusion}
\end{figure}
While it is common to select a few metrics appropriate to a given task and report them, we recommend reporting the full confusion matrix so readers can get a more complete picture of the results, and be able to calculate many different metrics, even when they aren't explicitly reported (see S7). The confusion matrix consists of the number of true positive (TP), false positive (FP), true negative (TN), and false negative (FN) classification results and are reported as whole numbers. Often, the sums of the rows or columns are also included, since adding up the numbers in the columns results in the total number of data points that were classified as positive or negative, while the row sums represent the number of data points that were actually positive or negative. Figure \ref{fig:confusion} shows a generic confusion matrix. \\

\textbf{S7. Report Metrics.}\\
\begin{table}[bh]
	\centering
	\caption{Calculation of common metrics for classifier performance.}
	\label{table:metrics}
	\begin{tabularx}{\textwidth}{|p{3cm}|X|}
		\hline
		\textbf{Metric} & \textbf{Formula} \\
		\hline
		Precision & $TP / (TP + FP)$ \\
		Recall (TPR) & $TP / (TP + FN)$ \\
		Specificity (TNR) & $TN / (TN + FP)$ \\
		Accuracy & $(TP + TN) / (TP + TN + FP + FN)$ \\
		F$_1$-score & $2 \cdot (\text{Precision} \cdot \text{Recall})/(\text{Precision} + \text{Recall})$ \\
		F$_\beta$-score & $(1 + \beta^2)\text{Recall})/((\beta^2\cdot\text{Precision}) + \text{Recall})$ \\
		MCC & $(TP \cdot TN - FP \cdot FN) / \sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}$ \\
		\hline
	\end{tabularx}
\end{table}
Many metrics can be calculated from the confusion matrix, including the \textit{Precision}, \textit{Recall}, \textit{Sensitivity}, \textit{Accuracy}, F$_1$-score, F$_\beta$-score and Matthews correlation coefficient (MCC). Table \ref{table:metrics} shows the calculation of these metrics using the values from the confusion matrix. Which metrics are reported generally depends on the domain and the specific application. For example, the precision gives the percentage of positive classifications that are correct, so a high precision is desirable for applications where the consequences of false positives are high. On the other hand, the recall gives the ratio of actual positives that were correctly classified, so a high recall is desirable when the consequences of false negatives are high. In most applications, a good balance of precision and recall is preferred, which can be measured by the F$_1$-score, the harmonic mean of recall and precision. The F$_\beta$-score changes the weights of precision and recall in case one of the metrics is more important for the given task.

The specificity or true negative rate (TNR) is the ratio of actual negatives that are correctly classified as negative. The accuracy is the overall ratio of correct classifications and can be a useful metric when the classes are balanced, i.e. there is a similar amount of data points for positive and negative classes. If the classes are not balanced, the accuracy can be misleading \cite{Lones2024pitfalls, Haixiang2017imbalance}.

The F$_1$-score is commonly used in the field of LLM4SE, but it has several drawbacks: (1) the F$_1$-score varies when the classes are swapped (i.e., positive is renamed to negative and vice-versa) and (2) the F$_1$-score is independent of the number of data points correctly classified as negative (TN) \cite{powers2015f,Chicco2020}. The Matthews correlation coefficient (MCC) is recommended by many researchers as a more balanced alternative to the F$_1$-score, since it balances all four categories of the confusion matrix and does so proportionally to the number of positive and negative data points \cite{Chicco2020,Foody2023,Yao2020}.

Why specific metrics were included or omitted from the results should be justified in the text \cite{Kapoor2024}. Ideally, this can be done by explaining why the chosen metrics are mathematically or empirically appropriate for the given task. Otherwise, the researcher should refer to similar previous research that used these metrics. If certain metrics were omitted, the results should report enough information to allow the reader to calculate these metrics themselves, for example by reporting the full confusion matrix and providing the raw results as supplementary material.

The main metrics for comparing LLM classifications are obtained by evaluating on the test set. However, the researcher should also share the metrics that were obtained for the training and validation sets during fine-tuning, hyper-parameter tuning or prompt development/comparison, to give readers more insight into the development and aid replicability. Providing the mean and standard-deviation of the metrics for cross-validation can also provide insight into the performance. Additionally, when using a specific output format such as a JSON object, a distinction should be made between the performance of (1) the test cases where the format constraints were adhered to and (2) all test cases, regardless of format satisfaction \cite{long2025format}.\\

\textbf{S8. Evaluate calibration, fairness, robustness \& sustainability.}\\
\infobox{Optional}{Each of the aspects of LLM evaluation contained in this section is optional; but we recommend considering these aspects when dealing with human participants or users; or sensitive data and applications.}
The previous step (S7) considered how to evaluate the correctness of classification results. However, the correctness does not tell the full story: an accurate model does not necessarily work equally well for all groups of users, nor does the accuracy take into account the resistance of the model to noisy data, changes in distribution, adversarial prompts or the calibration of predicted probabilities \cite{liu2024,chang2023,guo2017calibration}. In this step, we briefly discuss how (and why) to evaluate the calibration, fairness, robustness \& sustainability of LLM classifiers.

\textbf{Calibration}\\
The calibration of an LLM refers to how well the confidence of the model aligns with the actual probability of getting a correct classification \cite{guo2023survey}.  When the confidence is well calibrated, we can get an indication when model outputs are likely to be incorrect or whether we can trust the outputs. The confidence of a prediction can be obtained in several ways, as discussed in \cite{tian2023justaskcalibrationstrategies}. The simplest is by obtaining the conditional probability of the label given the question, which can sometimes not be obtained for closed-source models, in which case it can still be estimated by sampling multiple responses. Another method is prompting the model to generate the confidence of the labels in the answer itself, which can be a numerical probability or a linguistic response (e.g., "Almost certain" or "Unlikely").

A common metric for measuring the calibration is the\textbf{ expected calibration error (ECE)}, which is obtained by dividing predictions into $M$ bins of equal size and calculating the average difference between the accuracy and confidence over all $M$ bins \cite{guo2017calibration}. For high-risk applications, the \textbf{maximum calibration error (MCE)} can be used to get the worst-case difference between the confidence and accuracy. See \textcite{nixon2019calibration} for a discussion of possible pitfalls when using the ECE and alternative metrics.

\textbf{Fairness}\\
The fairness of an LLM refers to the equal treatment across different groups of people \cite{chang2023}. Taking the fairness into account helps ensure the alignment of model behaviour to human intentions \cite{kenton2021alignmentlanguageagents}. \citeauthor{liu2024} list four types of unfairness: injustice, stereotype bias, preference bias and disparate performance. Injustice arises when similar individual are not treated similarly by the model. For example, when two indistinguishable job applications are entered into a prompt, with the only difference being irrelevant group attributes (such as gender), we expect the LLM to classify both applications the same way. Stereotype bias refers to an LLM's prejudiced or misleading expectation about members of particular social groups, such as a presumed academic ability based on race or gender. Preference bias refers to a lack of neutrality concerning subjective topics such as politics, scientific interpretations, societal matters or product preference. Lastly, disparate performance refers to a difference in performance across groups of users. For example, LLMs often perform worse on languages other than English, which means that applications using LLMs might perform worse in certain cultures and societies \cite{bang2023multitaskmultilingualmultimodalevaluation}.

To evaluate the fairness, \textcite{wang2024decodingtrust} list two metrics:
\begin{itemize}
	\item \textbf{Demographic Parity Difference (DPD)}: the DPD measures the difference between the probability of positive predictions given the presence of a sensitive attribute and the probability given the absence of that attribute. A high DPD means that there is a large difference in the positive predictions between groups. Note that the DPD does not consider the ground truth labels.
	\item \textbf{Equalized Odds Difference (EOD)}: the EOD takes the maximum difference between the true positive rate (recall) and false positive rate given the sensitive attribute. This means that a low EOD requires both classes to have similar prediction errors. By comparing the true positive rate and false positive rates, it also takes into account the ground truth labels. 
\end{itemize}

\textbf{Robustness}\\
The robustness of an LLM refers to how well it deals with difficult or unexpected inputs \cite{chang2023}. There are multiple scenarios in which an LLM can encounter challenging inputs, including through changes in the data distribution, noise in the data, adversarial prompts and poisoning of the training data \cite{liu2024,chang2023}. These scenarios can be caused by malicious actors (such as changing labels to poison the training data), user error (such as typos in the prompt) or other reasons (such as a change in user demographic). 

\textcite{chang2023} list two common robustness metrics:
\begin{itemize}
	\item \textbf{Attack Success Rate (ASR)}: the ASR evaluates the success of adversarial attacks by calculating the rate with which the attack successfully produces adversarial examples \cite{liu2024attacksuccesrate}. 
	\item \textbf{Performance Drop Rate (PDR)}: the PDR evaluates the robustness of prompts by quantifying the drop in performance resulting from a prompt attack \cite{zhu2024}. 
\end{itemize}

A different aspect of robustness is how well models perform when some parts of it are deliberately removed. Ablation studies work by removing parts of a model, such as layers in a transformer or input features, and seeing whether the performance changes \cite{meyes2019ablation}. These studies show whether the model exhibits a graceful degradation of the performance when parts are removed. In the field of LLM4SE, ablation studies are commonly used to find the performance benefit of each component in models that consist of multiple interacting components, thereby evaluating redundancy \cite{10586831,10402095}. 


\textbf{Sustainability}\\
The use of LLMs can come with a significant cost of time, space and energy compared to traditional ML models. Especially the training phase of LLMs can have a large energy footprint; but the increased integration of LLMs into various tools such as Google search could also have a major impact on energy consumption over time \cite{wu2022sustainableai,deVries2023}. Researchers in LLM4SE can reduce energy expenditure primarily by reducing LLM inferences, which can be done by restricting the number of queries, using smaller data-sets for fine-tuning or using smaller models \cite{baltes2025guidelinesempiricalstudiessoftware}. By using more efficient algorithms, reducing LLM inferences does not necessarily reduce the accuracy of the results, although there can be a trade-off between accuracy and energy expenditure \cite{deVries2023}. In the results analysis, researchers should report what steps (if any) were taken to reduce energy consumption and justify why LLMs were used instead of less costly alternatives.\\

\textbf{S9. Analyse overfitting \& degradation.}\\
One of the fundamental problems in machine learning is the possible disconnect between the in-sample performance of a model on its training or validation sets and the out-of-sample performance on the unseen test set \cite{Ying2019overfitting}. A high training accuracy does not guarantee a high test accuracy; in fact, a model with a lower training accuracy can outperform a model with a higher training accuracy when applied to the test set. This problem is known as overfitting and can occur when a model learns from noise in the data, has limited training data or has more parameters than needed. For example, if a limited set of examples is used for fine-tuning an LLM, the performance on the fine-tuned data could be great, but not generalize well to the actual real-life task that it aims to solve. 

Overfitting can be quantified by measuring the difference between the performance on the test set and the train set using the metrics in S7. Let $M$ be any chosen metric, then $overfitting = M_{test} - M_{train}$. If we use multiple data sets for testing, we can instead calculate the average overfitting, where $Test$ is the test set:

\begin{equation}
	\text{\textit{average overfitting}} = \frac{1}{|Test|} \sum_{t\in Test} (M_{test} - M_{train})
\label{overfitting}
\end{equation}

Apart from the metrics discussed in S7, such as accuracy, other metrics can also be used to quantify overfitting. This includes the metrics of zero-one loss for binary classifiers and mean squared error (MSE) for when class outputs contain scores or probabilities \cite{Dellanna2022}. If a pre-trained LLM is used with prompting, it might be difficult or unnecessary to analyse the overfitting. However, the validation set that was used for developing and selecting prompts or hyper-parameters can still lead to degradation, where the performance on the test set is lower than the performance on the validation set.

Degradation compares the performance of the test set with the performance of the validation set, using the metrics of S7. Its calculation is similar to overfitting: given performance metric $M$, we calculate $degradation = M_{test} - M_{validation}$. Equivalently, if multiple test sets are used or multiple validation sets in the case of k-fold cross-validation, we recommend calculating the average degradation, as in (\ref{overfitting}). In the event that both k-fold cross-validation and multiple test sets are used, then the distributions of the performance metrics for the validation and test sets can be statistically compared to see if there is a significance difference. For example, if the data is normally distributed, the independent samples T-Test can be used, whereas the Mann-Whitney's U test can be used as a non-parametric alternative. The original \textit{ECSER} pipeline contains examples of how to apply these tests \cite{Dellanna2022}.\\

\textbf{S10. Visualize ROC}

\newpage 
\printbibliography[heading=bibintoc]

\newpage
\begin{appendices}
\section{Supplemental Mapping Study Results} \label{appendix:ref}
For brevity, sources were excluded for some figures and tables and are included here for further reference.\\
Table \ref{refs:application_types} shows the distribution of the application types of LLMs of 83 total papers (see Figure \ref{fig:LLMTypes}).
\begin{table}[ht]
	\caption{References for the distribution of LLM application types.}
	\begin{tabularx}{\textwidth}{|l|c|X|}
	    \hline
	    \textbf{Category} & \textbf{Number of studies} & \textbf{References} \\ \hline
		Generation & 58 & \cite{10.1145/3597503.3639150,10.1145/3597503.3649399,10.1145/3597503.3608132,10.1145/3597503.3623326,10.1145/3597503.3623298,10.1145/3597503.3623306,10.1145/3597503.3623316,10.1145/3597503.3608134,10.1145/3597503.3608137,10.1145/3597503.3623343,10.1145/3597503.3639085,10.1145/3597503.3639120,10.1145/3597503.3639133,10.1145/3597503.3639138,10.1145/3597503.3639219,10.1145/3597503.3639226,10.1145/3597503.3639184,10.1145/3597503.3639081,10.1145/3597503.3639157,10.1145/3597503.3639180,10.1145/3597503.3639121,10.1145/3597503.3639118,10.1145/3597503.3639210,10.1145/3597503.3639116,10.1145/3597503.3639223,10.1145/3597503.3639155,10.1145/3597503.3639183,10.1145/3597503.3639135,10.1145/3663529.3663829,10.1145/3663529.3663836,10.1145/3663529.3663839,10.1145/3663529.3663841,10.1145/3663529.3663842,10.1145/3663529.3663846,10.1145/3663529.3663855,10.1145/3663529.3663861,10.1145/3663529.3663801,10.1145/3663529.3663868,10.1145/3663529.3663869,10.1145/3663529.3663873,10.1145/3663529.3664463,10329992,10378848,10433002,10485640,10507163,10482873,10521881,10584357,10606356,10609742,10634302,10636040,10664637,10538301,10707668,10713474,10734067} \\ \hline
		Classification & 15 & \cite{10.1145/3597503.3623345,10.1145/3597503.3623304,10.1145/3597503.3639126,10.1145/3597503.3639217,10.1145/3597503.3639117,10.1145/3597503.3639194,10.1145/3597503.3639202,10.1145/3663529.3663785,10.1145/3663529.3663794,10323231,10402095,10586831,10648982,10659742,10746847} \\ \hline
		Recommendation & 6 & \cite{10.1145/3597503.3623342,10.1145/3597503.3639188,10.1145/3597503.3639187,10.1145/3663529.3663826,10.1145/3663529.3663803,10697930} \\ \hline
		Generation \& Classification & 3 & 
		\cite{10.1145/3597503.3639216,10599336,10704582} \\ \hline
		Classification \& Recommendation & 1 & \cite{10.1145/3597503.3623322} \\ \hline
	\end{tabularx} \label{refs:application_types}
\end{table}


The following tables apply to the 18 papers included in the mapping study. Table \ref{refs:venues} shows the number of studies belonging to each venue. Table \ref{refs:prompts} shows the number of papers using prompts for classification.
See also Figure \ref{fig:Venues} and \ref{fig:Prompts}.
\begin{table}[ht]
	\caption{References for distribution of venue in the mapping study.}
	\begin{tabularx}{\textwidth}{|l|c|X|}
		\hline
		\textbf{Venue} & \textbf{Number of studies} & \textbf{References} \\ \hline
		TSE & 8 & \cite{10323231,10402095,10586831,10648982,10659742,10746847,10599336,10704582} \\ \hline
		FSE & 2 & \cite{10.1145/3663529.3663785,10.1145/3663529.3663794} \\ \hline
		ICSE & 8 & \cite{10.1145/3597503.3623345,10.1145/3597503.3623304,10.1145/3597503.3639217,10.1145/3597503.3639117,10.1145/3597503.3639194,10.1145/3597503.3639202,10.1145/3597503.3639216,10.1145/3597503.3623322} \\ \hline
	\end{tabularx}
	\label{refs:venues}
\end{table}

\begin{table}[ht]
	\caption{References for the distribution of prompt usage in the mapping study.}
	\begin{tabularx}{\textwidth}{|l|c|X|}
		\hline
		\textbf{Prompt-based} & \textbf{Number of studies} & \textbf{References} \\ \hline
		No & 10 & \cite{10.1145/3597503.3623345,10.1145/3597503.3623304,10.1145/3597503.3639217,10.1145/3597503.3639202,10323231,10402095,10586831,10.1145/3597503.3639216,10704582,10.1145/3597503.3623322} \\ \hline
		Yes & 8 & \cite{10.1145/3597503.3639117,10.1145/3597503.3639194,10.1145/3663529.3663785,10.1145/3663529.3663794,10648982,10659742,10746847,10599336} \\ \hline
	\end{tabularx}
	\label{refs:prompts}
\end{table}

\begin{table}[ht]
	\caption{References for the mapping study results.}
	\begin{tabularx}{\textwidth}{|l|c|X|}
		\hline
		\textbf{Category} & \textbf{Total} & \textbf{References} \\
		\hline
		Total mapping study papers & 18 & \cite{10.1145/3597503.3623345,10.1145/3597503.3623304,10.1145/3597503.3639217,10.1145/3597503.3639117,10.1145/3597503.3639194,10.1145/3597503.3639202,10.1145/3663529.3663785,10.1145/3663529.3663794,10323231,10402095,10586831,10648982,10659742,10746847,10.1145/3597503.3639216,10599336,10704582,10.1145/3597503.3623322}\\
		\hline
		\textbf{Evaluation metrics} & & \\
		\hspace{3mm} Precision & 17 (94.4\%) & \cite{10.1145/3597503.3623345,10.1145/3597503.3623304,10.1145/3597503.3639217,10.1145/3597503.3639117,10.1145/3597503.3639194,10.1145/3597503.3639202,10.1145/3663529.3663785,10.1145/3663529.3663794,10323231,10402095,10586831,10648982,10746847,10.1145/3597503.3639216,10599336,10704582,10.1145/3597503.3623322}  \\
		\hspace{3mm} Recall & 17 (94.4\%) & \cite{10.1145/3597503.3623345,10.1145/3597503.3623304,10.1145/3597503.3639217,10.1145/3597503.3639117,10.1145/3597503.3639194,10.1145/3597503.3639202,10.1145/3663529.3663785,10.1145/3663529.3663794,10323231,10402095,10586831,10648982,10746847,10.1145/3597503.3639216,10599336,10704582,10.1145/3597503.3623322}\\
		\hspace{3mm} Accuracy & 11 (61.1\%) & \cite{10.1145/3597503.3623304,10.1145/3597503.3639217,10.1145/3597503.3639194,10.1145/3597503.3639202,10.1145/3663529.3663794,10323231,10402095,10586831,10659742,10746847,10599336}\\
		\hspace{3mm} F-Score & 17 (94.4\%) &  \cite{10.1145/3597503.3623345,10.1145/3597503.3623304,10.1145/3597503.3639217,10.1145/3597503.3639117,10.1145/3597503.3639194,10.1145/3597503.3639202,10.1145/3663529.3663785,10323231,10402095,10586831,10648982,10659742,10746847,10.1145/3597503.3639216,10599336,10704582,10.1145/3597503.3623322}\\
		\hspace{3mm} AUC & 3 (16.7\%) & \cite{10.1145/3597503.3639202,10402095,10659742}\\
		\hspace{3mm} ROC plots & 0 (0.0\%) & \\
		\hspace{3mm} MCC & 0 (0.0\%) & \\
		Calibration & 0 (0.0\%) &  \\
		Fairness &  0 (0.0\%) &  \\
		\textbf{Robustness} &  & \\
		\hspace{3mm} Data distribution \& Adversarial Attacks & 1 (5.5\%) & \cite{10586831} \\
		\hspace{3mm} Noisy data & 1 (5.5\%) & \cite{10.1145/3597503.3639217} \\
		\textbf{Metrics justification} & & \\
		\hspace{3mm} No & 9 (50.0\%) & \cite{10.1145/3597503.3639217,10.1145/3597503.3639117,10.1145/3597503.3639194,10.1145/3597503.3639202,10.1145/3663529.3663785,10.1145/3663529.3663794,10323231,10586831,10599336}\\
		\hspace{3mm} Implicit & 0 (0.0\%) &  \\
		\hspace{3mm} Previous work & 5 (27.8\%) & \cite{10402095,10659742,10746847,10.1145/3597503.3639216,10.1145/3597503.3623322}\\
		\hspace{3mm} Yes & 4 (22.2\%) & \cite{10.1145/3597503.3623345,10.1145/3597503.3623304,10648982,10704582} \\
		Confusion matrix & 4 (22.2\%) & \cite{10.1145/3597503.3639117,10.1145/3663529.3663794,10648982,10659742}\\
		Evaluation over multiple data sets & 5 (27.8\%) & \cite{10.1145/3597503.3639117,10402095,10648982,10.1145/3597503.3639216,10.1145/3597503.3623322}\\
		\textbf{Type of baseline} &&\\
		\hspace{3mm} None & 2 (11.1\%) & \cite{10.1145/3597503.3639117,10.1145/3663529.3663794} \\
		\hspace{3mm} Own & 3 (16.7\%) & \cite{10.1145/3597503.3639194,10.1145/3663529.3663785,10704582} \\
		\hspace{3mm} External & 4 (22.2\%) & \cite{10.1145/3597503.3623304,10402095,10599336,10.1145/3597503.3623322}  \\
		\hspace{3mm} External and Own & 9 (50.0\%) & \cite{10.1145/3597503.3623345,10.1145/3597503.3639217,10.1145/3597503.3639202,10323231,10586831,10648982,10659742,10746847,10.1145/3597503.3639216}\\
		Analysis of statistical significance & 5 (27.8\%) & \cite{10.1145/3597503.3623345,10648982,10659742,10746847,10704582}\\
		\textbf{Statistical significance justification}&& \\
		\hspace{3mm} No & 4 (80.0\%) & \cite{10648982,10659742,10746847,10704582} \\
		\hspace{3mm} Implicit & 0 (0.0\%) &  \\
		\hspace{3mm} Previous work & 1 (20.0\%) & \cite{10.1145/3597503.3623345}\\
		\hspace{3mm} Yes & 0 (0.0\%) &   \\
		\hline
	\end{tabularx}
	\label{refs:mapping_metrics}
\end{table}

For the main results of the mapping study, Table \ref{refs:mapping_metrics} shows the full references. Further, Table \ref{refs:prompt_results} shows the references for the papers that use a prompt-based approach to classification. 

\begin{table}[ht]
	\caption{References for the mapping study prompting results.}
	\begin{tabularx}{\textwidth}{|l|c|X|}
		\hline
		\textbf{Category} & \textbf{Total} & \textbf{References} \\
		\hline
		Total prompt-based classification papers & 8 & \cite{10.1145/3597503.3639117,10.1145/3597503.3639194,10.1145/3663529.3663785,10.1145/3663529.3663794,10648982,10659742,10746847,10599336} \\
		\hline
		\textbf{Prompt approach} && \\
		\hspace{3mm} Chain-of-verification & 1 (12.5\%) & \cite{10.1145/3597503.3639194} \\
		\hspace{3mm} Few-shot & 2 (25.0\%) & \cite{10746847,10599336} \\
		\hspace{3mm} Mimic-in-the-background & 1 (12.5\%) & \cite{10.1145/3597503.3639117} \\
		\hspace{3mm} Not reported & 1 (12.5\%) & \cite{10.1145/3663529.3663785} \\
		\hspace{3mm} Zero-shot & 3 (37.5\%) & \cite{10.1145/3663529.3663794,10648982,10659742} \\
		\textbf{Final prompt} && \\
		\hspace{3mm} Not reported & 2 (25.0\%) & \cite{10.1145/3663529.3663785,10746847} \\
		\hspace{3mm} Own & 6 (75.0\%) & \cite{10.1145/3597503.3639117,10.1145/3597503.3639194,10.1145/3663529.3663794,10648982,10659742,10599336} \\
		\textbf{Prompt approach justification} && \\
		\hspace{3mm} No & 5 (62.5\%) & \cite{10.1145/3663529.3663785,10.1145/3663529.3663794,10648982,10659742,10746847} \\
		\hspace{3mm} Yes & 3 (37.5\%) & \cite{10.1145/3597503.3639117,10.1145/3597503.3639194,10599336} \\
		\textbf{Final prompt justification} && \\
		\hspace{3mm} No & 4 (50.0\%) & \cite{10.1145/3597503.3639194,10.1145/3663529.3663785,10648982,10746847} \\
		\hspace{3mm} Previous Work & 1 (12.5\%) & \cite{10.1145/3663529.3663794} \\
		\hspace{3mm} Yes & 3 (37.5\%) & \cite{10.1145/3597503.3639117,10659742,10599336} \\
		\textbf{Evaluation over multiple prompts} && \\
		\hspace{3mm} No & 8 (100.0\%) & \cite{10.1145/3597503.3639117,10.1145/3597503.3639194,10.1145/3663529.3663785,10.1145/3663529.3663794,10648982,10659742,10746847,10599336} \\
		\textbf{Temperature} && \\
		\hspace{3mm} No & 7 (87.5\%) & \cite{10.1145/3597503.3639194,10.1145/3663529.3663785,10.1145/3663529.3663794,10648982,10659742,10746847,10599336} \\
		\hspace{3mm} Yes & 1 (12.5\%) & \cite{10.1145/3597503.3639117} \\
		\hline
	\end{tabularx}
	\label{refs:prompt_results}
\end{table}

\end{appendices}

\end{document}
