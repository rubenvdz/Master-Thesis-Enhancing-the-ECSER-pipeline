\documentclass[a4paper]{article}

\usepackage{graphicx,float} % Required for inserting images
\usepackage[skins]{tcolorbox}
%\usepackage{apacite}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{parskip} 
\usepackage{tabto}
\usepackage{adjustbox} % Scaling
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{hhline}
\usepackage[dvipsnames]{xcolor}
\graphicspath{ {figures/} }

% Fonts
\usepackage{times} 

\usepackage{caption}  % fine control over caption appearance
\captionsetup{labelfont=bf,singlelinecheck=false,labelsep=space,skip=2pt}

% Citation style
\usepackage[bibstyle=ieee,citestyle=numeric-comp]{biblatex}
\addbibresource{Sources.bib}
\addbibresource{MappingStudy.bib}

% Macros
%\newcommand{\emphasize}[1]{\begin{quote} \textbf{#1} \end{quote}}
\newcommand{\emphasize}[1]{\textbf{#1}}
\newcommand{\newecser}{\textit{ECSER-LLM} }

\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage[toc,page]{appendix}
%\newcommand{\comment}[1]{}
\makeatletter
\newcommand*{\centerfloat}{%
  \parindent \z@
  \leftskip \z@ \@plus 1fil \@minus \textwidth
  \rightskip\leftskip
  \parfillskip \z@skip}
\makeatother

\definecolor{darkishgray}{gray}{0.55}
\definecolor{lightergray}{gray}{0.85}
\newcommand{\infobox}[2]{
	\begin{tcolorbox}[
		skin=widget,
		boxrule=1mm,
		coltitle=black,
		colframe=darkishgray,
		colback=lightergray,
		width=(\linewidth),
		before=\hfill,
		after=\hfill,
		before skip=1pt,
		adjusted title={#1}
		]
		#2
	\end{tcolorbox}
}

\newcommand\MyBox[2]{
	\fbox{\lower0.75cm
		\vbox to 1.7cm{\vfil
			\hbox to 1.7cm{\hfil\parbox{1.4cm}{#1\\#2}\hfil}
			\vfil}%
	}%
}

% Start content
\title{\includegraphics[width=0.2\textwidth]{figures/UUlogo.png} 
\vspace{0.5cm}
\hrule
\vspace{0.5cm}
\textbf{Enhancing the ECSER pipeline: Evaluating Large Language Model Classifiers in SE Research}\\
\vspace{0.5cm}
\hrule
}
\author{Ruben van der Zeijden (7081111) \\ \\
\textit{Supervisor:} Dr. F.B. Aydemir 
\\ \textit{Second Reader:} Dr. D. Dell'Anna \\
}
\begin{document}
\date{}
\maketitle
\begin{center}
\begin{abstract}
The various research papers in the field of Software Engineering (SE) that use classification algorithms, LLMs, or other machine learning methods to obtain their results differ in how many and which evaluation metrics are reported, whether or not significance tests are performed, and which steps are taken to aid reproducibility. The ECSER pipeline was designed to mitigate this issue by providing a step-by-step pipeline that researchers can use to report their results when classification algorithms are used, and the pipeline was empirically shown to be effective in replicating the findings of several studies, as well as producing additional findings and occasionally contradicting the findings of the original papers. However, the ECSER pipeline is designed for evaluating simple classifiers and gives no specific recommendations for LLM classifiers, despite LLMs being an increasingly popular choice for classification tasks. 
The goal of this thesis is to expand the ECSER pipeline for the use of LLMs by adding recommendations from LLM4SE research and related fields. First, an exploratory mapping study was done of SE studies released in 2024 that use LLM classifiers, summarising which steps were taken and which metrics were (or weren't) reported, in order to get an overview of the current state of LLM4SE research. Subsequently, we designed the \newecser pipeline, an enhanced version of the ECSER pipeline for the use of LLMs, including recommendations for prompt engineering, evaluating the fairness and robustness of models, and more. Lastly, in order to evaluate the comprehensiveness and ease-of-use of the new pipeline, two replication studies were conducted using the pipeline to test its ability to strengthen or contradict the findings of the original papers.
\end{abstract}
\vfill 
\textbf{MSc. Artificial Intelligence}\\
\textbf{30EC} \\ 
\footnotesize{The Ethics and Privacy Quick Scan of the Utrecht University Research Institute of Information and Computing Sciences classified this research as low-risk with no fuller ethics review or privacy assessment required.}
\vspace{0.5cm}
\end{center}

\newpage
\tableofcontents

\newpage

\section{Introduction}
% Introduction of the topic, talk about ECSER pipeline (sources: aydemir, kitchenham)
In recent years, large language models (LLMs) have become an increasingly relevant tool in software engineering (SE), allowing researchers to accomplish complex tasks usually reserved for humans or other machine learning (ML) methods, such as code generation, summarization and classification. However, few guidelines exist for conducting and reporting on LLM research, leaving researchers on their own to decide what choices to make when designing prompts, what evaluation metrics to report in their results and the level of statistical validation needed for their comparisons. Because of these lack of guidelines, researchers using LLMs often omit important details, such as the F1 score or the significance of their results \cite{guo2023survey}. This problem is not new or unique to LLMs; many researchers have reported that there is a poor standard of empirical software engineering, which inhibits the usefulness of results and impairs reproducibility \cite{kitchenham2002,Menzies2012}.

To treat the observed poor standard of empirical SE, some researchers have called for a more systematic approach to conducting and evaluating experiments. The ECSER (Evaluating Classifiers in Software Engineering) pipeline, introduced by \textcite{Dellanna2022}, provides a systemic pipeline for training, validating and testing ML classifiers and analysing their results. ECSER is based on recommendations from the field of machine learning for software engineering (ML4SE) and is designed to be easy to use for SE researchers, regardless of expertise with machine learning methods. The ECSER pipeline has seen some empirical validation in the form of two replication studies, in requirements engineering and software testing, where \citeauthor{Dellanna2022} were able to confirm and strengthen some findings, as well as discover additional findings or findings that contradict the original ones. Figure \ref{fig:ECSER} shows the steps of the ECSER pipeline.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/ECSER.png}
    \caption{The ECSER pipeline. Reproduced from \textcite{Dellanna2022}. Steps with a dashed border are optional. The $\leftrightarrow$ arrows indicate feedback loops between phases.}
    \label{fig:ECSER}
\end{figure}


% What is the ECSER pipeline missing (LLMs) / Problems
However, while ECSER is tailored to traditional ML classifiers (such as logistic regression, SVMs and decision trees), it includes no specific recommendations for LLM-based classifiers, despite LLMs showing promise for various classification tasks \cite{Guo2024health,fields2024}. Thus, the ECSER pipeline is not sufficient for LLM research, since using LLMs for classification comes with unique considerations that are not present for traditional classifiers. For one, it is common to use pre-trained, generalised LLMs and give task-specific instructions to get the desired classification on the input data. This means that designing these instructions, also called prompts, becomes a crucial part to conducting LLM research \cite{Marvin2024}. There are also unique considerations when it comes to evaluating the fairness and robustness of the results: does the model behave in accordance with human values and are the results resilient to adversarial prompts, such as those containing typos \cite{Woodworth2017,zhu2024}?


% Contributions that will be made
The goal of this thesis is to address the lack of LLM-specific guidelines in the ECSER pipeline, by expanding it to include specific recommendations for conducting LLM classifier research. In doing so, we made the following contributions:
\begin{itemize}
    \item We conducted a mapping study to assess the current state of reporting in LLM4SE classification research.
    \item We developed the \newecser pipeline, an expanded version of the ECSER pipeline that includes specific recommendations for conducting LLM classifier research, such as prompt engineering and the evaluation of model fairness and robustness.
    \item We conducted two replication studies of existing LLM classifier papers using the \newecser pipeline, in order to evaluate the pipeline's applicability and comprehensiveness.
\end{itemize}

Supplemental materials, including all code that was used, the full intermediary and final results and the source for this document can be found on our GitHub page\footnote{\url{https://github.com/rubenvdz/Master-Thesis-Enhancing-the-ECSER-pipeline}}.\\

% Structure of the thesis
The remainder of this thesis is structured as follows. In Section \ref{Background}, we provide background information about LLMs. Section \ref{Research approach} describes the research approach and research questions. Lastly, Section \ref{Related Work} discusses related work.

\section{Background}\label{Background}
% LLMs (what are LLMs, types of LLMs, prompt engineering)
\textbf{Language Models (LMs)} are computational models designed to model the generative likelihood of word sequences \cite{zhao2025}. In other words, given a sequence of input text, they predict the next word in the sequence based on what is deemed most likely by the model. One of the oldest examples of language models are N-grams, which model the likelihood of a word given the previous N tokens \cite{shannon1948}. Early statistical models like N-gram models suffer from several problems, including limited context and difficulty with unseen words.

\textbf{Pre-trained Language Models (PLMs)} are LMs that are designed to take in vast amounts of unannotated text in order to learn knowledge about language and the world \cite{jurafsky2025}. The knowledge that is contained in these models can then be used on new tasks by way of \textbf{transfer learning} \cite{wang2023plm}. PLMs require much more information to be contained within than statistical language models such as N-grams could provide, which has led researchers to design more sophisticated models that rely on neural networks to model language, starting with recurrent neural networks (RNNs) and later Long Short-Term Memory (LSTM) models \cite{mikolov2012rnn,hochreiter1997lstm}. The biggest recent breakthrough came with the invention of \textbf{transformers}, which performed better than previous models, were easier to train and allowed for the parallel processing of inputs \cite{Vaswani2017transformer}. Transformers typically consist of an \textbf{encoder}, which embeds the input sequence into a hidden (latent) space and a \textbf{decoder}, which translates the abstract representation of the hidden space to the target text. Both the encoder and decoder use a self-attention mechanism to weigh the contribution of each token to the output \cite{Vaswani2017transformer,bahdanau2016attention}.

\textbf{Large Language Models (LLMs)} were introduced as a way to refer to PLMs with massive parameter sizes, since it was found that larger parameter sizes lead to better performance and emergent abilities \cite{kaplan2020scaling,wei2022emergent}. Almost all LLMs use transformer models with the self-attention mechanism, but not all of them use both the encoder and decoder component \cite{hou2024}:
\begin{itemize}
    \item \textbf{Encoder-decoder LLMs}, such as BART \cite{lewis2019bart} and T5X \cite{roberts2022t5x}, are good at both understanding and generating language \cite{hou2024}. As such they are useful for tasks like summarization and translation \cite{cho2014encoderdecoder, Asadi2020encoderdecoder}.
    \item \textbf{Encoder-only LLMs}, such as BERT and its variants \cite{devlin2019bert,liu2019roberta,sanh2019distilbert,lan2020albert}, are tailored towards language understanding \cite{hou2024}. As such they are good at tasks like classification and inference \cite{koroteev2021encoder}. 
    \item \textbf{Decoder-only LLMs}, such as the GPT-series \cite{radford2018gpt,brown2020gpt3,openai2024gpt4}, are tailored towards language generation \cite{hou2024}. As such they are good at all generation tasks, including code generation \cite{poldrack2023} and creating conversational agents like ChatGPT \cite{OpenAI2023chatgpt} and LLama 2 \cite{touvron2023llama2}.
\end{itemize}


\textbf{Prompt Engineering} is the process of systematically designing natural language instructions (or \textbf{prompts}) to guide LLMs towards a desired output \cite{ronanki2023}. Prompts condition the outputs of the LLM on the given instructions, so that the model generates the next tokens with both the prompt and previously generated tokens in mind \cite{jurafsky2025}. The most common approaches to prompt engineering are:
\begin{itemize}
    \item \textbf{Zero-shot Prompting}. The model is given no examples for a task, forcing it to rely entirely on the prompt \cite{radford2019language}.
    \item \textbf{Few-shot Prompting}. A limited number of examples for a task are given, which the model can learn from \cite{brown2020fewshot}.
    \item \textbf{Chain-of-Thought Prompting}. The prompt instructs the model to follow coherent reasoning steps \cite{wei2022chain}.
\end{itemize}
Another less common approach is automatic prompt engineering, where instructions are automatically generated and selected \cite{zhou2023ape}.

\section{Research Questions} \label{Research approach} 

% Research questions
The usage of LLMs in SE research has increased massively over the past few years, but the research of how to best conduct studies and report results in this field remains very limited. In order to approach the problem of LLM4SE reporting standards and design meaningful guidelines, we follow \citeauthor{Wieringa2014}'s design cycle of \textit{problem investigation}, \textit{treatment design} and \textit{treatment validation} \cite{Wieringa2014}. This leads us to the following research questions:

\emphasize{RQ1: What is the current state of reporting in LLM4SE classification research?} \\
RQ1 fulfils the step of \textit{problem investigation} and is both descriptive and evaluative: what do researchers report in their studies and does this level of reporting contribute or detract from the accuracy and reproducibility of the results. The aim of this question is to provide insights into the potential shortcomings of existing research by conducting a mapping study of the reporting practices of LLM4SE research, in order to provide a basis for additional guidelines that help researchers avoid these shortcomings. Section \ref{RQ1} presents the research approach of RQ1 and the results of the mapping study.
	
\emphasize{RQ2: What resource can be developed to assist researchers in conducting and reporting on LLM4SE classification research?}\\
Following the problem investigation of RQ1, RQ2 fulfils the step of \textit{treatment design}. The purpose of RQ2 is to design a tangible artefact that is comprehensive and widely applicable to the field of LLM4SE, with the intent of helping researchers avoid any problems identified in RQ1. This research question was designed to guide the modification of the existing ECSER pipeline \cite{Dellanna2022}, taking into consideration recommendations from the fields of LLM4SE, ML4SE and Design Science. Section \ref{RQ2} presents the research approach of RQ2 and the resulting \newecser pipeline.


\emphasize{RQ3: How applicable is the proposed pipeline in assisting LLM4SE classification research?}\\
After designing the \newecser pipeline, RQ3 follow the step of \textit{treatment validation}: does the proposed pipeline achieve its goals of applicability and comprehensiveness. The purpose of RQ3 is to apply the pipeline to existing LLM4SE classification research by way of a replication study, in order to evaluate whether the pipeline is able to replicate the research and whether there can be added benefit to using the pipeline. Section \ref{RQ3} presents the research approach of RQ3 and the results of the replication study.


\section{Related Work} \label{Related Work}
The main focus of this thesis is to study the usage and evaluation of LLM Classifiers in Software Engineering, but in doing so we touch on several related research fields, including Empirical Software Engineering, Machine Learning for Software Engineering (ML4SE), Large Language Models for Software Engineering (LLM4SE) and Prompt Engineering. Additionally, each of the three research questions has related work that is relevant to their methodologies. Hence, we have split the related work into several sections based on different areas of study to aid readability. 

First, we discuss challenges in empirical software engineering which motivated the creation of the ECSER pipeline and further motivated our mapping study of LLM4SE. Second, we discuss the topic of replication in SE, which is relevant to our replication studies and the field of empirical SE. Third, we discuss how Large Language Models have been used in SE, with a focus on classification tasks. Fourth, we discuss related work in Prompt Engineering, as it has become a vital part to using LLMs and informed our guidelines for prompt engineering in the \newecser pipeline. Finally, we discuss related work on the evaluation of LLMs and classifiers informed our guidelines for the evaluation of LLM Classifiers.

\subsection{Challenges in Empirical Software Engineering}
% Related work on evaluating "simple" classifiers in SE (Dell'ana 2022, Kitchenham 2004, Kitchenhham 2002, Menzies 2012, Zhang 2002)
Over the past couple decades, the use of machine learning methods has become ubiquitous in every field of science, among them software engineering. \textcite{Zhang2002} summarized seven main activities that use machine learning in SE: prediction, property/model discovery, transformation, generation, library construction and maintenance, acquisition of specifications and development knowledge management. In addition, they list specific tasks belonging to each activity, such as \textit{predicting} software quality and \textit{generating} test cases. The field of classification research falls almost exclusively in the prediction category.

With the increased popularity of machine learning methods, many software packages have been developed to allow anyone to use these methods without having to write any code or even understand how these methods work. This leads to many researchers using machine learning without fully understanding the underlying assumptions of the methods they are using and assuming the results are correct without proper (statistical) analysis. This problem is not new: \textcite{kitchenham2002} reported at the start of this century that there was a poor standard of empirical software engineering, mostly caused by a lack of statistical expertise from both the authors and reviewers. They go on to provide extensive guidelines for designing and conducting experiments as well as analysing, presenting and interpreting results. In another paper, \textcite{kitchenham2004} discuss the benefits of evidence-based software engineering approach by drawing an analogy to evidence-based medicine, but note that the infrastructure needed for widespread adoption of evidence-based SE isn't there yet, which means that SE experiments are vulnerable to subject and experimenter bias. 

Methodological problems in SE research can result in problems with the reproducibility of results, which severely reduces the real-world usefulness of said results. \textcite{Menzies2012} argue that the main goal of science is \textit{conclusion stability}, which means that when you discover an effect, it holds true irrespective of the situation. However, they note that in the software engineering literature there is often as much evidence \textit{for} a given effect, as \textit{against}. This lack of conclusion stability is caused by sources of bias and variance that are introduced at various steps: sampling, pre-processing, training, algorithm selection and the analysis of results. If these sources of bias and variance are accurately analysed and reported, that would make it easier for the authors and other researchers to interpret the conclusions.

Despite being a known issue, the problems of improper analysis and reporting have persisted over the years. In a mapping study of the proceedings of the International Conference of Software Engineering (ICSE) from 2019 to 2021, \textcite{Dellanna2022} analysed 60 SE papers related to classification and found that most papers (36) reported on the precision and recall of their experiment, fewer reported the F-score (27) and accuracy (24), but only 14 papers explicitly justified why they chose the reported metrics. The full confusion matrix was only included in six papers. Other metrics like the receiver operator characteristic (ROC) curve and the area under the curve (AUC) were mostly absent with 3 and 9 respective mentions. Of course, as we know from statistics, it is not enough to report a metric and claim it's better than that of other models, since it could be coincidental, so statistical significance testing is required to make such claims. However, only six papers analysed the statistical significance of their results. Evidently, the situation is dire, which is why \citeauthor{Dellanna2022} developed the ECSER pipeline to give SE researchers a systematic approach to conducting classification experiments and reporting and analysing the results. 

\subsection{Replications in SE}
Replication studies are a vital part of empirical software engineering, allowing researchers to investigate the effects of alternative values for important attributes, vary the strategy with which hypotheses are investigated and make up for certain threats to validity \cite{Basili1999}. The importance of replication is not unique to software engineering, after all the so-called "replication crisis" was a massive topic of debate in psychology research in recent years, since it was found that many previous results in the field could not be replicated, even if the original findings were statistically significant \cite{Shrout2018,Maxwell2015,Amrhein2018}. \textcite{Cruz2020} conducted a systematic literature review of replication studies in SE between 2013 and 2018 and found that the number of published replication studies has been steadily increasing, though there are research gaps in certain fields, suggesting that the SE community is taking an increased interest in replicating previous studies.

\textcite{Shull2008} identify two types of replication studies: \textit{exact replications}, in which the procedures of the original study are strictly followed; and \textit{conceptual replications}, in which the research problem of the original study is evaluated using a different procedure. Additionally, exact replications are subdivided into \textit{dependent} and \textit{independent} replications. Dependent replications keep most of the conditions of the experiment (close to) the same, which allows one to test the effect of specific variables on the results; whereas independent replications vary the original conditions of the experiment significantly, which allows one to test the robustness of the original results under different conditions. \textcite{Juristo2010} underline the value of non-exact replications, which they define similarly to how \citeauthor*{Shull2008} define independent exact replications, by conducting a multiple-case replication study using a newly proposed four-phase process, to show that non-identical replications can be used to learn new information about the original results.

While there is a consensus in SE that replication is important, \textcite{Shepperd2018} show that replication studies often suffer from the same problems as original studies, such as incomplete reporting, low power or potential researcher bias. Notably, many replication studies in their survey did not report any details on the dispersion (e.g., variance) of the response variables. In another paper, \textcite{Shepperd2018b} shows that because of wide prediction intervals, almost all replications are confirmatory, which means the added knowledge is negligible. Because of these problems, they suggest that researchers should focus more on broader meta-analyses instead of replication studies. Other common problems in conducting replication research include difficulty in getting them published, lack of guidelines and the unavailability of replication packages \cite{Cruz2020,Siegmund2015}.

Communication can also play a factor in successful replication: \textcite{Vegas2006} investigated the role of communication in successful replication of previous experiments and found that there should be some level of communication, orally or in writing, between the previous authors and authors of the replication study, to avoid unnecessary changes to the experiments. 

Given these challenges, guidelines for conducting replications are clearly needed. Despite this, the number of existing guidelines remains low. Guidelines were proposed by \textcite{carver2010}, which are still commonly used in replications, but these seem to be the only replication guidelines for software engineering and were intended as a starting point for future guidelines. Other artifacts do exist to aid replications: \textcite{Gomez2014} proposed a classification of replications in SE, differentiating between literal, operational and conceptual replications, that can be used to identify which changes can be made in each type of replication and understand the level or verification needed for that type. 

\textcite{Abualhaija2024} proposed an artifact, referred to as the ID-Card, for summarizing papers in RE research that use NLP techniques. The intention behind the ID-Card is that all the relevant information needed for replication is presented in an easy to read format and can be used for both replication and educational purposes. Lastly, \textcite{Brandt2014} developed the "Replication Recipe" for psychology research, which lists 36 questions that should be addressed when conducting a replication, most of which are also directly applicable to empirical SE. 

\subsection{Large Language Models for SE}
% Related work on LLMs in SE in general (Wang 2024, Fan 2023, find more for specific subdomains?)
A recent development in SE research is the increased relevance and use of LLMs. \textcite{fan2023} looked at all preprints on arXiv categorised under computer science whose title included "LLM", "Large Language Model" or "GPT" and found the number of SE papers that mentioned LLMs grew exponentially from 0 in 2019, 5 in 2020, to 181 in 2023. Naturally, this is only an approximation of the total number of preprints on arXiv that use LLMs, but it undoubtedly signifies a substantial increase. \citeauthor{fan2023} also conducted an extensive survey of how LLMs have been applied to various software development activities (such as code generation and testing) and research domains (such as human-computer interaction and education), but the survey is almost exclusively focused on the generative use of LLMs and does not go into depth on the use of LLMs in classification research. Many other good surveys and analyses of the use of LLMs in Software Engineering have been written, but most of them similarly do not mention any applications to classification tasks and instead exclusively focus on generation \cite{Belzner2024,Zheng2024,Ozkaya2023}.

Surveys have also been written about specific domains of software engineering: \textcite{wang2024} did a comprehensive review of 102 studies that use LLMs for software testing and found that LLMs have commonly been applied to various tasks, including test case generation, test input generation, debugging, program repair and more. They also found that while most papers used LLMs to address the entire task, many others combined LLMs with additional techniques to optimize the outputs of the LLM, including mutation testing, differential testing, syntactic checking, program analysis, statistical analysis and other techniques. With these extra techniques, researchers were able to generate more diverse and complex code and overcome some of the limitations of LLMs. 

% Related work on LLMs in CLASSIFICATION (Hou 2024, Zhang 2024, Fields 2024)
While LLMs are most frequently used to generate text (and code), they have seen considerable success in classification tasks. \textcite{Guo2024health} have shown that LLMs can outperform other methods like SVMs for health-related text classification. \textcite{fields2024} present an in-depth survey of text classification using transformers across domains and found that LLMs can perform remarkably well on many (but not all) classification tasks. However, \textcite{Chen2024} compared the performance of various LLMs to traditional ML methods in clinical prediction and found that LLMs could \textit{not} beat traditional methods in this case. \textcite{vajjala2025} also show that there are large performance disparities between languages in classification tasks. Thus, LLMs might not always be a better choice than traditional ML methods, but they certainly show potential. 

Software engineers have also started using LLMs for classification. \textcite{hou2024} conducted a systematic literature review of 395 software engineering studies that use LLMs and found that around 21.61\% involve classification tasks. The most common classification tasks where LLMs have been used include vulnerability detection, requirements classification, bug prediction and review/commit/code classification, with many other classification tasks having been attempted using LLMs. \textcite{zhang2024} conducted a systematic survey of 947 SE studies and summarized 62 unique LLMs of code, including six LLMs specifically fine-tuned for the tasks of code classification, clone detection and defect detection.

% Related work on GUIDELINES
Recently, \textcite{baltes2025guidelinesempiricalstudiessoftware} presented a list of community-driven guidelines for using and reporting on LLMs, providing eight detailed guidelines. These guidelines include: (1) declaring LLM usage and role; (2) reporting model versions, configurations and fine-tuning; (3) documenting tool architectures; (4) disclosing prompts and interaction logs; (5) using human validation; (6) employing an open LLM as baseline; (7) using suitable baselines, benchmarks and metrics; and (8) openly articulating limitations and mitigations. These guidelines were designed to increase the transparency and reproducibility of LLM research and are actively maintained on \url{https://llm-guidelines.org/}. The guidelines in this thesis serve a similar purpose of helping researchers conduct LLM4SE research; however, their guidelines have a broader focus, since they include all LLM applications, whereas our guidelines are specifically tailored to classification research. Furthermore, unlike their paper, this thesis also includes a mapping study to assess the state of LLM4SE classification reporting and provides a replication study to evaluate the guidelines. Despite the difference in scope, many of their recommendations directly apply to our research and were taken into account for the \newecser pipeline where relevant.

\subsection{Prompt Engineering} \label{Prompt Engineering}
% Prompt engineering (Hou 2024, Arvidsson 2023, Ronanki 2023, Ekin 2023, White 2023, Marvin 2024)
Designing a good prompt for a particular task is vital to ensuring the adequate performance of LLMs, which is why much research has gone into the systematic designing of prompts, or prompt engineering. \textcite{Marvin2024} provide an overview of prompt engineering principles and techniques and outline the main steps involved in the process of prompt engineering: 1) define the goal of the prompt; 2) understand the model's capabilities; 3) choose the right prompting format; 4) provide context to the LLM and 5) test and refine the prompt based on the goal. Several resources exist to make it easier to choose the right prompting approach: \textcite{white2023} introduce prompt patterns, which offer reusable solutions to common prompting challenges, and provide a framework for designing and documenting these patterns in terms of the intent, motivation, structure and consequences of the patterns. For example, the persona pattern can be used to make the LLM take the point of view of an expert in the field, which is especially useful if the user itself is not an expert. The paper provides a catalogue of this pattern and other successfully applied patterns with example implementations. Further, \textcite{ekin2023} provides an accessible (AI-generated) guide to prompt engineering.

Prompt engineering is an important consideration when using LLMs: \textcite{sclar2024quantifyinglanguagemodelssensitivity} show that choices in prompt design, even if minor, can have a very large impact on the performance of several popular LLMs, and this sensitivity is present regardless of the size of the model or the number of examples provided. Because of this, they argue that researchers should forego using only a single prompt and switch to reporting the performance of their models using multiple plausible prompt patterns. They present a novel algorithm, FormatSpread, to evaluate multiple prompts at the same time and measure the sensitivity of the LLM. \textcite{mizrahi2024stateartmultipromptllm} have found a similar sensitivity of LLMs to prompt design and discuss metrics to evaluate multiple prompts. Lastly, \textcite{polo2024multiprompt} propose PromptEval, an efficient method for evaluating a large number of prompts, and show it can accurately estimate the performance distribution of the prompts. 

Prompt engineering research has also been conducted specifically for software engineering: \textcite{hou2024} looked at what prompt engineering techniques are commonly applied in SE tasks and found the most common techniques involve few-shot prompting and zero-shot prompting, but the third largest group of studies had no explicit mention of prompting techniques or proposed their own strategies. Other techniques included chain-of-thought, automatic prompt engineering, chain-of-code, automatic chain-of-thought, modular-of-thought and structured chain-of-thought.

\textcite{arvidsson2023} collected prompt engineering recommendations for requirements engineering (RE) in a systematic literature review and classified the guidelines into various themes, after which they interviewed three RE experts to evaluate the guidelines. \textcite{ronanki2023} evaluated the effectiveness of 5 prompting patterns (from the catalogue by \textcite{white2023}) on two RE tasks and proposed a framework for evaluating the effectiveness of prompting patterns for any RE task, providing five steps for conducting a comparison of patterns.

A recent development in prompt engineering is automatic prompt engineering (APE), which has shown good performance compared to baseline models, while avoiding the need for manually designing prompts \cite{zhou2023ape,ye2024ape}. \textcite{zadenoori2025} investigated the use of automatic prompt engineering in RE and found that, on average, APE outperforms traditional prompt engineering techniques. However, research is still limited.

\subsection{Evaluating LLM Classifiers} \label{Evaluation of LLM Classifiers}
% Evaluation of classifiers (ECSER, papers mentioned in ECSER)
Few papers have been written specifically about the evaluation of LLM-based classifiers, but we can look at the evaluation of classifiers and LLMs separately to get a picture. The ECSER pipeline is the most relevant for evaluating classifiers, since it gives in-depth recommendations about the conducting and reporting of classifier research \cite{Dellanna2022}. Specific recommendations include the reporting of the full confusion matrix, reporting other metrics relevant to the domain such as specificity (true negative rate), analysing overfitting and degradation, visualising the ROC and applying statistical significance tests. Some researchers also recommend reporting the Matthews correlation coefficient (MCC), which combines all four metrics of the confusion matrix and can be more informative than the F1 score \cite{Chicco2020,Foody2023,Yao2020}. 

\textcite{hou2024} looked at whether these metrics were reported in LLM-based classification research and found that out of 147 papers that use LLMs for classification, the most frequently reported metric is precision with 35 papers, followed by recall (34), F1-score (33) and accuracy (23), but the AUC was reported 9 times, the ROC only 4 times and the MCC only twice. Unfortunately, they did not count how many studies did significance testing. These results are very similar to \textcite{Dellanna2022}'s aforementioned mapping study of classifier research as a whole and suggest that many important metrics are under-reported in LLM-based classification research. 

% Evaluation of LLMs/LLM classifiers (guo 2023, chang 2023) 
Using and evaluating LLMs comes with more considerations than just the accuracy of the results. \textcite{guo2023survey} categorize the evaluation of LLMs into three types: knowledge and capability evaluation, which assesses the fundamental knowledge and reasoning capabilities of the LLM; alignment evaluation, which refers to evaluating ethical and moral considerations like bias; and safety evaluation, which focuses on the robustness of LLMs and risk evaluation. \textcite{liu2024} have created guidelines specifically for evaluating LLM alignment in terms of reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms and robustness. 

Another consideration is how well the LLM outputs adhere to the format specified in the prompt. For example, it is common to ask an LLM to format its answer as a valid JSON object, allowing for easier processing of the output. Limiting responses to a specified format can be beneficial: \textcite{tam2024format} compared the performance of several LLMs between free-form response and formatted responses and found that, while reasoning ability was weakened for formatted responses, classification accuracy was increased. However, LLMs do not always follow the format accurately. \textcite{long2025format} define several measures to analyse the level of adherence to the output format: SysE, which measures the performance of the LLM for answers that \textit{meet the constraint}; TrueE, which measures the performance of all answers \textit{regardless of whether the format is satisfied}; and BiasF, which measures the (mean squared) difference between SysE and TrueE. \textcite{Li2024format} introduce the JScore measure, which measures the similarity between JSON objects and can be used to compare LLM generated JSON objects with the target JSON objects. Lastly, \textcite{xia2024fofo} introduce the FoFo benchmark for evaluating LLMs based on their ability to follow complex, domain-specific formats. 

\textcite{chang2023} conducted another survey of the evaluation of LLMs which focuses on downstream applications, resulting in a classification of four aspects of LLM evaluation: the accuracy, containing measures of correctness such as the F1 score; the calibration, which contains measures for the degree of alignment between the predicted probabilities and actual probabilities, such as the expected calibration error (ECE); the fairness, with measures pertaining to the equal treatment of different groups, such as the equalized odds difference (EOD); and robustness, with measures pertaining to the performance of a model in the face of challenging inputs and noise, such as performance drop rate (PDR). Further, they mention several ways in which human evaluation can be used to evaluate LLMs, such as the degree to which the model outputs align with human values.

Lastly, the model temperature, which regulates the randomness of the model, can also affect its performance. \textcite{peeperkorn2024} looked at the effect of model temperature on the level of creativity of the outputs and found that LLMs generate slightly more novel outputs with higher temperature, but the temperature was also correlated with incoherence. No relationship was found between temperature and cohesion or typicality. Model temperature might also affect a model's susceptibility to security attacks: \textcite{Yu2024} found that some LLMs became more susceptible to jailbreaking attacks as temperature increased, whereas others had decreased susceptibility to jailbreaking with increased temperature, possibly due to the fact that the former models had a lower susceptibility at 0 temperature and the latter models had a higher susceptibility at 0 temperature.


\section{RQ1: What is the current state of reporting in LLM4SE classification research?}
\label{RQ1}

In this section, we first discuss the methodology used to conduct a systematic mapping study on the state of reporting of LLM4SE classification research. Subsequently, we discuss the results from the mapping study and draw conclusions from these results.

\subsection{Research Approach}
To answer the research question, we conducted a mapping study of SE papers published in 2024 that use LLM classifiers using the guidelines by \citeauthor{kitchenham2011} \cite{kitchenham2011,kitchenham2007} and \textcite{petersen2008map}, consisting of five steps:
\begin{enumerate}
	\item Define the research question.
	\item Conduct a search for primary studies.
	\item Screen papers based on inclusion/exclusion criteria.
	\item Classify the papers.
	\item Extract data. 
\end{enumerate}

What follows is a detailed summary of the search strategy (steps 2 and 3) and the information that was obtained from the list of relevant papers (steps 4 and 5). 

\textbf{Search Strategy}\\
To get a complete image of the state of LLM4SE classifier evaluation, we collected all 528 papers published in 2024 in three of the top SE conferences and journals (Table \ref{table:venues}). These venues were narrowed down from the list of venues included in \cite{hou2024} based on their impact and relevance. Using the full list of 528 papers, an automatic keyword search was conducted on the title and abstract of each paper to identify the papers that were potentially relevant to LLM4SE.

The list of keywords was based on previous research (\cite{zhang2024,hou2024}) and designed with the intent of high recall, since it is easier to remove false positives than to retrieve false negatives. Keyword matching was done on the lowercased versions of the title and abstract and allowed for partial matches (e.g., "llm" matches "LLM", "LLMs", "LLM4SE", etc.). The complete list of keywords is as follows:
\begin{itemize}
	\item \textit{llm, large language model, language model, code generation model, plm, pre-trained, pretrained, pre-training, pretraining, chatgpt, gpt, bert, t5, llama, bart.}
\end{itemize}

The keyword search resulted in 129 potentially relevant papers. After manually removing false positives, mostly caused by the authors comparing their non-LLM model to previous LLM models, the total number of identified LLM4SE papers was 116, representing 21.97\% of total SE papers. 

To narrow down the list from all LLM-related papers to those pertaining to SE classification research, the title and abstract of the remaining 116 papers were manually screened for the inclusion and exclusion criteria (Table \ref{table:criteria}). These criteria were designed to exclude any paper not relevant for answering the research question, such as meta studies and those not related to classification. During the manual search, 18 papers were identified that use LLMs for SE classification research and satisfy all other criteria. 

\begin{table}[H]
	\raggedright
	\caption{Publication venues included in the mapping study.}
	\begin{tabularx}{\textwidth}{|c|X|} \hline
		\textbf{Acronym} & \textbf{Venue} \\ \hline
		ICSE  & International Conference on Software Engineering \\ \hline
		FSE   & International Conference on the Foundations of Software Engineering \\ \hline
		TSE   & Transactions on Software Engineering \\ \hline
	\end{tabularx}
	\label{table:venues}
\end{table}

\begin{table}[H]
	\caption{Inclusion/Exclusion Criteria}
	\begin{tabularx}{\textwidth}{|X|} \hline
		\textbf{Inclusion Criteria} \\ \hline
		The paper uses an LLM.  \\
		The paper focuses on a downstream application of the LLM. \\
		The downstream application is a SE classification task. \\
		The full text of the paper is accessible. \\
		The paper was published in 2024. \\ \hline
		\textbf{Exclusion Criteria} \\ \hline
		The paper focuses on LLM architecture with no downstream application. \\
		The paper or task is not relevant to SE. \\
		The full text is inaccessible. \\
		The paper is a survey or other meta-analysis. \\
		The paper focuses on research methodology. \\
		The paper is a reprint or different version of another paper. \\ \hline
	\end{tabularx}
	
	\label{table:criteria}
\end{table}

While only classification papers were included in the final analysis, all the papers that applied LLMs to SE tasks (a total of 83 papers) were additionally classified based on the type of LLM application (classification, generation or recommendation) to get a view of the relative academic interest in each type (Figure \ref{fig:LLMTypes}). As can be seen, LLMs are most often used for generation, with 73.5\% of papers using them in this way, including some that use generation alongside classification. However, classification represents the second largest application of LLMs, with 22.9\% in total. Appendix \ref{appendix:ref} contains references for Figure \ref{fig:LLMTypes} and other pie charts contained in this thesis.

\begin{figure}[h]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=0.5\linewidth]{figures/LLMTypes.png}
	\caption{Distribution of LLM application types in collected studies.}
	\label{fig:LLMTypes}
\end{figure}


\textbf{Information Extraction}\\
After completing the manual search and obtaining 18 relevant LLM4SE classification papers, the evaluation and reporting practices of each paper were manually analysed and recorded. The information that was extracted from the papers was determined based on what is recommended to be included by the ECSER pipeline \cite{Dellanna2022}, metrics that are recommended by other authors (such as the Matthews correlation coefficient \cite{Chicco2020,Foody2023,Yao2020}) and recommendations from the field of LLM4SE. For the latter, we recorded which metrics (if any) were reported for each of \textcite{chang2023}'s classification of general metrics: the accuracy, calibration, fairness \& robustness. Since the accuracy was already covered by the ECSER pipeline, this resulted only in the addition of calibration, fairness \& robustness to the final list of extracted information. 

The calibration of an LLM refers to how well the predicted probabilities of the model align with actual probabilities \cite{nixon2019calibration,guo2017calibration}. Evaluating the fairness of an LLM is done to ensure the alignment of LLM results with human values and prevent a difference in performance across groups of people \cite{liu2024}. The robustness refers to the performance of a model in the face of challenging inputs and can be analysed in several ways, including by making changes in the data distribution, the addition of noise to the data, and adversarial attacks \cite{chang2023}.

In total, the following list of evaluation metrics or evaluation metric types were added: \textit{precision, recall, accuracy, F-score, Area under curve (AUC), Matthews correlation coefficient (MCC), Confusion matrix, Receiver Operation Characteristic (ROC) plot, Calibration, Fairness} and \textit{Robustness}.

Further, we looked at several factors related to the reproducibility and generalisation of results, particularly whether justification was given for why the chosen evaluation metrics were used, whether results were obtained for multiple datasets, whether the results were compared to external baselines and whether the statistical significance was analysed and the choice of significance test justified. 

Lastly, for the papers that used a prompt-based approach for classification, we analysed whether the prompt approach and final prompt text were reported and justified, whether the results were evaluated over multiple prompts and whether the temperatures of the models were reported.

\subsection{Mapping Study Results}
\begin{figure}[h]
	\hfill
	\begin{minipage}{0.45\textwidth}
		\includegraphics[width=1\textwidth]{figures/Venues.png}
		\caption{Distribution of venues in mapping study.}
		\label{fig:Venues}
	\end{minipage}
	\hfill
	\begin{minipage}{0.45\textwidth}
		\includegraphics[width=1\textwidth]{figures/Prompts.png}
		\caption{Distribution of prompt usage in mapping study.}
		\label{fig:Prompts}
	\end{minipage}
	\hfill
\end{figure}

During the search process, 18 papers were identified as relevant for answering the research question. Each venue included in the search strategy is represented in the final selection, with eight ICSE papers, eight TSE papers and two FSE papers (Figure \ref{fig:Venues}). By including papers from three of the most influential Software Engineering venues, we increase the likelihood that the results are representative of the field. 
	
One of the first things we analysed was whether the papers used a prompt-based approach to LLM classification or another approach such as fine-tuning, since additional information about the prompt engineering approach was extracted from the papers where prompts were used. Figure \ref{fig:Prompts} shows that 44.4\% or 8 papers used a prompt-based approach, while 55.6\% or 10 papers used a different approach. Note that additional references for the figures and the subsequent tables are included in Appendix \ref{appendix:ref}.


\begin{table}[H]
	\centering
	\captionsetup{justification=centering}
	\caption{Summary of mapping study results.}
	\begin{tabularx}{0.7\textwidth}{|X|c|}
		\hline
		\textbf{Category} & \textbf{Total} \\
		\hline
		Total mapping study papers & 18 \\
		\hline
		\textbf{Evaluation metrics} & \\
		\hspace{3mm} Precision & 17 (94.4\%)  \\
		\hspace{3mm} Recall & 17 (94.4\%) \\
		\hspace{3mm} Accuracy & 11 (61.1\%) \\
		\hspace{3mm} F-Score & 17 (94.4\%)\\
		\hspace{3mm} AUC & 3 (16.7\%) \\
		\hspace{3mm} ROC plots & 0 (0.0\%) \\
		\hspace{3mm} MCC & 0 (0.0\%) \\
		\textbf{Calibration} & 0 (0.0\%) \\
		\textbf{Fairness} &  0 (0.0\%) \\
		\textbf{Robustness} &  \\
		\hspace{3mm} Data distribution \& Adversarial Attacks & 1 (5.5\%)  \\
		\hspace{3mm} Noisy data & 1 (5.5\%) \\
		\textbf{Metrics justification} &\\
		\hspace{3mm} No & 9 (50.0\%) \\
		\hspace{3mm} Implicit & 0 (0.0\%) \\
		\hspace{3mm} Previous work & 5 (27.8\%) \\
		\hspace{3mm} Yes & 4 (22.2\%) \\
		Confusion matrix & 4 (22.2\%) \\
		Evaluation over multiple data sets & 5 (27.8\%) \\
		\textbf{Type of baseline} &\\
		\hspace{3mm} None & 2 (11.1\%) \\
		\hspace{3mm} Own & 3 (16.7\%) \\
		\hspace{3mm} External & 4 (22.2\%)\\
		\hspace{3mm} External and Own & 9 (50.0\%) \\
		Analysis of statistical significance & 5 (27.8\%) \\
		\textbf{Statistical significance justification}&\\
		\hspace{3mm} No & 4 (80.0\%)  \\
		\hspace{3mm} Implicit & 0 (0.0\%) \\
		\hspace{3mm} Previous work & 1 (20.0\%) \\
		\hspace{3mm} Yes & 0 (0.0\%)  \\
		\hline
	\end{tabularx}
	\label{table:mapping_results}
\end{table}

Table \ref{table:mapping_results} shows a summary of the information that was extracted from each paper and how many papers reported each piece of information. This includes which evaluation metrics were reported; whether the calibration, fairness and robustness of the models were analysed; whether the confusion matrix was reported; the level of justification for the reported metrics; whether the models were evaluated over multiple data sets; the type of baseline used; and whether the statistical significance of the results was reported and justified.

Starting with the evaluation metrics that are recommended by the ECSER pipeline \cite{Dellanna2022}, 17 papers reported the precision, recall and F-score of the results, while only 11 papers reported the accuracy and only four papers reported the full confusion matrix. Although less known, some authors also recommend reporting the Matthews Correlation Coefficient (MCC), since it takes the full confusion matrix into account and might therefore give a more balanced representation of the results than the F-score \cite{Chicco2020}, but it was not reported in any of the papers. Further, the AUC, which some researchers consider to be a theoretically and empirically better metric for comparing classification models than the accuracy \cite{huang2005auc,ling2003auc}, was reported in only three papers, while none provided a plot of the full ROC curve.

Apart from metrics related to the correctness of the classification results, we extracted three other types of LLM evaluation that are frequently used: the calibration, fairness and robustness \cite{chang2023}. None of the papers reported measures of the calibration or fairness of the LLMs.  Only two papers tested the robustness of their models in one or more of these ways; one looked at both altering the data distribution as well as adversarial attacks and the other looked at the effect of noisy data. However, some papers tested something similar to robustness by performing ablation studies, which work by removing parts of the model to see the impact on the performance, which allows one to assess whether the model exhibits graceful degradation \cite{meyes2019ablation}. Ablation studies were not initially included in the mapping study but after seeing their successful implementation were added to the \newecser pipeline as an optional part of evaluating the robustness.


We also looked at whether justifications were provided in the papers for the selection of metrics they chose to include or exclude from the results and whether these justifications went beyond referring to previous research, which runs the risk of bad habits propagating through the field \cite{Dellanna2022}. Only nine papers included a justification for their choice of metrics, with five of those exclusively referring to previous research as a justification, while four provided a more elaborate explanation.

Lastly, we looked at methodological information about the evaluation and comparison of models and found that only five papers evaluated their models over multiple data sets. Further, 13 papers compared their results to external baselines (nine of which compared to both external and their own baselines), though three papers only compared the results to their own baselines and two papers made no comparisons. 

Despite almost all papers comparing their results to their own baselines or other research and usually making explicit claims about the superior performance of their models, only five papers did any statistical significance testing on their results. Some papers even claimed their results show a \textit{significant} improvement despite not doing or not reporting on the significance testing. Further, of the five papers that did a statistical analysis of the results, only one justified the statistical test that was chosen, by referring to previous research. 


\begin{table}[H]
	\centering
	\captionsetup{justification=centering}
	\caption{Summary of prompt reporting results in the mapping study.}
	\begin{tabularx}{0.7\textwidth}{|X|c|}
		\hline
		\textbf{Category} & \textbf{Total} \\
		\hline
		Total prompt-based classification papers & 8 \\
		\hline
		\textbf{Prompt approach} & \\
		\hspace{3mm} Chain-of-verification & 1 (12.5\%)  \\
		\hspace{3mm} Few-shot & 2 (25.0\%)  \\
		\hspace{3mm} Mimic-in-the-background & 1 (12.5\%) \\
		\hspace{3mm} Not reported & 1 (12.5\%) \\
		\hspace{3mm} Zero-shot & 3 (37.5\%)  \\
		\textbf{Final prompt} &\\
		\hspace{3mm} Not reported & 2 (25.0\%)\\
		\hspace{3mm} Own & 6 (75.0\%) \\
		\textbf{Prompt approach justification} &\\
		\hspace{3mm} No & 5 (62.5\%)  \\
		\hspace{3mm} Yes & 3 (37.5\%)  \\
		\textbf{Final prompt justification} & \\
		\hspace{3mm} No & 4 (50.0\%)  \\
		\hspace{3mm} Previous Work & 1 (12.5\%) \\
		\hspace{3mm} Yes & 3 (37.5\%)  \\
		\textbf{Evaluation over multiple prompts} &\\
		\hspace{3mm} No & 8 (100.0\%) \\
		\textbf{Temperature} & \\
		\hspace{3mm} No & 7 (87.5\%) \\
		\hspace{3mm} Yes & 1 (12.5\%) \\
		\hline
	\end{tabularx}
	\label{table:prompt_results}
\end{table}


Eight of the papers used prompting to obtain results from pre-trained LLMs, which comes with additional considerations and challenges, such as what prompt approach to use (few-shot, zero-shot, etc.) and how to effectively write the text of the prompt. Table \ref{table:prompt_results} shows a summary of the reported prompting information of the papers that used prompts in the mapping study. 

Firstly, papers used various prompt design approaches; one used chain-of-verification, two used few-shot, one used a novel approach called mimic-in-the-background, three used zero-shot prompting, and one paper failed to report the prompt approach used or didn't use a systematic approach. However, only three papers provided justification for the chosen prompt approach. 

Secondly, the final prompt, which refers to the actual prompt text used with possible placeholder values for data insertion, was only reported in six out of eight papers. Further, the authors of the six papers that provided the prompt wrote the prompts themselves without consulting established patterns, which means there is no prior evidence of the efficacy of the prompt pattern. Only four papers provided a justification for how the prompt was designed, one of which referred to previous work.

The performance of LLMs can be heavily impacted by minor changes in the prompt design, which is why \textcite{sclar2024quantifyinglanguagemodelssensitivity} recommend comparing multiple plausible prompts. However, none of the papers evaluated the performance of their models using multiple prompts, which leaves the vulnerability of the models to prompt variability unknown. Lastly, only one paper reported the temperature used for the LLM, despite this parameter having a potential impact on the performance of LLMs \cite{peeperkorn2024}.

\textbf{Conclusion}\\
Evidently, the results confirm a pattern of incomplete reporting and unsubstantiated claims similar to that found previously in the fields of ML4SE \cite{Dellanna2022} and LLM4SE \cite{hou2024}. Metrics such as the AUC, ROC plots, the MCC, the full confusion matrix and metrics related to the calibration, fairness and robustness of LLMs go completely or almost completely unreported.

Since none of the included papers seemingly involve human participants or sensitive applications, this could be a valid reason for not evaluating the fairness, but these considerations are not discussed in any of the papers, nor do the papers explain the absence of calibration and robustness metrics. 

Even when metrics \textit{are} reported, there is often no explanation as to the why the metrics were chosen for the specific tasks. Further, many papers only use one data set and sometimes make no comparisons to external baselines. Further, many papers make unsubstantiated claims about the relative performance of their models without significant statistical backing and when significance testing is performed, little explanation is given to the choice of test and significance threshold. 

For papers that use prompts for LLM classification, there is often little to no explanation for the chosen prompt approach and the design of the final prompt. Further, the prompt texts are generally not based on established patterns with empirical backing, but designed without a systematic approach. Lastly, none of the papers evaluated multiple prompts to avoid the potential sensitivity of LLMs to minor changes in wording, and the temperature of LLMs goes largely unreported as well. 

Naturally, although multiple venues were considered and all papers from 2024 were considered from these venues, these results are still based on a limited set of papers. However, the results align with findings from a large base of previous research \cite{Dellanna2022,hou2024,guo2023survey,kitchenham2002,Menzies2012}, so it is not unexpected to find similar problems here, especially since there are still very few guidelines for conducting LLM research.

In conclusion, there is a clear need for easy to use and comprehensive guidelines to aid researchers in conducting LLM4SE classification research, especially for statistical significance testing and prompt engineering. We designed the \newecser pipeline (section \ref{RQ2}) to take into account the mentioned common shortcomings of existing research and provide extra attention to these topics. 

\section{RQ2: What resource can be developed to assist researchers in conducting and reporting on LLM4SE classification research?}
\label{RQ2}
In this section, we first discuss the approach that was used to modify the ECSER pipeline with recommendations tailored to the conducting and reporting of LLM4SE classification research. Subsequently, we present the \newecser pipeline, which features a step by step guide to the implementation and results reporting of LLM-based classification research. 

\subsection{Research Approach}
To answer the research question, we developed the \newecser pipeline by enhancing the existing ECSER pipeline to include the use of LLMs, taking into consideration recommendations from the fields of LLM4SE and ML4SE, as well as relevant statistical or methodological research in other fields (see section \ref{Related Work}). 

The ECSER pipeline was chosen as the basis for the development of a new LLM-specific resource, since most ECSER recommendations apply to all classification research, including LLM-based classification \cite{Dellanna2022}. However, using LLMs comes with additional challenges that weren't considered during the development of the original pipeline, such as the use of prompts to guide pre-trained LLMs or the alignment of LLM behaviour with human values \cite{liu2024}. 

To enhance the ECSER pipeline with LLM-specific recommendations, we consulted various existing guidelines and surveys for using and evaluating LLMs, including \textcite{hou2024}'s systematic literature review of LLM4SE, \textcite{chang2023}'s survey on the evaluation of LLMs, \textcite{Marvin2024}'s chapter on prompt engineering and \textcite{baltes2025guidelinesempiricalstudiessoftware}'s guidelines for using LLMs in SE, among many others. These higher level sources lead to more specific sources and recommendations that were included in the pipeline.

Additionally, the results of RQ1 were taken into account to ensure that common issues encountered in the mapping study received additional care in the new pipeline, including issues such as the lack of reporting for common evaluation metrics, the lack of significance testing and the lack of consistent reporting for prompt usage and development. The results of RQ1 also lead to the inclusion of ablation studies as part of the the pipeline (S8), which were overlooked during the initial literature review but were shown to be useful in assessing the graceful degradation of the models in the papers that used them \cite{meyes2019ablation}.

In Table \ref{table:additions}, we list each step of the original ECSER pipeline along with the new version of the step and the major changes that were made in the new \newecser pipeline, along with the primary source(s) that motivated these changes. Minor changes are not included in the table.

\begin{table}[H]
	\centering
	\caption{Summary of changes and additions to the ECSER pipeline.}
	\begin{tabularx}{\textwidth}{|p{3cm}|p{3.5cm}|X|}
		\hline
		\textbf{\textit{ECSER} Step} & \textbf{\newecser Step} & \textbf{Changes from \textit{ECSER} to \newecser} \\ \hline
		
		S1. Select an evaluation method and split the data.
		& S1. Select an evaluation method and split the data.
		& Add specification of desired level of model alignment and safety \cite{guo2023survey}. \\ \hline
		
		\multirow[c]{2}{*}{\parbox{3cm}{S2. Train the model.}}
		& \multirow[c]{2}{*}{\parbox{3.5cm}{S2. Fine-tune the model}} 
		& Change focus to fine-tuning LLMs. \\ 
		& & Add reporting of LLM configuration and interaction \cite{baltes2025guidelinesempiricalstudiessoftware}. \\ \hline
		
		\multirow[c]{2}{*}{\parbox{3cm}{-}}
		& \multirow[c]{2}{*}{\parbox{3.5cm}{S3. Design Prompts.}}
		& Add prompt engineering guidelines \cite{arvidsson2023,Marvin2024}.\\
		& & Add reporting of LLM configuration and interaction \cite{baltes2025guidelinesempiricalstudiessoftware}. \\ \hline
		
		\multirow[c]{1}{*}{\parbox{3cm}{S3. Hyper-parameter tuning \& validation.}}
		& \multirow[c]{1}{*}{\parbox{3.5cm}{S4a. Hyper-parameter tuning}}
		& Tune LLM model temperature and other hyper-parameters \cite{peeperkorn2024, baltes2025guidelinesempiricalstudiessoftware}. \\ \hline
		
		\multirow[c]{2}{*}{\parbox{3cm}{-}}
		& \multirow[c]{2}{*}{\parbox{3.5cm}{S4b. Prompt comparison.}}
		& Compare similar prompts \cite{sclar2024quantifyinglanguagemodelssensitivity,ronanki2023}. \\ 
		& & Report prompt revisions and LLM interaction logs \cite{baltes2025guidelinesempiricalstudiessoftware} \\ \hline
		
		S4. Re-train with optimized parameters. 
		& -
		& Merged into S4a.\\ \hline
		
		S5. Test the models.
		& S5. Test the models.
		& - \\ \hline
		
		S6. Report confusion matrix.
		& S6. Report confusion matrix.
		& - \\ \hline
		
		S7. Report Metrics.
		& S7. Report Metrics.
		&  Report Matthews Correlation Coefficient (MCC) \cite{Chicco2020,Foody2023,Yao2020} \\ \hline

		\multirow[c]{2}{*}{\parbox{3cm}{-}}
		& \multirow[c]{2}{*}{\parbox{3.5cm}{S8. Evaluate calibration, fairness, robustness \& sustainability.}}
		& New step that includes guidelines on how to evaluate the calibration, fairness, robustness and sustainability of LLMs  \cite{chang2023,liu2024}. \\ & & Conduct ablation studies \cite{meyes2019ablation}. \\ \hline
		
		S8. Analyse overfitting and degradation.
		& S9. Analyse overfitting and degradation.
		& - \\ \hline
		
		S9. Visualize ROC.
		& S10. Visualize ROC.
		& - \\ \hline
		
		S10. Apply statistical significance tests.
		& S11. Apply statistical significance tests.
		& Justify the significance test and significance threshold \cite{benjamin2018significance,amrhein2017significance}. \\ \hline
	\end{tabularx}
	\label{table:additions}
\end{table}


%		\multirow[c]{4}{*}{\parbox{3.5cm}{S7. Report Metrics}}
%		& \multirow[c]{4}{*}{\parbox{4cm}{S7. Report Metrics}}
%		& Report Matthews correlation coefficient (MCC) \cite{Chicco2020,Foody2023,Yao2020} \\ 
%		& & Report calibration (e.g., expected calibration error \cite{Pakdaman2015}) \\ 
%		& & Report fairness (e.g., equalized odds difference \cite{Woodworth2017}) \\ 
%		& & Report robustness (e.g., performance drop rate \cite{zhu2024}) \\ \hline

\subsection{The \newecser Pipeline}
\label{ECSER-LLM}
In this section we present the \newecser pipeline, a step-by-step guideline for SE researchers to conduct classification experiments using LLMs and evaluate the results, which serves to provide an answer to the research question. The pipeline encompasses the \textit{treatment validation} step of \textcite{Wieringa2014}'s design cycle, which happens after the \textit{treatment design} step. Steps belonging to the \textit{treatment design}, such as feature engineering and algorithms selection, are out of scope of the pipeline. 

Figure \ref{fig:ECSER-LLM} shows the full pipeline, consisting of 11 steps divided into two macro activities: (1) training, validation and testing and (2) results analysis. The steps are shown consecutively, but it is always possible to return to previous steps. For example, after finding optimal hyper-parameters in S4, one will likely return to S2 to retrain the models with these parameters. Further, there is a possible feedback loop between macro activities, including the \textit{treatment design}. For example, when it is found that the model shows signs of overfitting (S9), one can reconsider the algorithms used (\textit{treatment design}) or increase regularization (S2).

Some steps are marked as optional with a dashed border, since they depend on the \textit{treatment design} or other considerations such as time and space constraints. In particular, when using a pre-trained LLM without fine-tuning, S2 (Fine-tuning) becomes obsolete since no training is performed, and when \textit{not} using prompts to guide LLM outputs, S3 (Design prompts) and S4b (Prompt comparison) become obsolete since they provide guidelines for using prompts. Lastly, S4 is split into two steps: (a) hyper-parameter tuning and (b) prompt comparison. This is done to indicate that these steps can be done in parallel to find the best combination of prompt and/or hyper-parameters, but since both are marked as optional it is possible to do only one of the steps or neither. 

\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=1\linewidth]{figures/ECSER-LLM.png}
	\caption{The \newecser pipeline. Dashed borders indicate optional steps. Double arrows show possible feedback loops between macro activities. The steps in S4. can be done in parallel.}
	\label{fig:ECSER-LLM}
\end{figure}

\subsubsection{Training, Validation, Testing}
\textbf{S1. Select an evaluation method and split the data.}\\
In experimental software engineering, the treatment that is designed to answer a research question generally consists of a model that must be trained using a data set, but how well the trained model fits the training data is not a good indicator of the performance of the model on unseen data, since the training data is only a sample of the true data distribution. Therefore, some of the available data must be set aside to test whether the trained model generalises well to data that wasn't encountered in the process of training the model; and in the case of hyper-parameter tuning, some of the data must be used to evaluate the performance of models with different parameters. 

When using a pre-trained LLM with no additional training, it is not needed to set aside data for training, but the data should still be split into validation and test sets. The validation set should be used to select hyper-parameters for the LLM, such as the temperature, and during the development and comparison of prompts \cite{Homiar2025promptdev}. The test set should only be used when the final prompt is developed (along with any other part of the model) to ensure the prompt design was not influenced by the test set. 

Several methods exists to partition data into training, validation and test sets. The simplest is the \textit{holdout method}, where the data is split into training, validation and test sets. The training set is used for fine-tuning (S2), the validation set is used for prompt development and hyper-parameter tuning (S3 \& S4), and the test set is used for evaluating the model (S5). Generally, at least 50-70\% of the data is used for training and validation, with the rest being used for testing \cite{Bichri2024traintest,xu2018splitting}. The holdout method does have a major issue: since only a portion of the training data is used for validation, the test error rate can be highly variable, leading to unreliable results \cite{kim2009holdout,James2023isl5}. 

A more robust alternative is \textit{resampling}, where models are fitted on multiple samples drawn from the training data. One example is \textit{k-fold cross-validation}, where the training set is split into \textit{k} groups (or folds) of equal size, chosen randomly from the data. The model is trained $k$ times, once for each fold, but each time a different fold is used as the validation set, with the remaining $k-1$ folds being used for training. The result of this process is $k$ estimates of the test error, the average of which can be used for hyper-parameter selection \cite{James2023isl5}. A special case of k-fold cross-validation is \textit{leave-one-out cross-validation} (LOOCV), where $k$ is set to $N$, with $N$ being the number of observations in the training set. This means that the model is trained $N$ times on $N-1$ observations, with a different observation being used as the validation set each time. LOOCV is more computationally expensive than k-fold cross-validation, although efficient algorithms exist for some applications that don't require training the model $N$ times \cite{Cawley2003,wang2018approximateloocv}. In terms of performance, LOOCV has a lower bias than k-fold cross-validation, at the cost of a higher variance \cite{Cawley2003,Wong2015}.

It is also possible to use projects to partition the data using \textit{leave-one-project-out cross-validation} (LOPO), where the training data is not split randomly, but grouped with all the data belonging to the same SE project. Each project is set aside in turn to serve as the validation set, while the other projects are used for training \cite{Herbold2020,Dalpiaz2019}. The advantage of LOPO is that a different project is used for validation than what is used for training, which possibly enhances the generalisability of the results \cite{scikit-learn-cv2025}.

Which of these methods is most appropriate might depend on the available computational power and time, with methods like LOOCV requiring much more training than the holdout method; and whether the data is obtained from distinct projects. In all cases, 10-30\% of the data should be set aside for testing and not touched until all parameters are tuned and prompts are finalised.

Finally, when using a pre-trained LLM, if the researcher uses open source data, there is a legitimate concern that the validation and testing sets were also used in training the LLM, leading to data leakage. LLMs have been shown to sometimes repeat training data verbatim  \cite{carlini2021extractingtrainingdatalarge}. If possible, this can be avoided by using testing data that was released after the creation of the LLM or proprietary data. \\

\textbf{S2. Fine-tune models.} \\ 
\infobox{Optional}{This step does not apply when using a pre-trained LLM without fine-tuning or additional training, such as research that only uses prompts to guide the output of the LLM.}
After splitting the data and deciding on the evaluation method, the training set is used to fine-tune the LLM or train a classification model on the LLM encodings. Training or fine-tuning a model entails changing the \textit{parameters} of the model to produce outputs that are most similar to the supervised labels in the training data. The training process is usually influenced by \textit{hyper-parameters} that are set before training and don't change during training. For example, when fine-tuning an LLM, the learning rate, batch size and epochs are examples of hyper-parameters that influence the training process, but the weights of the model are the parameters that are actually changed during training. 

Since LLMs are often updated after release, the exact LLM version, configuration and experiment dates should be reported to make it easier for other researchers to replicate findings \cite{baltes2025guidelinesempiricalstudiessoftware}. Additionally, when using fine-tuning, the fine-tuning data and weights should be shared whenever possible.

It should be noted that training a traditional classification model on LLM encodings also falls under the scope of the original ECSER pipeline \cite{Dellanna2022}, which deals exclusively with traditional classification models. However, additional information should still be included about the way the LLM is used, its version, experiment dates and configuration (including seed); and the additional evaluation of calibration, fairness, robustness \& sustainability should still be considered when using LLM encodings.

See \textcite{parthasarathy2024hyperUltimate} for a detailed guide to fine-tuning LLMs.\\


\textbf{S3. Design prompts.} \\ 
\infobox{Optional}{This step only applies when using prompts to guide LLM outputs, instead of using only fine-tuning or using LLM embeddings with traditional classifiers.}
Prompt engineering is an important part of interacting with LLMs, since the prompt design can have a substantial effect on the performance of the LLM. We split the process of designing a prompt into five steps, based on \textcite{Marvin2024}.

1. \textit{Define the goal of the prompt}: the first step of prompt engineering is to clearly define the goal of a prompt. Setting a goal for the type of output that you desire the LLM to produce helps inform the content of the prompt \cite{Marvin2024}. For example, having the goal of producing requirement classification results that resemble those of a human expert might inform the choice to add a stipulation to the prompt: "act like an expert in requirements engineering". 

2. \textit{Choose the right prompt engineering approach}: there are many approaches to prompt engineering, but the most notable are zero-shot prompting, few-shot prompting and chain-of-thought prompting. Of these, the most popular are few-shot and zero-shot in that order. When providing examples in the prompts, these examples should be removed from the validation set before testing and refining the prompt or comparing different prompts (S4b). A relatively novel approach to prompt engineering is automatic prompt engineering (APE), which avoids the problem of manually having to write, test and refine prompts by automatically generating prompts \cite{zhou2023ape,zadenoori2025}. 

3. \textit{Design the prompt}: writing a high quality prompt is an often overlooked aspect of prompt engineering, with most researchers writing prompts from scratch without potentially putting much thought into the exact wording, despite small changes in wording sometimes having a large effect on the performance \cite{sclar2024quantifyinglanguagemodelssensitivity}. Some resources exist that make it easier to write high quality prompts, such as \textcite{white2023}'s catalogue of prompt patterns. These patterns represent techniques that have been successfully used to solve common problems, put into reusable formats. For example, the Persona pattern tells the LLM to take up a certain role (e.g., "act as a software engineering researcher"). Some suitable patterns for binary classification are Question Refinement, Cognitive Verifier, Persona, Template and Context Manager \cite{ronanki2023}. Another approach is adapting prompts from related research, to more consistently compare results and provide empirical backing to the prompt design. 

4. \textit{Provide context}: providing context related to the specific domain can be a useful way to improve the performance of a prompt and avoid irrelevant responses \cite{Marvin2024}. Context can include examples like in few-shot prompting, but also information such as the desired content of the output, the format of the output or additional information about the topic. 

5. \textit{Test and refine}: it is vital to test and refine the designed prompt, with respect to the defined goal. The validation set should be used to fill in any placeholder data, after which the output of the LLM with respect to the prompt can be used to evaluate the performance and whether the LLM adheres to the desired output format. Any changes made to the prompt or insights gained from the evaluation of the prompt, as well as the full interaction logs of the prompts, should be made available, since this information can be useful for reproductions and replications \cite{baltes2025guidelinesempiricalstudiessoftware}.

Lastly, whenever an impactful prompt engineering decision is made, such as choosing to use few-shot prompting instead of another method, the choice should be motivated in the paper. This can be done by explaining the choice based on the specific nature of the task or by referencing previous research which made the same choice. If certain information, such as the prompt or interaction log, cannot be provided for privacy or confidentiality reasons, this should be clearly stated and the researcher should still provide a summary of this information \cite{baltes2025guidelinesempiricalstudiessoftware}.\\

\textbf{S4a. Hyper-parameter tuning.}\\
\infobox{Optional}{This step only applies when fine-tuning or when using other configurable parameters that influence LLM outputs, like the temperature.}
Hyper-parameters are parameters that are set before training, as opposed to parameters that are optimized by training, such as the weights of an LLM. Some hyper-parameters are used during fine-tuning, such as the number of epochs, batch size and learning rate; whereas others influence the outputs of a pre-trained model, such as the temperature \cite{halfon2024hyper,peeperkorn2024}. Although hyper-parameters are not learned, they can have a substantial effect on the performance of an LLM \cite{halfon2024hyper}. For this reason, it is important to carefully select the hyper-parameters. 

Several methods exist to choose hyper-parameters. The simplest is manual search, where the researcher manually compares hyper-parameter values and develops an intuition for choosing the parameters, but this method makes it difficult to reproduce results \cite{bergstra2012hyperRandomSearch}. Grid search automates the process by testing every combination of different subsets of the hyper-parameter space. For example, the researcher picks the subset \{1e-5,1e-4,1e-3\} for the learning rate and \{5,10,20\} for the number of epochs and then fine-tunes with all nine combinations of parameters and continues with the parameters that have the highest performance on the validation set. Since grid search brute-forces all combinations of the chosen subsets, it can become very computationally expensive \cite{liashchynskyi2019hyperGridsearch}. 

A more efficient alternative to grid search is \textit{random search}. Random search works by drawing multiple samples of the hyper-parameters from a discrete or continuous distribution of the hyper-parameter space and fine-tuning using these samples \cite{bergstra2012hyperRandomSearch}. Another advantage of random search is that it can be stopped after any amount of trials, instead of only after the full grid has been explored. Lastly, some other less common methods for tuning are genetic algorithms \cite{Loussaief2018} and DODGE \cite{agrawal2022hyper}.

After the optimal hyper-parameters are determined, the model is retrained using the full training data, including any data that was set aside for validation. This can also be done using nested cross-validation, where the final model is trained at the same time as tuning the hyper-parameters.\\

\textbf{S4b. Prompt comparison.}\\
\infobox{Optional}{This step only applies when using prompts to guide LLM outputs, instead of using only fine-tuning or using LLM embeddings with traditional classifiers.}
The specific wording of a prompt can have a large impact on the performance of an LLM, even when the prompt retains its semantic content and intent \cite{sclar2024quantifyinglanguagemodelssensitivity, he2024doespromptformattingimpact,mizrahi2024stateartmultipromptllm}. This includes seemingly innocuous changes such as capitalisation. Thus, relying on a single human-written prompt could lead to unreliable results. One way to mitigate this problem is by writing multiple prompts that use different prompt patterns (such as Question Refinement, Cognitive Verifier and Persona) and compare the results using the validation set \cite{ronanki2023}. The comparison of different prompts can be done efficiently using PromptEval, a technique for estimating the performance of a large set of prompts, allowing one to pick the best prompt or evaluate the average performance over multiple prompts \cite{polo2024multiprompt}. However, these techniques require manual prompt writing, which still leaves the risk of specific word choices or syntax affecting the performance of the prompts. 

Alternatively, \textcite{sclar2024quantifyinglanguagemodelssensitivity} developed the algorithm FormatSpread, which is able to generate and evaluate a large set of plausible prompt formats, reporting the range of performances over these prompt formats. Some advantages of this technique include that it provides insight into the sensitivity of the original prompt design and that it gives a lower bound on the model's performance. A similar technique is Mixture of Formats (MOF), where the style of each few-shot example is modified by asking the LLM to change the style, leading to a mix of different styles being used, reinforcing model understanding \cite{ngweta2025MOFrobustness}.

The approach to prompt comparison that was used should be reported and justified in the paper, along with prompt revisions and interaction logs \cite{baltes2025guidelinesempiricalstudiessoftware}. The result of prompt comparison could be a single prompt which has shown good performance, which will then be used for the final evaluation; or a set of plausible prompts, in which case the evaluation will be done over this set of prompts. If no prompt comparison is done, the reason should be explained in the paper.\\

\textbf{S5. Test the models.}\\
Finally, after fine-tuning and determining the optimal hyper-parameters and/or prompts, the performance of the LLM is evaluated on the test set. It is recommended to include multiple data sets into the test set, to assess the generalisability of the model and to run statistical tests across these multiple data sets (S11) \cite{Dellanna2022}. \\

\subsubsection{Results Analysis}
 
\textbf{S6. Report the Confusion Matrix.}

 \begin{figure}[H]
	\noindent
	\renewcommand\arraystretch{1.5}
	\setlength\tabcolsep{0pt}
	\begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c c }
		\multirow{10}{*}{\parbox{1.1cm}{\bfseries\raggedleft \rotatebox[origin=c]{90}{Actual}}} &
		& \multicolumn{2}{c}{\bfseries Predicted} \\
		& & \bfseries Positive & \bfseries Negative \\
		& Positive& \MyBox{True}{Positive} & \MyBox{False}{Negative} \\
		& Negative & \MyBox{False}{Positive} & \MyBox{True}{Negative} \\
	\end{tabular}
	\caption{Generic confusion Matrix. All values add up to the number of data points.}
	\label{fig:confusion_generic}
\end{figure}
While it is common to select a few metrics appropriate to a given task and report them, we recommend reporting the full confusion matrix so readers can get a more complete picture of the results, and be able to calculate many different metrics, even when they aren't explicitly reported (see S7). The confusion matrix consists of the number of true positive (TP), false positive (FP), true negative (TN), and false negative (FN) classification results and are reported as whole numbers. Often, the sums of the rows or columns are also included, since adding up the numbers in the columns results in the total number of data points that were classified as positive or negative, while the row sums represent the number of data points that were actually positive or negative. Figure \ref{fig:confusion_generic} shows a generic confusion matrix. \\

\textbf{S7. Report Metrics.}\\
\begin{table}[bh]
	\centering
	\caption{Calculation of common metrics for classifier performance.}
	\label{table:metrics}
	\begin{tabularx}{\textwidth}{|p{3cm}|X|}
		\hline
		\textbf{Metric} & \textbf{Formula} \\
		\hline
		Precision & $TP / (TP + FP)$ \\
		Recall (TPR) & $TP / (TP + FN)$ \\
		Specificity (TNR) & $TN / (TN + FP)$ \\
		Accuracy & $(TP + TN) / (TP + TN + FP + FN)$ \\
		F$_1$-score & $2 \cdot (\text{Precision} \cdot \text{Recall})/(\text{Precision} + \text{Recall})$ \\
		F$_\beta$-score & $(1 + \beta^2)\text{Recall})/((\beta^2\cdot\text{Precision}) + \text{Recall})$ \\
		MCC & $(TP \cdot TN - FP \cdot FN) / \sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}$ \\
		\hline
	\end{tabularx}
\end{table}
Many metrics can be calculated from the confusion matrix, including the \textit{Precision}, \textit{Recall}, \textit{Sensitivity}, \textit{Accuracy}, F$_1$-score, F$_\beta$-score and Matthews correlation coefficient (MCC). Table \ref{table:metrics} shows the calculation of these metrics using the values from the confusion matrix. Which metrics are reported generally depends on the domain and the specific application. For example, the precision gives the percentage of positive classifications that are correct, so a high precision is desirable for applications where the consequences of false positives are high. On the other hand, the recall gives the ratio of actual positives that were correctly classified, so a high recall is desirable when the consequences of false negatives are high. In most applications, a good balance of precision and recall is preferred, which can be measured by the F$_1$-score, the harmonic mean of recall and precision. The F$_\beta$-score changes the weights of precision and recall in case one of the metrics is more important for the given task.

The specificity or true negative rate (TNR) is the ratio of actual negatives that are correctly classified as negative. The accuracy is the overall ratio of correct classifications and can be a useful metric when the classes are balanced, i.e. there is a similar amount of data points for positive and negative classes. If the classes are not balanced, the accuracy can be misleading \cite{Lones2024pitfalls, Haixiang2017imbalance}.

The F$_1$-score is commonly used in the field of LLM4SE, but it has several drawbacks: (1) the F$_1$-score varies when the classes are swapped (i.e., positive is renamed to negative and vice-versa) and (2) the F$_1$-score is independent of the number of data points correctly classified as negative (TN) \cite{powers2015f,Chicco2020}. The Matthews correlation coefficient (MCC) is recommended by many researchers as a more balanced alternative to the F$_1$-score, since it balances all four categories of the confusion matrix and does so proportionally to the number of positive and negative data points \cite{Chicco2020,Foody2023,Yao2020}. The MCC ranges between $-1$ and $1$, with $-1$ representing a total disagreement between the predicted and actual results, $0$ representing no correlation and $+1$ representing a perfect agreement.

Why specific metrics were included or omitted from the results should be justified in the text \cite{Kapoor2024}. Ideally, this can be done by explaining why the chosen metrics are mathematically or empirically appropriate for the given task. Otherwise, the researcher should refer to similar previous research that used these metrics. If certain metrics were omitted, the results should report enough information to allow the reader to calculate these metrics themselves, for example by reporting the full confusion matrix and providing the raw results as supplementary material.

The main metrics for comparing LLM classifications are obtained by evaluating on the test set. However, the researcher should also share the metrics that were obtained for the training and validation sets during fine-tuning, hyper-parameter tuning or prompt development/comparison, to give readers more insight into the development and aid replicability. Providing the mean and standard-deviation of the metrics for cross-validation can also provide insight into the performance. Additionally, when using a specific output format such as a JSON object, a distinction should be made between the performance of (1) the test cases where the format constraints were adhered to and (2) all test cases, regardless of format satisfaction \cite{long2025format}.\\

\textbf{S8. Evaluate calibration, fairness, robustness \& sustainability.}\\
\infobox{Optional}{Each of the aspects of LLM evaluation contained in this section is optional; but we recommend considering these aspects when dealing with human participants or users; or sensitive data and applications.}
The previous step (S7) considered how to evaluate the correctness of classification results. However, the correctness does not tell the full story: an accurate model does not necessarily work equally well for all groups of users, nor does the accuracy take into account the resistance of the model to noisy data, changes in distribution, adversarial prompts or the calibration of predicted probabilities \cite{liu2024,chang2023,guo2017calibration}. In this step, we briefly discuss how (and why) to evaluate the calibration, fairness, robustness \& sustainability of LLM classifiers.

\textbf{Calibration}\\
The calibration of an LLM refers to how well the confidence of the model aligns with the actual probability of getting a correct classification \cite{guo2023survey}.  When the confidence is well calibrated, we can get an indication when model outputs are likely to be incorrect or whether we can trust the outputs. The confidence of a prediction can be estimated in several ways, as discussed in \cite{tian2023justaskcalibrationstrategies}.  The most accurate estimate of the confidence can be obtained by calculating the conditional probability of the label given the prompt \cite{xiong2024confidence}. However, the probabilities of tokens can sometimes not be obtained for closed-source models, in which case the confidence can be estimated by sampling multiple responses and measuring the consistency or by prompting the model to give the confidence of the labels in the generated answer, which can be a numerical probability or a linguistic response (e.g., "Almost certain" or "Unlikely").

A common metric for measuring the calibration is the\textbf{ expected calibration error (ECE)}, which is obtained by dividing predictions into $M$ bins of equal size and calculating the average difference between the accuracy and confidence over all $M$ bins \cite{guo2017calibration}. For high-risk applications, the \textbf{maximum calibration error (MCE)} can be used to get the worst-case difference between the confidence and accuracy. See \textcite{nixon2019calibration} for a discussion of possible pitfalls when using the ECE and alternative metrics.

\textbf{Fairness}\\
The fairness of an LLM refers to the equal treatment across different groups of people \cite{chang2023}. Taking the fairness into account helps ensure the alignment of model behaviour to human intentions \cite{kenton2021alignmentlanguageagents}. \citeauthor{liu2024} list four types of unfairness: injustice, stereotype bias, preference bias and disparate performance. Injustice arises when similar individual are not treated similarly by the model. For example, when two indistinguishable job applications are entered into a prompt, with the only difference being irrelevant group attributes (such as gender), we expect the LLM to classify both applications the same way. Stereotype bias refers to an LLM's prejudiced or misleading expectation about members of particular social groups, such as a presumed academic ability based on race or gender. Preference bias refers to a lack of neutrality concerning subjective topics such as politics, scientific interpretations, societal matters or product preference. Lastly, disparate performance refers to a difference in performance across groups of users. For example, LLMs often perform worse on languages other than English, which means that applications using LLMs might perform worse in certain cultures and societies \cite{bang2023multitaskmultilingualmultimodalevaluation}.

To evaluate the fairness, \textcite{wang2024decodingtrust} list two metrics:
\begin{itemize}
	\item \textbf{Demographic Parity Difference (DPD)}: the DPD measures the difference between the probability of positive predictions given the presence of a sensitive attribute and the probability given the absence of that attribute. A high DPD means that there is a large difference in the positive predictions between groups. Note that the DPD does not consider the ground truth labels.
	\item \textbf{Equalized Odds Difference (EOD)}: the EOD takes the maximum difference between the true positive rate (recall) and false positive rate given the sensitive attribute. This means that a low EOD requires both classes to have similar prediction errors. By comparing the true positive rate and false positive rates, it also takes into account the ground truth labels. 
\end{itemize}

\textbf{Robustness}\\
The robustness of an LLM refers to how well it deals with difficult or unexpected inputs \cite{chang2023}. There are multiple scenarios in which an LLM can encounter challenging inputs, including through changes in the data distribution, noise in the data, adversarial prompts and poisoning of the training data \cite{liu2024,chang2023}. These scenarios can be caused by malicious actors (such as changing labels to poison the training data), user error (such as typos in the prompt) or other reasons (such as a change in user demographic). 

\textcite{chang2023} list two common robustness metrics:
\begin{itemize}
	\item \textbf{Attack Success Rate (ASR)}: the ASR evaluates the success of adversarial attacks by calculating the rate with which the attack successfully produces adversarial examples \cite{liu2024attacksuccesrate}. 
	\item \textbf{Performance Drop Rate (PDR)}: the PDR evaluates the robustness of prompts by quantifying the drop in performance resulting from a prompt attack \cite{zhu2024}. 
\end{itemize}

A different aspect of robustness is how well models perform when some parts of it are deliberately removed. Ablation studies work by removing parts of a model, such as layers in a transformer or input features, and seeing whether the performance changes \cite{meyes2019ablation}. These studies show whether the model exhibits a graceful degradation of the performance when parts are removed. In the field of LLM4SE, ablation studies are commonly used to find the performance benefit of each component in models that consist of multiple interacting components, thereby evaluating redundancy \cite{10586831,10402095}. 


\textbf{Sustainability}\\
The use of LLMs can come with a significant cost of time, space and energy compared to traditional ML models. Especially the training phase of LLMs can have a large energy footprint; but the increased integration of LLMs into various tools such as Google search could also have a major impact on energy consumption over time \cite{wu2022sustainableai,deVries2023}. Researchers in LLM4SE can reduce energy expenditure primarily by reducing LLM inferences, which can be done by restricting the number of queries, using smaller data-sets for fine-tuning or using smaller models \cite{baltes2025guidelinesempiricalstudiessoftware}. By using more efficient algorithms, reducing LLM inferences does not necessarily reduce the accuracy of the results, although there can be a trade-off between accuracy and energy expenditure \cite{deVries2023}. In the results analysis, researchers should report what steps (if any) were taken to reduce energy consumption and justify why LLMs were used instead of less costly alternatives.\\

\textbf{S9. Analyse overfitting \& degradation.}\\
One of the fundamental problems in machine learning is the possible disconnect between the in-sample performance of a model on its training or validation sets and the out-of-sample performance on the unseen test set \cite{Ying2019overfitting}. A high training accuracy does not guarantee a high test accuracy; in fact, a model with a lower training accuracy can outperform a model with a higher training accuracy when applied to the test set. This problem is known as overfitting and can occur when a model learns from noise in the data, has limited training data or has more parameters than needed. For example, if a limited set of examples is used for fine-tuning an LLM, the performance on the fine-tuned data could be great, but not generalize well to the actual real-life task that it aims to solve. 

Overfitting can be quantified by measuring the difference between the performance on the test set and the train set using the metrics in S7. Let $M$ be any chosen metric, then $overfitting = M_{test} - M_{train}$. If we use multiple data sets for testing, we can instead calculate the average overfitting, where $Test$ is the test set:

\begin{equation}
	\text{\textit{average overfitting}} = \frac{1}{|Test|} \sum_{t\in Test} (M_{test} - M_{train})
\label{overfitting}
\end{equation}

Apart from the metrics discussed in S7, such as accuracy, other metrics can also be used to quantify overfitting. This includes the metrics of zero-one loss for binary classifiers and mean squared error (MSE) for when class outputs contain scores or probabilities \cite{Dellanna2022}. If a pre-trained LLM is used with prompting, it might be difficult or unnecessary to analyse the overfitting. However, the validation set that was used for developing and selecting prompts or hyper-parameters can still lead to degradation, where the performance on the test set is lower than the performance on the validation set.

Degradation compares the performance of the test set with the performance of the validation set, using the metrics of S7. Its calculation is similar to overfitting: given performance metric $M$, we calculate $degradation = M_{test} - M_{validation}$. Equivalently, if multiple test sets are used or multiple validation sets in the case of k-fold cross-validation, we recommend calculating the average degradation, as in (\ref{overfitting}). In the event that both k-fold cross-validation and multiple test sets are used, then the distributions of the performance metrics for the validation and test sets can be statistically compared to see if there is a significance difference. For example, if the data is normally distributed, the independent samples T-Test can be used, whereas the Mann-Whitney's U test can be used as a non-parametric alternative. The original \textit{ECSER} pipeline contains examples of how to apply these tests \cite{Dellanna2022}.\\

\textbf{S10. Visualize ROC.}\\
Metrics such as the accuracy and F-score depend heavily on the arbitrary choice of the decision threshold (i.e., the confidence value above which a data point is classified as positive). The receiver operating characteristic curve (ROC) gives a visual description of this effect by showing the trade-off between the true positive rate (recall) and the true negative rate (specificity) across threshold values \cite{metz1978roc}. Figure \ref{fig:RocToyPlot} shows an example of an ROC plot with true positive rate on the y axis and false positive rate on the x axis. The diagonal represents random chance. Visually, the closer a classifier is towards the top-left corner, the better the classification performance. 

For discrete classifiers, the outputs will not be confidence values, but only class labels. In this case, the resulting true positive and false positive rate can be plotted as a single point in the ROC space \cite{fawcett2006roc}. Multiple discrete classifiers can thus be visually compared by plotting each result as a point in the ROC plot, or as a cluster of points when multiple data sets are used for each classifier. 

\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=0.5\linewidth]{figures/Roc-ToyPlot.png}
	\caption{Example of an ROC plot comparing a classifier model (blue) to random chance (orange).}
	\label{fig:RocToyPlot}
\end{figure}

The area under the ROC curve (AUC) provides a summary of the ROC curve by taking the average across all threshold values. Since the AUC of a random classifier is 0.5, the realistic range of AUC values is between 0.5 and 1 \cite{fawcett2006roc}. It has been suggested that the AUC provides a better basis for comparing classifiers than the accuracy, since it is independent of the decision threshold \cite{huang2005auc,ling2003auc}.\\

\textbf{S11. Apply statistical significance tests.}\\
It is not enough to see a difference in performance metrics between two classification models to make the claim that one model is better than the other. There are many reasons why the performance of two models can differ, including randomness in the training data, evaluation data or training process \cite{Kapoor2024}. Statistical significance testing assesses whether the obtained results are incompatible with a given null-hypothesis (e.g., "the two models have equal performance"). The smaller the p-value of a test, the less the results agree with the null-hypothesis, given that the assumptions of the chosen test are met \cite{wasserstein2016stats}. It is general practice that a p-value below the significance threshold of $0.05$ represent a statistically significant result. However, the significance threshold that is used should be justified based on the research topic, the number of hypotheses tested, the study design and the cost of errors \cite{benjamin2018significance}. Further, statistical significance is not a license for claiming a scientific finding, since the results depend on the quality of the research itself. Instead, conclusions should be based on non-automated informed judgement and replicability \cite{amrhein2017significance}.

Many tests exist to evaluate the statistical significance between two or more classifiers, each with their own underlying assumptions. For example, parametric tests like the paired samples T-Test assume that the data is normally distributed, whereas non-parametric tests like the Wilcoxons Signed-Rank test do not have this assumption. In general, LLM4SE researchers should consider non-parametric tests, since classifier results are generally not normally distributed \cite{demsar2006statistical}. Readers should refer to the original ECSER pipeline for a detailed discussion of how to perform statistical testing on single or multiple datasets, noting that the Wilcoxon Signed-Rank and the Friedman plus Nemenyi's post-hoc tests are recommended tests by the authors \cite{Dellanna2022}. Another useful resource for choosing the significance test is \textcite{Rainio2024}'s flowchart, which considers the task, evaluation metric, the number of test sets and the number of models that are compared. In all cases, researchers should justify the chosen statistical test based on the assumptions it makes and its limitations. 

\section{RQ3: How applicable is the proposed pipeline in assisting LLM4SE classification research?}
\label{RQ3}
In this section, we first discuss the approach to evaluating the applicability of the \newecser pipeline by way of a replication study. Subsequently, we present the results of the replication study and use these results to analyse the applicability and the added benefit of using the pipeline.

\subsection{Research Approach}
% Useful guidelines: \textcite{carver2010}. 
In order for the \newecser pipeline to be considered a suitable treatment for the problems investigated in RQ1 (Section \ref{RQ1}), the results obtained by applying the pipeline should avoid the shortcomings of existing LLM4SE research. For this reason, the choice was made to conduct a replication of an existing LLM4SE classification study by following the steps of the \newecser pipeline, in order to evaluate whether applying the pipeline is beneficial for enhancing the original findings. In conducting the replication study, we followed \textcite{carver2010}'s reporting guidelines.

Studies were considered for replication based on the following criteria:
\begin{enumerate}
	\item The authors provide a ready-to-use replication package.
	\item The study is recent (past five years) and represents current practices in LLM4SE well.
	\item The study is peer-reviewed and published in a reputable journal.
	\item The study does not already follow all the proposed guidelines, so that the added benefit of the pipeline can be evaluated.
\end{enumerate}

These criteria were designed to select a study that is high-quality and easy to replicate, but does not follow all of the best practices in the field, since the purpose of the \newecser pipeline is to make it easier for researchers to follow these best practices.

We chose to replicate \textcite{10.1145/3663529.3663794}'s study on predicting Python test results without execution, since the paper included a repository with the data and LLM outputs, was published recently (2024), was published in a reputable journal (FSE) and did not follow all the proposed guidelines. Additionally, the paper uses prompts to guide predictions, making it well suited to test the prompt engineering recommendations in the pipeline, which represents one of the major contributions of the \newecser pipeline compared to the original ECSER pipeline. In the next section, we describe the paper in more detail and present and discuss the results of each step of the pipeline.

%By conducting these replication studies, we can evaluate the comprehensiveness of the pipeline and identify potential shortcomings. After the replication study, the findings from RQ3 were taken into account to make changes to the guidelines from RQ2 where needed, creating a feedback-loop between RQ2 and RQ3. The second replication was conducted using the updated pipeline.

\subsection{Case Study: Predicting Test Results without Execution}
For our case study we consider the topic of predicting test results without execution, a novel area of research proposed by \textcite{10.1145/3663529.3663794}, which aims to evaluate whether LLMs can understand code execution sufficiently to predict the results of running tests without the challenges associated with executing the code. The original study presents this problem as a binary classification task where the model predicts that executing the test either causes an exception ('fail') or runs without exceptions ('pass').

%TODO Table of test cases
The original study investigated this problem by evaluating the performance of GPT-4 in predicting Python test results in Python version 3.10. The authors constructed a data set of Python test cases by selecting test cases from five different libraries of the Python Standard Library: ast, calender, csv, gzip and string. From each of these five libraries, one test was selected that was considered to be relatively simple and one that was considered more complex, for a total of 10 unique test cases. Then, each test case was manually modified to create 10 passing tests and 10 failing tests, resulting in a data set of 200 test cases (100 passing and 100 failing), with 100 simple and 100 complex cases and 40 cases for each test suite (ast, calender, csv, gzip and string).

%This data set was used to answer the following research question: what is the performance of GPT-4 to predict test results? This question was further divided into RQ1: all test cases, RQ2: test case complexity and RQ3: test suite. 

For our replication, we decided against using GPT-4 for predicting the results, since the usage of a closed-source model such as GPT-4 comes with several downsides, including cost, restricted access to model parameters and no guarantee of long-time availability. These downsides could hinder the replicability of the results \cite{baltes2025guidelinesempiricalstudiessoftware}. Instead, we use the open-source LLama 3.1 with eight billion parameters and instruction tuning, since Llama 3 has shown competitive performance with GPT-4 on text classification \cite{kostina2025llama}. However, we do not use the largest available model of 70 billion parameters, because of the available computational resources. Since we intentionally alter the procedure of the original study by applying the \newecser pipeline, our replication can be considered a non-exact or conceptual replication \cite{Shull2008}.

By applying the \newecser pipeline, we make several contributions to the literature: 
\begin{enumerate}
\item We increase the replicability of the research by using an open-source LLM as well as providing all our code, prompts and the processed data set on our GitHub page\footnote{\url{https://github.com/rubenvdz/Master-Thesis-Enhancing-the-ECSER-pipeline}}.
\item We perform a more systematic approach to prompt engineering, including a comparison of different prompts patterns.
\item We provide more insight into the results by providing additional performance metrics and analysing the calibration, robustness and sustainability.
\item We conduct statistical tests that show that we cannot corroborate some of the original findings of the paper.
\end{enumerate}

\subsubsection{Training, Validation, Testing}
\textbf{S1. Select an evaluation method and split the data.}\\
In order to get an unbiased estimate of the generalisability of the results, we have to avoid testing the model on the same data that was used for prompt engineering and prompt comparison. Since we do not perform hyper-parameter tuning or fine-tuning, we choose to use the holdout method to split the data evenly into a validation and test set, consisting of 100 samples each. These samples are chosen to provide an even split in terms of test suite, simple and complex test cases and label (pass or fail). This means that the validation and test set both consist of 50 passing and 50 failing tests, 50 simple and complex cases and 20 test cases per suite. The validation set is used for S3-S4, while the test set is exclusively used for S5-S11. The original study did not split the data into validation and test sets, which leaves open the possibility of the test data having influenced the design of their prompt.\\

\textbf{S2. Fine-tune models.}\\
Since the aim is to evaluate the performance of a pre-trained LLM on test result prediction, we do not perform S2 of the pipeline.\\

\textbf{S3. Design prompts.}\\
The prompt of the original study, although inspired by related research \cite{tufano2023predictingcodecoverageexecution}, was not accompanied by an explanation of how the prompt was designed, nor whether alternative approaches or prompts were considered. Instead, using the prompt of the original study as a blueprint, we follow the five steps of prompt design as follows:

1. \textit{Define the goal of the prompt}: the goal of the prompt is to make the model understand that it is asked to provide a binary judgement of whether a given test will pass or fail if it were run in Python version 3.10. 

2. \textit{Choose the right prompt engineering approach}: for our approach, we selected the two most widely used prompt engineering approaches, zero-shot and few-shot, which we compare in S4b. Since the original study used a zero-shot approach, we based our zero-shot prompt on theirs, with the added output context mentioned in step 4 (see Figure \ref{fig:prompts_compared}). For the few-shot prompt, we selected four test cases from different suites, of which two were failing and two passing tests, and providing these tests and the desired outputs as part of the prompt. The desired outputs took the form of an explanation and a label as defined in step 4. The explanations were taken from the outputs of GPT-4 provided in the original study, making sure that these explanations were taken from correctly classified test cases, which were then shortened to prevent the context length from exceeding the allowed limit of 2048 tokens. The few-shot examples were removed from the validation set before testing.

3. \textit{Design the prompt}: the prompt of the original study was not written using any established patterns with empirical backing. To enhance the text of the prompt, we selected three prompt patterns from \textcite{white2023}'s catalogue: the Persona, Cognitive Verifier and Question Refinement patterns. These patterns were chosen since they were the three highest performing prompt patterns in \textcite{ronanki2023}'s comparison of patterns for requirements classification. Each prompt pattern provides contextual statements that should be included in the prompt. For example, the Persona pattern has the contextual statements "Act as persona X" and "Provide outputs that persona X would create". The Question Refinement pattern asks the LLM to generate additional questions for it to answer before providing the label. The Question Refinement pattern tells the LLM to change the original question if needed and answer that question instead. For this last pattern we created two versions: one that asks LLama 3.1 to generate a better question for each instance and one that uses a pre-generated refined question that was generated by the more powerful GPT 5. All of these prompts, including the patternless zero-shot and few-shot prompts, can be found on our GitHub page and are compared in S4b.

4. \textit{Provide context}: we provided context to our prompts by specifying the desired format of the LLM response: an explanation followed by the label "PASS" or "FAIL" on a separate line, allowing us to easily extract the predicted label from the generated output. Figure \ref{fig:prompts_compared} shows an example of how we changed the original (zero-shot) prompt to add context for our zero-shot prompt. In order to ensure that the LLM followed this format and to avoid responses that couldn't be automatically parsed, we made use of the GBNF grammar; a formal grammar that constrains the output, designed for use with the llama.cpp library. In this grammar, we also restricted the length of the explanation to 2000 characters. 

\begin{figure}[H]
	\centering
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\vskip 0pt
		\infobox{Original Prompt}{Consider the following test of the Python Standard
			Library, version 3.10:\\
			\textlangle test\_case\textrangle\\
			Your job is to figure out whether this test will pass or
			fail. If it fails, provide the rationale.
			\vspace{1.79cm}}
		\caption[b]{Prompt from the original study \cite{10.1145/3663529.3663794}.}
		\label{fig:original_prompt}
	\end{subfigure}
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\vskip 0pt
		\infobox{Our Prompt (Zero-shot)}{Consider the following test of the Python Standard
			Library, version 3.10:\\
			\textlangle test\_case\textrangle\\
			Your job is to figure out whether this test will PASS or
			FAIL. Provide the rationale before giving an answer.\\
			
			Answer in this exact format:\\
			Explanation: \textlangle free text\textrangle\\
			Label: \textlangle PASS $\vert$ FAIL\textrangle}
		\caption[b]{Our zero-shot prompt.}
		\label{fig:zero-shot}
	\end{subfigure}
	\caption{The original prompt next to our zero-shot prompt with added context.}
	\label{fig:prompts_compared}
\end{figure}

5. \textit{Test and refine}: we tested all of our prompts on smaller partitions of the validation set to ensure that the responses resembled our expectations with regards to the content and structure, before using the full validation set for comparison. These smaller tests revealed that for some prompts, without a specified grammar, up to 25\% of responses would not follow the specified format, which lead to our implementing a strict grammar for the responses in step 4, solving the issue of low format adherence. Further, our original few-shot prompt contained examples with explanations that exceeded LLama's context limits, which lead us to shorten the explanations to fit within the limits. Other common issues with the few-shot prompt were that the responses would sometimes repeat the few-shot examples instead of classifying the actual test case, which was solved by specifying explicitly that the LLM should not repeat the examples and only answer the actual test case. Lastly, for transparency and reproducibility, we included all interaction logs (i.e., prompts and responses) in our results.

Compared to the prompt design approach of the original paper, which was not explained apart from a reference to related research, we have followed a systematic approach to prompt design based on the available recommendations and empirical evidence. Further, our prompts have additional contextual information to guide the output into specific formats, using prompt patterns and grammars. The most important change is the development of multiple prompts, which allows us to compare the relative performance of these prompts, avoiding the unpredictability of using a single prompt \cite{mizrahi2024stateartmultipromptllm}.\\

\textbf{S4a. Hyper-parameter tuning.}\\
Since we used a pre-trained model, we do not have control over training-related parameters. For parameters that guide the generation process, we selected specific values to ensure consistent results, since we are more concerned with replicability than performance. The temperature could be tuned to find the value with the best performance, but we chose to set it to 0 in order to remove any randomness in favour of replicability. We set the context size of the model to 2048 tokens to ensure our few-shot prompt did not exceed the context size. All other parameters were kept at their default values\footnote{See \url{https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.create_chat_completion}}. The original study did not report the temperature or the value of any other parameter, which means we do not know how the temperature affected the randomness of their result or which parameter values to use if we wish to conduct an exact replication.\\

\textbf{S4b. Prompt comparison.}\\
Instead of designing a single prompt, we designed a total of six prompts. The first two, which we refer to as the zero-shot and few-shot prompts were made to be very similar to the prompt of the original paper, only with added context for both and few-shot examples for the latter prompt. The other four are modified versions of our zero-shot prompt that make use of established prompt patterns. This includes the Persona, Cognitive Verifier and two versions of the Question Refinement pattern. Each of these prompts were evaluated on the validation set in order to select a single prompt for the final evaluation (S5). 

Table \ref{table:prompt_metrics} shows the performance metrics for each prompt (calculated from the confusion matrices in Table \ref{table:confusion_matrix}). Note that for these results, we consider "fail" to be the positive class, as in the original paper, since we want test cases to fail in case there is a problem. Hence, a true positive (TP) represents a test case that is correctly classified as failing, whereas a true negative (TN) is a test that is correctly classified as passing. These results show that our Cognitive Verifier and Persona prompts perform the best on the validation set, with the Cognitive Verifier prompt having the highest precision (0.545), accuracy (0.550) and MCC (0.101); whereas the Persona prompt has the highest recall (0.740), $F_1$-score (0.607), $F_2$-score (0.680) and AUC (0.577). The few-shot prompt has the highest specificity (0.729), but this comes at the cost of a very low recall (0.255). 
\begin{table}[H]
	\centering
	\caption{Performance metrics for each prompt on the validation set. Highest values are in bold.}
	\label{table:prompt_metrics}
	\begin{tabularx}{\textwidth}{|>{\small}X|>{\small}c|>{\small}c|>{\small}c|>{\small}c|>{\small}c|>{\small}c|>{\small}c|>{\small}c|}
		\hline
		\textbf{Prompt} & \textbf{Precision} & \textbf{Recall} & \textbf{Specificity} & \textbf{Accuracy} & \textbf{F1} & \textbf{F2} & \textbf{MCC} & \textbf{AUC} \\
		\hline
		Zeroshot & 0.531 & 0.680 & 0.400 & 0.540 & 0.596 & 0.644 & 0.083 & 0.549 \\
		Fewshot & 0.480 & 0.255 & \textbf{0.729} & 0.495 & 0.333 & 0.282 & -0.018 & 0.472 \\
		Cognitive Verifier & \textbf{0.545} & 0.600 & 0.500 & \textbf{0.550} & 0.571 & 0.588 & \textbf{0.101} & 0.541 \\
		Persona & 0.514 & \textbf{0.740} & 0.300 & 0.520 & \textbf{0.607} & \textbf{0.680} & 0.045 & \textbf{0.577} \\
		Q. Refinement (Llama) & 0.508 & 0.620 & 0.400 & 0.510 & 0.559 & 0.594 & 0.021 & 0.458 \\
		Q. Refinement (GPT) & 0.466 & 0.540 & 0.380 & 0.460 & 0.500 & 0.523 & -0.081 & 0.511 \\
		\hline
	\end{tabularx}
\end{table}

Based on these results, we selected the Persona prompt for the final evaluation for two reasons. Firstly, it has the highest $F_1$-score, which signifies a good balance between precision and recall. Secondly, it has the highest recall and $F_2$-score, which we weigh more heavily than the precision, since the primary purpose of running test cases is to find code that fails, so that issues can be resolved, instead of finding code that runs as expected. Figure \ref{fig:persona_prompt} shows the full Persona prompt as it is used in the rest of the pipeline. It should be noted that these results are specific to our implementations of the prompt patterns, the general wording of the prompt and the data set, which means we cannot make claims about which prompt pattern is better in the general case. For this reason, we recommend all researchers to conduct their own comparisons to find the best prompt for their case. 

Since the original study used a single prompt, without comparing it to alternative prompt designs, and also did not use established prompt patterns, we have no way of knowing whether their results would have been different if they used other prompting approaches or if the Persona pattern would have also had the highest performance in their case. 

\begin{figure}[H]
	\centering
	\infobox{Persona Prompt}{\textcolor{red}{Act as a Python expert} and consider the following test of the Python Standard
		Library, version 3.10:\\
		\textlangle test\_case\textrangle\\
		Your job is to figure out whether this test will PASS or
		FAIL. Provide the rationale before giving an answer.\\
		\textcolor{red}{Provide outputs that a Python expert would create.}
		
		Answer in this exact format:\\
		Explanation: \textlangle free text\textrangle\\
		Label: \textlangle PASS $\vert$ FAIL\textrangle}
	\caption{The Persona prompt, which was selected from the prompt comparison and will be used in the rest of the pipeline. Contextual statements unique to the Persona prompt pattern are coloured red for emphasis.}
	\label{fig:persona_prompt}
\end{figure}

\textbf{S5. Test the models.}\\
After developing multiple prompts and comparing the results on the validation set, we determined that the Persona prompt has the best performance. Next, we use this prompt with the test set to obtain the final results for S6-S11. 

\subsubsection{Results Analysis}

\textbf{S6. Report the Confusion Matrix.}\\
Table \ref{table:confusion_matrix} shows the confusion matrices obtained on the validation and test sets. These confusion matrices were used to calculate the metrics for each prompt on the validation set in S4b, and are used to calculate the metrics on the test set for the Persona prompt in S7. The benefit of providing the confusion matrix along with the calculated metrics is that if the reader is interested in any metrics that we did not consider, they have the ability to calculate these metrics themselves using the confusion matrix. 
\begin{table}[H]
 	\caption{Confusion matrices for validation and test sets. Persona prompt is bold for easier comparison between the validation and test set.}
 	\label{table:confusion_matrix}
	\begin{tabular}{|l|l|c|c|c|c|}
		\hline
		\textbf{Data set} & \textbf{Prompt} & \textbf{TP} & \textbf{FP} & \textbf{FN} & \textbf{TN} \\
		\hline
		\multirow{6}{*}{Validation}
		& Zeroshot & 34 & 30 & 16 & 20 \\
		& Fewshot & 12 & 13 & 35 & 35 \\
		& Cognitive Verifier & 30 & 25 & 20 & 25 \\
		& \textbf{Persona} & \textbf{37} & \textbf{35} & \textbf{13} & \textbf{15} \\
		& Question Refinement (Llama) & 31 & 30 & 19 & 20 \\
		& Question Refinement (GPT) & 27 & 31 & 23 & 19 \\
		\hline
		Test &\textbf{Persona} & \textbf{40} & \textbf{35} & \textbf{10} & \textbf{15} \\
		\hline
	\end{tabular}
\end{table}

\textbf{S7. Report Metrics.}\\
Using the confusion matrix on the test set (S6), we calculated the performance metrics presented in Table \ref{table:test_metrics}. Overall, the results show that the model has a much higher recall (0.800) than precision (0.533), suggesting that while it finds most failing test cases, in the process it also misclassifies many passing test cases as failing test cases. This result is unsurprising, since we specifically favoured recall over precision when selecting the prompt in S4b, as the primary goal of running tests is to find incorrect code. As a result of the high difference in recall and precision, the model has a high $F_2$-score (0.727) and decent $F_1$-score (0.640), but a relatively low accuracy (0.550) and specificity (0.300). The MCC (0.115) shows that there is some level of agreement between the predicted and actual labels, but far from a perfect prediction (which would be 1).

Apart from the results across all test cases, we have included the results for simple and complex test cases and the results for each test suite (ast, calendar, csv, gzip and string), as in the original paper. These results show that the simple test cases have a higher or equal performance than the complex test cases on every metric. Further, there are considerably differences in the performance metrics depending on the test suite. However, these observations are not enough to claim that the model is more capable of classifying simple test cases than complex test cases or actually performs better on some test suites \textit{in general}. We have to consider the possibility that these differences are coincidental, especially given the limited number of observations (50 each for simple and complex test cases, and 20 per test suite). Hence, we analyse the statistical significance of these results in S11 before drawing conclusions, a step that was not performed in the original study. 

\begin{table}[H]
	\centering
	\caption{Performance metrics across different parts of the test set.}
	\label{table:test_metrics}
	\begin{tabularx}{\textwidth}{|X|c|c|c|c|c|c|c|c|}
		\hline
		\textbf{Tests} & \textbf{Precision} & \textbf{Recall} & \textbf{Specificity} & \textbf{Accuracy} & \textbf{F1} & \textbf{F2} & \textbf{MCC} & \textbf{AUC} \\
		\hline
		All & 0.533 & 0.800 & 0.300 & 0.550 & 0.640 & 0.727 & 0.115 & 0.573 \\
		\hline
		Simple & 0.556 & 0.800 & 0.360 & 0.580 & 0.656 & 0.735 & 0.178 & 0.630 \\
		Complex & 0.513 & 0.800 & 0.240 & 0.520 & 0.625 & 0.719 & 0.048 & 0.507 \\
		\hline
		ast & 0.643 & 0.900 & 0.500 & 0.700 & 0.750 & 0.833 & 0.436 & 0.640 \\
		calendar & 0.563 & 0.900 & 0.300 & 0.600 & 0.692 & 0.804 & 0.250 & 0.580 \\
		csv & 0.471 & 0.800 & 0.100 & 0.450 & 0.593 & 0.702 & -0.140 & 0.360 \\
		gzip & 0.500 & 0.800 & 0.200 & 0.500 & 0.615 & 0.714 & 0.000 & 0.650 \\
		string & 0.500 & 0.600 & 0.400 & 0.500 & 0.545 & 0.577 & 0.000 & 0.650 \\
		\hline
	\end{tabularx}
\end{table}

Compared to the original paper, we have reported many common metrics that were not provided by the original authors, including the specificity, $F_1$-score, $F_2$-score, MCC and AUC. However, since they included the confusion matrix for the full test set, we were able to calculate that their model (GPT-4) had an $F_1$-score of 0.789, an $F_2$-score of 0.740 and an MCC of 0.633. These metrics are higher than ours, which is unsurprising given that GPT-4 is a much larger model than our Llama model of 8 billion parameters. However, their results are much harder to replicate, since they used a closed-source model, did not explain how they designed their prompt and did not provide all the code they used to run the experiments and analyse the results. It is possible that with a more systematic approach to prompt design and prompt comparison, their model would have performed even better, given that our zero-shot prompt, which is very similar to their prompt, was not our highest performing prompt.\\

%TODO S8. Evaluate calibration, fairness, robustness & sustainability.
\textbf{S8. Evaluate calibration, fairness, robustness \& sustainability.}\\
The evaluation of the correctness of the results does not give a complete picture of the trustworthiness of said results. In this step, we consider the calibration, fairness, robustness \& sustainability of the results. These aspects of the evaluation provide additional insights into the real-world usefulness of the model. 

%TODO S8. Calibration
\textbf{Calibration}\\
In order to calculate the calibration of the model, the confidence of each prediction has to be estimated. We used the key token probability to estimate the confidence, since it is considered one of the best methods for confidence elicitation \cite{xiong2024confidence}. This method works by focusing on the probability of result-specific tokens, which in our case means taking the conditional probability of the final label ("PASS" or "FAIL"), which was generated after the explanation. By not calculating the probability of the entire sequence, we remove the influence of the length of the explanation and any irrelevant tokens. We were able to easily obtain these probabilities, since we have used an open-source model. If we had used a closed-source model as in the original study, we would have had to use less reliable methods for estimating the confidence. 

We calculated the expected calibration error (ECE) with 10 bins to measure the difference between the estimated confidences and the actual accuracy of the predictions, resulting in an ECE of 0.249. This means that we can expect a 25\% difference between the average confidence and the actual accuracy, showing that the model is poorly calibrated. This result is unsurprising, since LLMs have been shown to be overconfident in their predictions, and it has been demonstrated that instruction tuning worsens the calibration \cite{zhu2023calibration}. However, the fact that the model is poorly calibrated means that we cannot rely on the model's confidence to determine whether we should trust its predictions. I.e., if the model is very confident that a given test case will pass, there could still be a high probability of it failing.

Since the original study did not evaluate the calibration of the model, we don't know how much we can trust the explanations provided by their model, even when the explanations express a high level of certainty of a particular test case passing or failing.

\textbf{Fairness}\\
In the pipeline, we consider four types of unfairness: injustice, stereotype bias, preference bias and disparate performance \cite{liu2024}. Since we do not use human participants, sensitive data, or subjective data; we do not perform this step of the pipeline.

%TODO S8. Robustness
\textbf{Robustness}\\

%TODO S8. Sustainability
\textbf{Sustainability}\\

%TODO S9. Analyse overfitting & degradation.
\textbf{S9. Analyse overfitting \& degradation.}\\
%TODO S9. Overfitting
%TODO S9. Degradation

%TODO S10. Visualize ROC.
\textbf{S10. Visualize ROC.}\\

%TODO S11. Apply statistical significance tests.
\textbf{S11. Apply statistical significance tests.}\\
%TODO S11. RQ1
%TODO S11. RQ2
%TODO S11. RQ3

\subsubsection{Summary of Findings}
%TODO: Summarize results of all steps

\newpage 
\printbibliography[heading=bibintoc]

\newpage
\begin{appendices}
\section{Supplemental Mapping Study Results} \label{appendix:ref}
For brevity, sources were excluded for some figures and tables and are included here for further reference.\\
Table \ref{refs:application_types} shows the distribution of the application types of LLMs of 83 total papers (see Figure \ref{fig:LLMTypes}).
\begin{table}[ht]
	\caption{References for the distribution of LLM application types.}
	\begin{tabularx}{\textwidth}{|l|c|X|}
	    \hline
	    \textbf{Category} & \textbf{Number of studies} & \textbf{References} \\ \hline
		Generation & 58 & \cite{10.1145/3597503.3639150,10.1145/3597503.3649399,10.1145/3597503.3608132,10.1145/3597503.3623326,10.1145/3597503.3623298,10.1145/3597503.3623306,10.1145/3597503.3623316,10.1145/3597503.3608134,10.1145/3597503.3608137,10.1145/3597503.3623343,10.1145/3597503.3639085,10.1145/3597503.3639120,10.1145/3597503.3639133,10.1145/3597503.3639138,10.1145/3597503.3639219,10.1145/3597503.3639226,10.1145/3597503.3639184,10.1145/3597503.3639081,10.1145/3597503.3639157,10.1145/3597503.3639180,10.1145/3597503.3639121,10.1145/3597503.3639118,10.1145/3597503.3639210,10.1145/3597503.3639116,10.1145/3597503.3639223,10.1145/3597503.3639155,10.1145/3597503.3639183,10.1145/3597503.3639135,10.1145/3663529.3663829,10.1145/3663529.3663836,10.1145/3663529.3663839,10.1145/3663529.3663841,10.1145/3663529.3663842,10.1145/3663529.3663846,10.1145/3663529.3663855,10.1145/3663529.3663861,10.1145/3663529.3663801,10.1145/3663529.3663868,10.1145/3663529.3663869,10.1145/3663529.3663873,10.1145/3663529.3664463,10329992,10378848,10433002,10485640,10507163,10482873,10521881,10584357,10606356,10609742,10634302,10636040,10664637,10538301,10707668,10713474,10734067} \\ \hline
		Classification & 15 & \cite{10.1145/3597503.3623345,10.1145/3597503.3623304,10.1145/3597503.3639126,10.1145/3597503.3639217,10.1145/3597503.3639117,10.1145/3597503.3639194,10.1145/3597503.3639202,10.1145/3663529.3663785,10.1145/3663529.3663794,10323231,10402095,10586831,10648982,10659742,10746847} \\ \hline
		Recommendation & 6 & \cite{10.1145/3597503.3623342,10.1145/3597503.3639188,10.1145/3597503.3639187,10.1145/3663529.3663826,10.1145/3663529.3663803,10697930} \\ \hline
		Generation \& Classification & 3 & 
		\cite{10.1145/3597503.3639216,10599336,10704582} \\ \hline
		Classification \& Recommendation & 1 & \cite{10.1145/3597503.3623322} \\ \hline
	\end{tabularx} \label{refs:application_types}
\end{table}


The following tables apply to the 18 papers included in the mapping study. Table \ref{refs:venues} shows the number of studies belonging to each venue. Table \ref{refs:prompts} shows the number of papers using prompts for classification.
See also Figure \ref{fig:Venues} and \ref{fig:Prompts}.
\begin{table}[ht]
	\caption{References for distribution of venue in the mapping study.}
	\begin{tabularx}{\textwidth}{|l|c|X|}
		\hline
		\textbf{Venue} & \textbf{Number of studies} & \textbf{References} \\ \hline
		TSE & 8 & \cite{10323231,10402095,10586831,10648982,10659742,10746847,10599336,10704582} \\ \hline
		FSE & 2 & \cite{10.1145/3663529.3663785,10.1145/3663529.3663794} \\ \hline
		ICSE & 8 & \cite{10.1145/3597503.3623345,10.1145/3597503.3623304,10.1145/3597503.3639217,10.1145/3597503.3639117,10.1145/3597503.3639194,10.1145/3597503.3639202,10.1145/3597503.3639216,10.1145/3597503.3623322} \\ \hline
	\end{tabularx}
	\label{refs:venues}
\end{table}

\begin{table}[ht]
	\caption{References for the distribution of prompt usage in the mapping study.}
	\begin{tabularx}{\textwidth}{|l|c|X|}
		\hline
		\textbf{Prompt-based} & \textbf{Number of studies} & \textbf{References} \\ \hline
		No & 10 & \cite{10.1145/3597503.3623345,10.1145/3597503.3623304,10.1145/3597503.3639217,10.1145/3597503.3639202,10323231,10402095,10586831,10.1145/3597503.3639216,10704582,10.1145/3597503.3623322} \\ \hline
		Yes & 8 & \cite{10.1145/3597503.3639117,10.1145/3597503.3639194,10.1145/3663529.3663785,10.1145/3663529.3663794,10648982,10659742,10746847,10599336} \\ \hline
	\end{tabularx}
	\label{refs:prompts}
\end{table}

\begin{table}[ht]
	\caption{References for the mapping study results.}
	\begin{tabularx}{\textwidth}{|l|c|X|}
		\hline
		\textbf{Category} & \textbf{Total} & \textbf{References} \\
		\hline
		Total mapping study papers & 18 & \cite{10.1145/3597503.3623345,10.1145/3597503.3623304,10.1145/3597503.3639217,10.1145/3597503.3639117,10.1145/3597503.3639194,10.1145/3597503.3639202,10.1145/3663529.3663785,10.1145/3663529.3663794,10323231,10402095,10586831,10648982,10659742,10746847,10.1145/3597503.3639216,10599336,10704582,10.1145/3597503.3623322}\\
		\hline
		\textbf{Evaluation metrics} & & \\
		\hspace{3mm} Precision & 17 (94.4\%) & \cite{10.1145/3597503.3623345,10.1145/3597503.3623304,10.1145/3597503.3639217,10.1145/3597503.3639117,10.1145/3597503.3639194,10.1145/3597503.3639202,10.1145/3663529.3663785,10.1145/3663529.3663794,10323231,10402095,10586831,10648982,10746847,10.1145/3597503.3639216,10599336,10704582,10.1145/3597503.3623322}  \\
		\hspace{3mm} Recall & 17 (94.4\%) & \cite{10.1145/3597503.3623345,10.1145/3597503.3623304,10.1145/3597503.3639217,10.1145/3597503.3639117,10.1145/3597503.3639194,10.1145/3597503.3639202,10.1145/3663529.3663785,10.1145/3663529.3663794,10323231,10402095,10586831,10648982,10746847,10.1145/3597503.3639216,10599336,10704582,10.1145/3597503.3623322}\\
		\hspace{3mm} Accuracy & 11 (61.1\%) & \cite{10.1145/3597503.3623304,10.1145/3597503.3639217,10.1145/3597503.3639194,10.1145/3597503.3639202,10.1145/3663529.3663794,10323231,10402095,10586831,10659742,10746847,10599336}\\
		\hspace{3mm} F-Score & 17 (94.4\%) &  \cite{10.1145/3597503.3623345,10.1145/3597503.3623304,10.1145/3597503.3639217,10.1145/3597503.3639117,10.1145/3597503.3639194,10.1145/3597503.3639202,10.1145/3663529.3663785,10323231,10402095,10586831,10648982,10659742,10746847,10.1145/3597503.3639216,10599336,10704582,10.1145/3597503.3623322}\\
		\hspace{3mm} AUC & 3 (16.7\%) & \cite{10.1145/3597503.3639202,10402095,10659742}\\
		\hspace{3mm} ROC plots & 0 (0.0\%) & \\
		\hspace{3mm} MCC & 0 (0.0\%) & \\
		Calibration & 0 (0.0\%) &  \\
		Fairness &  0 (0.0\%) &  \\
		\textbf{Robustness} &  & \\
		\hspace{3mm} Data distribution \& Adversarial Attacks & 1 (5.5\%) & \cite{10586831} \\
		\hspace{3mm} Noisy data & 1 (5.5\%) & \cite{10.1145/3597503.3639217} \\
		\textbf{Metrics justification} & & \\
		\hspace{3mm} No & 9 (50.0\%) & \cite{10.1145/3597503.3639217,10.1145/3597503.3639117,10.1145/3597503.3639194,10.1145/3597503.3639202,10.1145/3663529.3663785,10.1145/3663529.3663794,10323231,10586831,10599336}\\
		\hspace{3mm} Implicit & 0 (0.0\%) &  \\
		\hspace{3mm} Previous work & 5 (27.8\%) & \cite{10402095,10659742,10746847,10.1145/3597503.3639216,10.1145/3597503.3623322}\\
		\hspace{3mm} Yes & 4 (22.2\%) & \cite{10.1145/3597503.3623345,10.1145/3597503.3623304,10648982,10704582} \\
		Confusion matrix & 4 (22.2\%) & \cite{10.1145/3597503.3639117,10.1145/3663529.3663794,10648982,10659742}\\
		Evaluation over multiple data sets & 5 (27.8\%) & \cite{10.1145/3597503.3639117,10402095,10648982,10.1145/3597503.3639216,10.1145/3597503.3623322}\\
		\textbf{Type of baseline} &&\\
		\hspace{3mm} None & 2 (11.1\%) & \cite{10.1145/3597503.3639117,10.1145/3663529.3663794} \\
		\hspace{3mm} Own & 3 (16.7\%) & \cite{10.1145/3597503.3639194,10.1145/3663529.3663785,10704582} \\
		\hspace{3mm} External & 4 (22.2\%) & \cite{10.1145/3597503.3623304,10402095,10599336,10.1145/3597503.3623322}  \\
		\hspace{3mm} External and Own & 9 (50.0\%) & \cite{10.1145/3597503.3623345,10.1145/3597503.3639217,10.1145/3597503.3639202,10323231,10586831,10648982,10659742,10746847,10.1145/3597503.3639216}\\
		Analysis of statistical significance & 5 (27.8\%) & \cite{10.1145/3597503.3623345,10648982,10659742,10746847,10704582}\\
		\textbf{Statistical significance justification}&& \\
		\hspace{3mm} No & 4 (80.0\%) & \cite{10648982,10659742,10746847,10704582} \\
		\hspace{3mm} Implicit & 0 (0.0\%) &  \\
		\hspace{3mm} Previous work & 1 (20.0\%) & \cite{10.1145/3597503.3623345}\\
		\hspace{3mm} Yes & 0 (0.0\%) &   \\
		\hline
	\end{tabularx}
	\label{refs:mapping_metrics}
\end{table}

For the main results of the mapping study, Table \ref{refs:mapping_metrics} shows the full references. Further, Table \ref{refs:prompt_results} shows the references for the papers that use a prompt-based approach to classification. 

\begin{table}[ht]
	\caption{References for the mapping study prompting results.}
	\begin{tabularx}{\textwidth}{|l|c|X|}
		\hline
		\textbf{Category} & \textbf{Total} & \textbf{References} \\
		\hline
		Total prompt-based classification papers & 8 & \cite{10.1145/3597503.3639117,10.1145/3597503.3639194,10.1145/3663529.3663785,10.1145/3663529.3663794,10648982,10659742,10746847,10599336} \\
		\hline
		\textbf{Prompt approach} && \\
		\hspace{3mm} Chain-of-verification & 1 (12.5\%) & \cite{10.1145/3597503.3639194} \\
		\hspace{3mm} Few-shot & 2 (25.0\%) & \cite{10746847,10599336} \\
		\hspace{3mm} Mimic-in-the-background & 1 (12.5\%) & \cite{10.1145/3597503.3639117} \\
		\hspace{3mm} Not reported & 1 (12.5\%) & \cite{10.1145/3663529.3663785} \\
		\hspace{3mm} Zero-shot & 3 (37.5\%) & \cite{10.1145/3663529.3663794,10648982,10659742} \\
		\textbf{Final prompt} && \\
		\hspace{3mm} Not reported & 2 (25.0\%) & \cite{10.1145/3663529.3663785,10746847} \\
		\hspace{3mm} Own & 6 (75.0\%) & \cite{10.1145/3597503.3639117,10.1145/3597503.3639194,10.1145/3663529.3663794,10648982,10659742,10599336} \\
		\textbf{Prompt approach justification} && \\
		\hspace{3mm} No & 5 (62.5\%) & \cite{10.1145/3663529.3663785,10.1145/3663529.3663794,10648982,10659742,10746847} \\
		\hspace{3mm} Yes & 3 (37.5\%) & \cite{10.1145/3597503.3639117,10.1145/3597503.3639194,10599336} \\
		\textbf{Final prompt justification} && \\
		\hspace{3mm} No & 4 (50.0\%) & \cite{10.1145/3597503.3639194,10.1145/3663529.3663785,10648982,10746847} \\
		\hspace{3mm} Previous Work & 1 (12.5\%) & \cite{10.1145/3663529.3663794} \\
		\hspace{3mm} Yes & 3 (37.5\%) & \cite{10.1145/3597503.3639117,10659742,10599336} \\
		\textbf{Evaluation over multiple prompts} && \\
		\hspace{3mm} No & 8 (100.0\%) & \cite{10.1145/3597503.3639117,10.1145/3597503.3639194,10.1145/3663529.3663785,10.1145/3663529.3663794,10648982,10659742,10746847,10599336} \\
		\textbf{Temperature} && \\
		\hspace{3mm} No & 7 (87.5\%) & \cite{10.1145/3597503.3639194,10.1145/3663529.3663785,10.1145/3663529.3663794,10648982,10659742,10746847,10599336} \\
		\hspace{3mm} Yes & 1 (12.5\%) & \cite{10.1145/3597503.3639117} \\
		\hline
	\end{tabularx}
	\label{refs:prompt_results}
\end{table}

\end{appendices}

\end{document}
