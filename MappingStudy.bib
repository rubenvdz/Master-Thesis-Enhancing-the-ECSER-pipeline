@inproceedings{10.1145/3597503.3623345,
	author = {Steenhoek, Benjamin and Gao, Hongyang and Le, Wei},
	title = {Dataflow Analysis-Inspired Deep Learning for Efficient Vulnerability Detection},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3623345},
	doi = {10.1145/3597503.3623345},
	abstract = {Deep learning-based vulnerability detection has shown great performance and, in some studies, outperformed static analysis tools. However, the highest-performing approaches use token-based transformer models, which are not the most efficient to capture code semantics required for vulnerability detection. Classical program analysis techniques such as dataflow analysis can detect many types of bugs based on their root causes. In this paper, we propose to combine such causal-based vulnerability detection algorithms with deep learning, aiming to achieve more efficient and effective vulnerability detection. Specifically, we designed DeepDFA, a dataflow analysis-inspired graph learning framework and an embedding technique that enables graph learning to simulate dataflow computation. We show that DeepDFA is both performant and efficient. DeepDFA outperformed all non-transformer baselines. It was trained in 9 minutes, 75x faster than the highest-performing baseline model. When using only 50+ vulnerable and several hundreds of total examples as training data, the model retained the same performance as 100\% of the dataset. DeepDFA also generalized to real-world vulnerabilities in DbgBench; it detected 8.7 out of 17 vulnerabilities on average across folds and was able to distinguish between patched and buggy versions, while the highest-performing baseline models did not detect any vulnerabilities. By combining DeepDFA with a large language model, we surpassed the state-of-the-art vulnerability detection performance on the Big-Vul dataset with 96.46 F1 score, 97.82 precision, and 95.14 recall. Our replication package is located at https://doi.org/10.6084/m9.figshare.21225413.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {16},
	numpages = {13},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3623342,
	author = {Yang, Aidan Z. H. and Le Goues, Claire and Martins, Ruben and Hellendoorn, Vincent},
	title = {Large Language Models for Test-Free Fault Localization},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3623342},
	doi = {10.1145/3597503.3623342},
	abstract = {Fault Localization (FL) aims to automatically localize buggy lines of code, a key first step in many manual and automatic debugging tasks. Previous FL techniques assume the provision of input tests, and often require extensive program analysis, program instrumentation, or data preprocessing. Prior work on deep learning for APR struggles to learn from small datasets and produces limited results on real-world programs. Inspired by the ability of large language models (LLMs) of code to adapt to new tasks based on very few examples, we investigate the applicability of LLMs to line level fault localization. Specifically, we propose to overcome the left-to-right nature of LLMs by fine-tuning a small set of bidirectional adapter layers on top of the representations learned by LLMs to produce LLMAO, the first language model based fault localization approach that locates buggy lines of code without any test coverage information. We fine-tune LLMs with 350 million, 6 billion, and 16 billion parameters on small, manually curated corpora of buggy programs such as the Defects4J corpus. We observe that our technique achieves substantially more confidence in fault localization when built on the larger models, with bug localization performance scaling consistently with the LLM size. Our empirical evaluation shows that LLMAO improves the Top-1 results over the state-of-the-art machine learning fault localization (MLFL) baselines by 2.3\%--54.4\%, and Top-5 results by 14.4\%-35.6\%. LLMAO is also the first FL technique trained using a language model architecture that can detect security vulnerabilities down to the code line level.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {17},
	numpages = {12},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3623304,
	author = {Ma, Lipeng and Yang, Weidong and Xu, Bo and Jiang, Sihang and Fei, Ben and Liang, Jiaqing and Zhou, Mingjie and Xiao, Yanghua},
	title = {KnowLog: Knowledge Enhanced Pre-trained Language Model for Log Understanding},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3623304},
	doi = {10.1145/3597503.3623304},
	abstract = {Logs as semi-structured text are rich in semantic information, making their comprehensive understanding crucial for automated log analysis. With the recent success of pre-trained language models in natural language processing, many studies have leveraged these models to understand logs. Despite their successes, existing pre-trained language models still suffer from three weaknesses. Firstly, these models fail to understand domain-specific terminology, especially abbreviations. Secondly, these models struggle to adequately capture the complete log context information. Thirdly, these models have difficulty in obtaining universal representations of different styles of the same logs. To address these challenges, we introduce KnowLog, a knowledge-enhanced pre-trained language model for log understanding. Specifically, to solve the previous two challenges, we exploit abbreviations and natural language descriptions of logs from public documentation as local and global knowledge, respectively, and leverage this knowledge by designing novel pre-training tasks for enhancing the model. To solve the last challenge, we design a contrastive learning-based pre-training task to obtain universal representations. We evaluate KnowLog by fine-tuning it on six different log understanding tasks. Extensive experiments demonstrate that KnowLog significantly enhances log understanding and achieves state-of-the-art results compared to existing pre-trained language models without knowledge enhancement. Moreover, we conduct additional experiments in transfer learning and low-resource scenarios, showcasing the substantial advantages of KnowLog. Our source code and detailed experimental data are available at https://github.com/LeaperOvO/KnowLog.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {32},
	numpages = {13},
	keywords = {pre-trained language model, knowledge enhancement, log understanding},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3623322,
	author = {Zhang, Yakun and Zhang, Wenjie and Ran, Dezhi and Zhu, Qihao and Dou, Chengfeng and Hao, Dan and Xie, Tao and Zhang, Lu},
	title = {Learning-based Widget Matching for Migrating GUI Test Cases},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3623322},
	doi = {10.1145/3597503.3623322},
	abstract = {GUI test case migration is to migrate GUI test cases from a source app to a target app. The key of test case migration is widget matching. Recently, researchers have proposed various approaches by formulating widget matching as a matching task. However, since these matching approaches depend on static word embeddings without using contextual information to represent widgets and manually formulated matching functions, there are main limitations of these matching approaches when handling complex matching relations in apps. To address the limitations, we propose the first learning-based widget matching approach named TEMdroid (TEst Migration) for test case migration. Unlike the existing approaches, TEMdroid uses BERT to capture contextual information and learns a matching model to match widgets. Additionally, to balance the significant imbalance between positive and negative samples in apps, we design a two-stage training strategy where we first train a hard-negative sample miner to mine hard-negative samples, and further train a matching model using positive samples and mined hard-negative samples. Our evaluation on 34 apps shows that TEM-droid is effective in event matching (i.e., widget matching and target event synthesis) and test case migration. For event matching, TEM-droid's Top1 accuracy is 76\%, improving over 17\% compared to baselines. For test case migration, TEMdroid's F1 score is 89\%, also 7\% improvement compared to the baseline approach.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {69},
	numpages = {13},
	keywords = {test migration, GUI testing, deep learning},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639126,
	author = {Zhou, Zhenhao and Sha, Chaofeng and Peng, Xin},
	title = {On Calibration of Pre-trained Code Models},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639126},
	doi = {10.1145/3597503.3639126},
	abstract = {Pre-trained code models have achieved notable success in the field of Software Engineering (SE). However, existing studies have predominantly focused on improving model performance, with limited attention given to other critical aspects such as model calibration. Model calibration, which refers to the accurate estimation of predictive uncertainty, is a vital consideration in practical applications. Therefore, in order to advance the understanding of model calibration in SE, we conduct a comprehensive investigation into the calibration of pre-trained code models in this paper. Our investigation focuses on five pre-trained code models and four code understanding tasks, including analyses of calibration in both in-distribution and out-of-distribution settings. Several key insights are uncovered: (1) pre-trained code models may suffer from the issue of over-confidence; (2) temperature scaling and label smoothing are effective in calibrating code models in in-distribution data; (3) the issue of over-confidence in pre-trained code models worsens in different out-of-distribution settings, and the effectiveness of temperature scaling and label smoothing diminishes. All materials used in our experiments are available at https://github.com/queserasera22/Calibration-of-Pretrained-Code-Models.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {77},
	numpages = {13},
	keywords = {pre-trained code models, model calibration, model reliability},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639217,
	author = {Wang, Wenhan and Li, Yanzhou and Li, Anran and Zhang, Jian and Ma, Wei and Liu, Yang},
	title = {An Empirical Study on Noisy Label Learning for Program Understanding},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639217},
	doi = {10.1145/3597503.3639217},
	abstract = {Recently, deep learning models have been widely applied in program understanding tasks, and these models achieve state-of-the-art results on many benchmark datasets. A major challenge of deep learning for program understanding is that the effectiveness of these approaches depends on the quality of their datasets, and these datasets often contain noisy data samples. A typical kind of noise in program understanding datasets is label noise, which means that the target outputs for some inputs are incorrect.Researchers have proposed various approaches to alleviate the negative impact of noisy labels, and formed a new research topic: noisy label learning (NLL). In this paper, we conduct an empirical study on the effectiveness of noisy label learning on deep learning for program understanding datasets. We evaluate various NLL approaches and deep learning models on three tasks: program classification, vulnerability detection, and code summarization. From the evaluation results, we come to the following findings: 1) small trained-from-scratch models are prone to label noises in program understanding, while large pre-trained models are highly robust against them. 2) NLL approaches significantly improve the program classification accuracies for small models on noisy training sets, but they only slightly benefit large pre-trained models in classification accuracies. 3) NLL can effectively detect synthetic noises in program understanding, but struggle in detecting real-world noises. We believe our findings can provide insights on the abilities of NLL in program understanding, and shed light on future works in tackling noises in software engineering datasets. We have released our code at https://github.com/jacobwwh/noise_SE.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {95},
	numpages = {12},
	keywords = {program understanding, deep learning, noisy label learning},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639150,
	author = {Ma, Zeyang and Chen, An Ran and Kim, Dong Jae and Chen, Tse-Hsun and Wang, Shaowei},
	title = {LLMParser: An Exploratory Study on Using Large Language Models for Log Parsing},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639150},
	doi = {10.1145/3597503.3639150},
	abstract = {Logs are important in modern software development with runtime information. Log parsing is the first step in many log-based analyses, that involve extracting structured information from unstructured log data. Traditional log parsers face challenges in accurately parsing logs due to the diversity of log formats, which directly impacts the performance of downstream log-analysis tasks. In this paper, we explore the potential of using Large Language Models (LLMs) for log parsing and propose LLMParser, an LLM-based log parser based on generative LLMs and few-shot tuning. We leverage four LLMs, Flan-T5-small, Flan-T5-base, LLaMA-7B, and ChatGLM-6B in LLMParsers. Our evaluation of 16 open-source systems shows that LLMParser achieves statistically significantly higher parsing accuracy than state-of-the-art parsers (a 96\% average parsing accuracy). We further conduct a comprehensive empirical analysis on the effect of training size, model size, and pre-training LLM on log parsing accuracy. We find that smaller LLMs may be more effective than more complex LLMs; for instance where Flan-T5-base achieves comparable results as LLaMA-7B with a shorter inference time. We also find that using LLMs pre-trained using logs from other systems does not always improve parsing accuracy. While using pre-trained Flan-T5-base shows an improvement in accuracy, pre-trained LLaMA results in a decrease (decrease by almost 55\% in group accuracy). In short, our study provides empirical evidence for using LLMs for log parsing and highlights the limitations and future research direction of LLM-based log parsers.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {99},
	numpages = {13},
	keywords = {log parsing, log analysis, large language model},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639117,
	author = {Sun, Yuqiang and Wu, Daoyuan and Xue, Yue and Liu, Han and Wang, Haijun and Xu, Zhengzi and Xie, Xiaofei and Liu, Yang},
	title = {GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639117},
	doi = {10.1145/3597503.3639117},
	abstract = {Smart contracts are prone to various vulnerabilities, leading to substantial financial losses over time. Current analysis tools mainly target vulnerabilities with fixed control- or data-flow patterns, such as re-entrancy and integer overflow. However, a recent study on Web3 security bugs revealed that about 80\% of these bugs cannot be audited by existing tools due to the lack of domain-specific property description and checking. Given recent advances in Large Language Models (LLMs), it is worth exploring how Generative Pre-training Transformer (GPT) could aid in detecting logic vulnerabilities.In this paper, we propose GPTScan, the first tool combining GPT with static analysis for smart contract logic vulnerability detection. Instead of relying solely on GPT to identify vulnerabilities, which can lead to high false positives and is limited by GPT's pre-trained knowledge, we utilize GPT as a versatile code understanding tool. By breaking down each logic vulnerability type into scenarios and properties, GPTScan matches candidate vulnerabilities with GPT. To enhance accuracy, GPTScan further instructs GPT to intelligently recognize key variables and statements, which are then validated by static confirmation. Evaluation on diverse datasets with around 400 contract projects and 3K Solidity files shows that GPTScan achieves high precision (over 90\%) for token contracts and acceptable precision (57.14\%) for large projects like Web3Bugs. It effectively detects ground-truth logic vulnerabilities with a recall of over 70\%, including 9 new vulnerabilities missed by human auditors. GPTScan is fast and cost-effective, taking an average of 14.39 seconds and 0.01 USD to scan per thousand lines of Solidity code. Moreover, static confirmation helps GPTScan reduce two-thirds of false positives.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {166},
	numpages = {13},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639194,
	author = {Tanzil, Minaoar Hossain and Khan, Junaed Younus and Uddin, Gias},
	title = {ChatGPT Incorrectness Detection in Software Reviews},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639194},
	doi = {10.1145/3597503.3639194},
	abstract = {We conducted a survey of 135 software engineering (SE) practitioners to understand how they use Generative AI-based chatbots like ChatGPT for SE tasks. We find that they want to use ChatGPT for SE tasks like software library selection but often worry about the truthfulness of ChatGPT responses. We developed a suite of techniques and a tool called CID (ChatGPT Incorrectness Detector) to automatically test and detect the incorrectness in ChatGPT responses. CID is based on the iterative prompting to ChatGPT by asking it contextually similar but textually divergent questions (using an approach that utilizes metamorphic relationships in texts). The underlying principle in CID is that for a given question, a response that is different from other responses (across multiple incarnations of the question) is likely an incorrect response. In a benchmark study of library selection, we show that CID can detect incorrect responses from ChatGPT with an F1-score of 0.74 -- 0.75.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {180},
	numpages = {12},
	keywords = {large language model, chatGPT, hallucination, testing},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639202,
	author = {Sun, Jiamou and Chen, Jieshan and Xing, Zhenchang and Lu, Qinghua and Xu, Xiwei and Zhu, Liming},
	title = {Where is it? Tracing the Vulnerability-relevant Files from Vulnerability Reports},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639202},
	doi = {10.1145/3597503.3639202},
	abstract = {With the widely usage of open-source software, supply-chain-based vulnerability attacks, including SolarWind and Log4Shell, have posed significant risks to software security. Currently, people rely on vulnerability advisory databases or commercial software bill of materials (SBOM) to defend against potential risks. Unfortunately, these datasets do not provide finer-grained file-level vulnerability information, compromising their effectiveness. Previous works have not adequately addressed this issue, and mainstream vulnerability detection methods have their drawbacks that hinder resolving this gap. Driven by the real needs, we propose a framework that can trace the vulnerability-relevant file for each disclosed vulnerability. Our approach uses NVD descriptions with metadata as the inputs, and employs a series of strategies with a LLM model, search engine, heuristic-based text matching method and a deep learning classifier to recommend the most likely vulnerability-relevant file, effectively enhancing the completeness of existing NVD data. Our experiments confirm that the efficiency of the proposed framework, with CodeBERT achieving 0.92 AUC and 0.85 MAP, and our user study proves our approach can help with vulnerability-relevant file detection effectively. To the best of our knowledge, our work is the first one focusing on tracing vulnerability-relevant files, laying the groundwork of building finer-grained vulnerability-aware software bill of materials.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {200},
	numpages = {13},
	keywords = {vulnerability-relevant file, security, software supply chain},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3663529.3663785,
	author = {Wang, Yiran and L\'{o}pez, Jos\'{e} Antonio Hern\'{a}ndez and Nilsson, Ulf and Varr\'{o}, D\'{a}niel},
	title = {Using Run-Time Information to Enhance Static Analysis of Machine Learning Code in Notebooks},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3663785},
	doi = {10.1145/3663529.3663785},
	abstract = {A prevalent method for developing machine learning (ML) prototypes involves the use of notebooks. Notebooks are sequences of cells containing both code and natural language documentation. When executed during development, these code cells provide valuable run-time information. Nevertheless, current static analyzers for notebooks do not leverage this run-time information to detect ML bugs. Consequently, our primary proposition in this paper is that harvesting this run-time information in notebooks can significantly improve the effectiveness of static analysis in detecting ML bugs. To substantiate our claim, we focus on bugs related to tensor shapes and conduct experiments using two static analyzers: 1) PYTHIA, a traditional rule-based static analyzer, and 2) GPT-4, a large language model that can also be used as a static analyzer. The results demonstrate that using run-time information in static analyzers enhances their bug detection performance and it also helped reveal a hidden bug in a public dataset.},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {497–501},
	numpages = {5},
	keywords = {large language models, machine learning bugs, notebook, run-time information, static analysis},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@inproceedings{10.1145/3663529.3663794,
	author = {Hora, Andre},
	title = {Predicting Test Results without Execution},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3663794},
	doi = {10.1145/3663529.3663794},
	abstract = {As software systems grow, test suites may become complex, making it challenging to run the tests frequently and locally. Recently, Large Language Models (LLMs) have been adopted in multiple software engineering tasks. It has demonstrated great results in code generation, however, it is not yet clear whether these models understand code execution. Particularly, it is unclear whether LLMs can be used to predict test results, and, potentially, overcome the issues of running real-world tests. To shed some light on this problem, in this paper, we explore the capability of LLMs to predict test results without execution. We evaluate the performance of the state-of-the-art GPT-4 in predicting the execution of 200 test cases of the Python Standard Library. Among these 200 test cases, 100 are passing and 100 are failing ones. Overall, we find that GPT-4 has a precision of 88.8\%, recall of 71\%, and accuracy of 81\% in the test result prediction. However, the results vary depending on the test complexity: GPT-4 presented better precision and recall when predicting simpler tests (93.2\% and 82\%) than complex ones (83.3\% and 60\%). We also find differences among the analyzed test suites, with the precision ranging from 77.8\% to 94.7\% and recall between 60\% and 90\%. Our findings suggest that GPT-4 still needs significant progress in predicting test results.},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {542–546},
	numpages = {5},
	keywords = {GPT-4, LLMs, large language models, software testing},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@article{10323231,
	author = {Shen, Yuchen and Breaux, Travis},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ Stakeholder Preference Extraction From Scenarios }},
	year = {2024},
	volume = {50},
	number = {01},
	ISSN = {1939-3520},
	pages = {69-84},
	abstract = { Companies use personalization to tailor user experiences. Personalization appears in search engines and online stores, which include salutations and statistically learned correlations over search-, browsing- and purchase-histories. However, users have a wider variety of substantive, domain-specific preferences that affect their choices when they use directory services, and these have largely been overlooked or ignored. The contributions of this paper include: (1) a grounded theory describing how stakeholder preferences are expressed in text scenarios; (2) an app feature survey to assess whether elicited preferences represent missing requirements in existing systems; (3) an evaluation of three classifiers to label preference words in scenarios; and (4) a linker to build preference phrases by linking labeled preference words to each other based on word position. In this study, the authors analyzed 217 elicited directory service scenarios across 12 domain categories to yield a total of 7,661 stakeholder preferences labels. The app survey yielded 43 stakeholder preferences that were missed on average 49.7% by 15 directory service websites studied. The BERT-based transformer showed the best average overall 81.1% precision, 84.4% recall and 82.6% F1-score when tested on unseen domains. Finally, the preference linker correctly links preference phrases with 90.1% accuracy. Given these results, we believe directory service developers can use this approach to automatically identify user preferences to improve service designs. },
	keywords = {Stakeholders;Software;Surveys;Feature extraction;Transformers;Syntactics;Machine learning},
	doi = {10.1109/TSE.2023.3333265},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3333265},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {jan}
}


@article{10402095,
	author = {Zhang, Quanjun and Fang, Chunrong and Sun, Weisong and Liu, Yan and He, Tieke and Hao, Xiaodong and Chen, Zhenyu},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ APPT: Boosting Automated Patch Correctness Prediction via Fine-Tuning Pre-Trained Models }},
	year = {2024},
	volume = {50},
	number = {03},
	ISSN = {1939-3520},
	pages = {474-494},
	abstract = { Automated program repair (APR) aims to fix software bugs automatically without human debugging efforts and plays a crucial role in software development and maintenance. Despite the recent significant progress in the number of fixed bugs, APR is still challenged by a long-standing overfitting problem (i.e., the generated patch is plausible but overfitting). Various techniques have thus been proposed to address the overfitting problem. Recently, researchers have employed BERT to extract code features, which are then used to train a classifier for patch correctness prediction, indicating the potential of such pre-trained models in reasoning about patch correctness. However, BERT is restricted to feature extraction for classifier training without benefiting from the training process, potentially generating sub-optimal vector representations for patched code snippets. In this paper, we propose APPT, a pre-trained model-based automated patch correctness assessment technique by both pre-training and fine-tuning. APPT adopts a pre-trained model as the encoder stack, followed by an LSTM stack and a deep learning classifier. More importantly, the pre-trained model is fine-tuned in conjunction with other components as a whole pipeline to fully adapt it specifically for reasoning about patch correctness. Although our idea is general and can be built on various existing pre-trained models, we have implemented APPT based on the BERT model. We conduct an extensive experiment on 1,183 Defects4J patches and the experimental results show that APPT achieves prediction accuracy of 79.7% and recall of 83.2%, outperforming the state-of-the-art technique CACHE by 4.3% and 6.7%. Our additional investigation on 49,694 real-world patches shows that APPT achieves the optimum performance (exceeding 99% in five common metrics for assessing patch classification techniques) compared with existing representation learning techniques. We further investigate the impact of each component and find that they all positively contribute to APPT, e.g., the fine-tuning process and the LSTM stack increase F1-score by 10.22% and 4.11%, respectively. We also prove that adopting advanced pre-trained models can further provide substantial advancement (e.g., GraphCodeBERT-based APPT improves BERT-based APPT by 2.8% and 3.3% in precision and AUC, respectively), highlighting the generalizability of APPT. Overall, our study highlights the promising future of fine-tuning pre-trained models to assess patch correctness and reduce the manual inspection effort of debugging experts when deploying APR tools in practice. },
	keywords = {Codes;Feature extraction;Task analysis;Predictive models;Maintenance engineering;Computer bugs;Adaptation models},
	doi = {10.1109/TSE.2024.3354969},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3354969},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {mar}
}


@article{10586831,
	author = {Cui, Lei and Yin, Junnan and Cui, Jiancong and Ji, Yuede and Liu, Peng and Hao, Zhiyu and Yun, Xiaochun},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ API2Vec++: Boosting API Sequence Representation for Malware Detection and Classification }},
	year = {2024},
	volume = {50},
	number = {08},
	ISSN = {1939-3520},
	pages = {2142-2162},
	abstract = { Analyzing malware based on API call sequences is an effective approach, as these sequences reflect the dynamic execution behavior of malware. Recent advancements in deep learning have facilitated the application of these techniques to mine valuable information from API call sequences. However, these methods typically operate on raw sequences and may not effectively capture crucial information, especially in the case of multi-process malware, due to the API call interleaving problem. Furthermore, they often fail to capture contextual behaviors within or across processes, which is particularly important for identifying and classifying malicious activities. Motivated by this, we present API2Vec++, a graph-based API embedding method for malware detection and classification. First, we construct a graph model to represent the raw sequence. Specifically, we design the Temporal Process Graph (TPG) to model inter-process behaviors and the Temporal API Property Graph (TAPG) to model intra-process behaviors. Compared to our previous graph model, the TAPG model exposes operations with associated behaviors within the process through node properties and thus enhances detection and classification abilities. Using these graphs, we develop a heuristic random walk algorithm to generate numerous paths that can capture fine-grained malicious familial behavior. By pre-training these paths using the BERT model, we generate embeddings of paths and APIs, which can then be used for malware detection and classification. Experiments on a real-world malware dataset demonstrate that API2Vec++ outperforms state-of-the-art embedding methods and detection/classification methods in both accuracy and robustness, particularly for multi-process malware. },
	keywords = {Malware;Logic;Legged locomotion;Task analysis;Feature extraction;Encoding;Runtime},
	doi = {10.1109/TSE.2024.3422990},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3422990},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {aug}
}


@article{10648982,
	author = {Sun, Weifeng and Guo, Zhenting and Yan, Meng and Liu, Zhongxin and Lei, Yan and Zhang, Hongyu},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ Method-Level Test-to-Code Traceability Link Construction by Semantic Correlation Learning }},
	year = {2024},
	volume = {50},
	number = {10},
	ISSN = {1939-3520},
	pages = {2656-2676},
	abstract = { Test-to-code traceability links (TCTLs) establish links between test artifacts and code artifacts. These links enable developers and testers to quickly identify the specific pieces of code tested by particular test cases, thus facilitating more efficient debugging, regression testing, and maintenance activities. Various approaches, based on distinct concepts, have been proposed to establish method-level TCTLs, specifically linking unit tests to corresponding focal methods. Static methods, such as naming-convention-based methods, use heuristic- and similarity-based strategies. However, such methods face the following challenges: 1. Developers, driven by specific scenarios and development requirements, may deviate from naming conventions, leading to TCTL identification failures. 2. Static methods often overlook the rich semantics embedded within tests, leading to erroneous associations between tests and semantically unrelated code fragments. Although dynamic methods achieve promising results, they require the project to be compilable and the tests to be executable, limiting their usability. This limitation is significant for downstream tasks requiring massive test-code pairs, as not all projects can meet these requirements. To tackle the abovementioned limitations, we propose a novel static method-level TCTL approach, named TestLinker. For the first challenge of existing static approaches, TestLinker introduces a two-phase TCTL framework to accommodate different project types in a triage manner. As for the second challenge, we employ the semantic correlation learning, which learns and establishes the semantic correlations between tests and focal methods based on Pre-trained Code Models (PCMs). TestLinker further establishes mapping rules to accurately link the recommended function name to the concrete production function declaration. Empirical evaluation on a meticulously labeled dataset reveals that TestLinker significantly outperforms traditional static techniques, showing average F1-score improvements ranging from 73.48% to 202.00%. Moreover, compared to state-of-the-art dynamic methods, TestLinker, which only leverages static information, demonstrates comparable or even better performance, with an average F1-score increase of 37.40%. },
	keywords = {Codes;Task analysis;Phase change materials;Semantics;Software;Production;Correlation},
	doi = {10.1109/TSE.2024.3449917},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3449917},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {oct}
}


@article{10659742,
	author = {Zhou, Xin and Xu, Bowen and Kim, Kisub and Han, DongGyun and Nguyen, Hung Huu and Le-Cong, Thanh and He, Junda and Le, Bach and Lo, David},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ Leveraging Large Language Model for Automatic Patch Correctness Assessment }},
	year = {2024},
	volume = {50},
	number = {11},
	ISSN = {1939-3520},
	pages = {2865-2883},
	abstract = { Automated Program Repair (APR) techniques have shown more and more promising results in fixing real-world bugs. Despite the effectiveness, APR techniques still face an overfitting problem: a generated patch can be incorrect although it passes all tests. It is time-consuming to manually evaluate the correctness of generated patches that can pass all available test cases. To address this problem, many approaches have been proposed to automatically assess the correctness of patches generated by APR techniques. These approaches are mainly evaluated within the cross-validation setting. However, for patches generated by a new or unseen APR tool, users are implicitly required to manually label a significant portion of these patches (e.g., 90% in 10-fold cross-validation) in the cross-validation setting before inferring the remaining patches (e.g., 10% in 10-fold cross-validation). To mitigate the issue, in this study, we propose LLM4PatchCorrect, the patch correctness assessment by adopting a large language model for code. Specifically, for patches generated by a new or unseen APR tool, LLM4PatchCorrect does not need labeled patches of this new or unseen APR tool for training but directly queries the large language model for code to get predictions on the correctness labels without training. In this way, LLM4PatchCorrect can reduce the manual labeling effort when building a model to automatically assess the correctness of generated patches of new APR tools. To provide knowledge regarding the automatic patch correctness assessment (APCA) task to the large language model for code, LLM4PatchCorrect leverages bug descriptions, execution traces, failing test cases, test coverage, and labeled patches generated by existing APR tools, before deciding the correctness of the unlabeled patches of a new or unseen APR tool. Additionally, LLM4PatchCorrect prioritizes labeled patches from existing APR tools that exhibit semantic similarity to those generated by new APR tools, enhancing the accuracy achieved by LLM4PatchCorrect for patches from new APR tools. Our experimental results showed that LLM4PatchCorrect can achieve an accuracy of 84.4% and an F1-score of 86.5% on average although no labeled patch of the new or unseen APR tool is available. In addition, our proposed technique significantly outperformed the prior state-of-the-art. },
	keywords = {Computer bugs;Codes;Task analysis;Large language models;Feature extraction;Training;Manuals},
	doi = {10.1109/TSE.2024.3452252},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3452252},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {nov}
}


@article{10746847,
	author = {Jiang, Yuan and Zhang, Yujian and Su, Xiaohong and Treude, Christoph and Wang, Tiantian},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ StagedVulBERT: Multigranular Vulnerability Detection With a Novel Pretrained Code Model }},
	year = {2024},
	volume = {50},
	number = {12},
	ISSN = {1939-3520},
	pages = {3454-3471},
	abstract = { The emergence of pre-trained model-based vulnerability detection methods has significantly advanced the field of automated vulnerability detection. However, these methods still face several challenges, such as difficulty in learning effective feature representations of statements for fine-grained predictions and struggling to process overly long code sequences. To address these issues, this study introduces StagedVulBERT, a novel vulnerability detection framework that leverages a pre-trained code language model and employs a coarse-to-fine strategy. The key innovation and contribution of our research lies in the development of the CodeBERT-HLS component within our framework, specialized in hierarchical, layered, and semantic encoding. This component is designed to capture semantics at both the token and statement levels simultaneously, which is crucial for achieving more accurate multi-granular vulnerability detection. Additionally, CodeBERT-HLS efficiently processes longer code token sequences, making it more suited to real-world vulnerability detection. Comprehensive experiments demonstrate that our method enhances the performance of vulnerability detection at both coarse- and fine-grained levels. Specifically, in coarse-grained vulnerability detection, StagedVulBERT achieves an F1 score of 92.26%, marking a 6.58% improvement over the best-performing methods. At the fine-grained level, our method achieves a Top-5% accuracy of 65.69%, which outperforms the state-of-the-art methods by up to 75.17%. },
	keywords = {Codes;Transformers;Feature extraction;Semantics;Training;Accuracy;Technological innovation;Predictive models;Heating systems;Computer architecture},
	doi = {10.1109/TSE.2024.3493245},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3493245},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {dec}
}


@inproceedings{10.1145/3597503.3639216,
	author = {Gao, Shuzheng and Mao, Wenxin and Gao, Cuiyun and Li, Li and Hu, Xing and Xia, Xin and Lyu, Michael R.},
	title = {Learning in the Wild: Towards Leveraging Unlabeled Data for Effectively Tuning Pre-trained Code Models},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639216},
	doi = {10.1145/3597503.3639216},
	abstract = {Pre-trained code models have recently achieved substantial improvements in many code intelligence tasks. These models are first pre-trained on large-scale unlabeled datasets in a task-agnostic manner using self-supervised learning, and then fine-tuned on labeled datasets in downstream tasks. However, the labeled datasets are usually limited in size (i.e., human intensive efforts), which may hinder the performance of pre-trained code models in specific tasks. To mitigate this, one possible solution is to leverage the large-scale unlabeled data in the tuning stage by pseudo-labeling, i.e., generating pseudo labels for unlabeled data and further training the pre-trained code models with the pseudo-labeled data. However, directly employing the pseudo-labeled data can bring a large amount of noise, i.e., incorrect labels, leading to suboptimal performance. How to effectively leverage the noisy pseudo-labeled data is a challenging yet under-explored problem.In this paper, we propose a novel approach named HINT to improve pre-trained code models with large-scale unlabeled datasets by better utilizing the pseudo-labeled data. HINT includes two main modules: Hybrid pseudo-labeled data selection and Noise-tolerant Training. In the hybrid pseudo-data selection module, considering the robustness issue, apart from directly measuring the quality of pseudo labels through training loss, we propose to further employ a retrieval-based method to filter low-quality pseudo-labeled data. The noise-tolerant training module aims to further mitigate the influence of errors in pseudo labels by training the model with a noise-tolerant loss function and by regularizing the consistency of model predictions. We evaluate the effectiveness of HINT on three popular code intelligence tasks, including code summarization, defect detection, and assertion generation. We build our method on top of three popular open-source pre-trained code models. The experimental results show that HINT can better leverage those unlabeled data in a task-specific way and provide complementary benefits for pre-trained models, e.g., improving the best baseline model by 15.33\%, 16.50\%, and 8.98\% on code summarization, defect detection, and assertion generation, respectively.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {80},
	numpages = {13},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@article{10599336,
	author = {Nashaat, Mona and Miller, James},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ Towards Efficient Fine-Tuning of Language Models With Organizational Data for Automated Software Review }},
	year = {2024},
	volume = {50},
	number = {09},
	ISSN = {1939-3520},
	pages = {2240-2253},
	abstract = { Large language models like BERT and GPT possess significant capabilities and potential impacts across various applications. Software engineers often use these models for code-related tasks, including generating, debugging, and summarizing code. Nevertheless, large language models still have several flaws, including model hallucination. (e.g., generating erroneous code and producing outdated and inaccurate programs) and the substantial computational resources and energy required for training and fine-tuning. To tackle these challenges, we propose CodeMentor, a framework for few-shot learning to train large language models with the data available within the organization. We employ the framework to train a language model for code review activities, such as code refinement and review generation. The framework utilizes heuristic rules and weak supervision techniques to leverage available data, such as previous review comments, issue reports, and related code updates. Then, the framework employs the constructed dataset to fine-tune LLMs for code review tasks. Additionally, the framework integrates domain expertise by employing reinforcement learning with human feedback. This allows domain experts to assess the generated code and enhance the model performance. Also, to assess the performance of the proposed model, we evaluate it with four state-of-the-art techniques in various code review tasks. The experimental results attest that CodeMentor enhances the performance in all tasks compared to the state-of-the-art approaches, with an improvement of up to 22.3%, 43.4%, and 24.3% in code quality estimation, review generation, and bug report summarization tasks, respectively. },
	keywords = {Codes;Reviews;Task analysis;Data models;Large language models;Computational modeling;Training},
	doi = {10.1109/TSE.2024.3428324},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3428324},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {sep}
}


@article{10704582,
	author = {Fatima, Sakina and Hemmati, Hadi and C. Briand, Lionel},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ FlakyFix: Using Large Language Models for Predicting Flaky Test Fix Categories and Test Code Repair }},
	year = {2024},
	volume = {50},
	number = {12},
	ISSN = {1939-3520},
	pages = {3146-3171},
	abstract = { Flaky tests are problematic because they non-deterministically pass or fail for the same software version under test, causing confusion and wasting development effort. While machine learning models have been used to predict flakiness and its root causes, there is much less work on providing support to fix the problem. To address this gap, in this paper, we focus on predicting the type of fix that is required to remove flakiness and then repair the test code on that basis. We do this for a subset of flaky tests where the root cause of flakiness is in the test itself and not in the production code. One key idea is to guide the repair process with additional knowledge about the test's flakiness in the form of its predicted fix category. Thus, we first propose a framework that automatically generates labeled datasets for 13 fix categories and trains models to predict the fix category of a flaky test by analyzing the test code only. Our experimental results using code models and few-shot learning show that we can correctly predict most of the fix categories. To show the usefulness of such fix category labels for automatically repairing flakiness, we augment the prompts of GPT 3.5 Turbo, a Large Language Model (LLM), with such extra knowledge to request repair suggestions. The results show that our suggested fix category labels, complemented with in-context learning, significantly enhance the capability of GPT 3.5 Turbo in generating fixes for flaky tests. Based on the execution and analysis of a sample of GPT-repaired flaky tests, we estimate that a large percentage of such repairs, (roughly between 51% and 83%) can be expected to pass. For the failing repaired tests, on average, 16% of the test code needs to be further changed for them to pass. },
	keywords = {Codes;Predictive models;Maintenance engineering;Analytical models;Production;Large language models;Java;Few shot learning;Python;Manuals},
	doi = {10.1109/TSE.2024.3472476},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3472476},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {dec}
}


@inproceedings{10.1145/3597503.3649399,
	author = {Rinard, Martin},
	title = {Software Engineering Research in a World with Generative Artificial Intelligence},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3649399},
	doi = {10.1145/3597503.3649399},
	abstract = {Generative artificial intelligence systems such as large language models (LLMs) exhibit powerful capabilities that many see as the kind of flexible and adaptive intelligence that previously only humans could exhibit. I address directions and implications of LLMs for software engineering research.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {2},
	numpages = {5},
	keywords = {software engineering, generative artificial intelligence, large language models},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3608132,
	author = {Peng, Yun and Gao, Shuzheng and Gao, Cuiyun and Huo, Yintong and Lyu, Michael},
	title = {Domain Knowledge Matters: Improving Prompts with Fix Templates for Repairing Python Type Errors},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3608132},
	doi = {10.1145/3597503.3608132},
	abstract = {As a dynamic programming language, Python has become increasingly popular in recent years. Although the dynamic type system of Python facilitates the developers in writing Python programs, it also brings type errors at run-time which are prevalent yet not easy to fix. There exist rule-based approaches for automatically repairing Python type errors. The approaches can generate accurate patches for the type errors covered by manually defined templates, but they require domain experts to design patch synthesis rules and suffer from low template coverage of real-world type errors. Learning-based approaches alleviate the manual efforts in designing patch synthesis rules and have become prevalent due to the recent advances in deep learning. Among the learning-based approaches, the prompt-based approach which leverages the knowledge base of code pre-trained models via pre-defined prompts, obtains state-of-the-art performance in general program repair tasks. However, such prompts are manually defined and do not involve any specific clues for repairing Python type errors, resulting in limited effectiveness. How to automatically improve prompts with the domain knowledge for type error repair is challenging yet under-explored.In this paper, we present TypeFix, a novel prompt-based approach with fix templates incorporated for repairing Python type errors. TypeFix first mines generalized fix templates via a novel hierarchical clustering algorithm. The identified fix templates indicate the common edit patterns and contexts of existing type error fixes. TypeFix then generates code prompts for code pre-trained models by employing the generalized fix templates as domain knowledge, in which the masks are adaptively located for each type error instead of being pre-determined. Experiments on two benchmarks, including BugsInPy and TypeBugs, show that TypeFix successfully repairs 26 and 55 type errors, outperforming the best baseline approach by 9 and 14, respectively. Besides, the proposed fix template mining approach can cover 75\% of developers' patches in both benchmarks, increasing the best rule-based approach PyTER by more than 30\%.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {4},
	numpages = {13},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3623326,
	author = {Xu, Junjielong and Cui, Ziang and Zhao, Yuan and Zhang, Xu and He, Shilin and He, Pinjia and Li, Liqun and Kang, Yu and Lin, Qingwei and Dang, Yingnong and Rajmohan, Saravan and Zhang, Dongmei},
	title = {UniLog: Automatic Logging via LLM and In-Context Learning},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3623326},
	doi = {10.1145/3597503.3623326},
	abstract = {Logging, which aims to determine the position of logging statements, the verbosity levels, and the log messages, is a crucial process for software reliability enhancement. In recent years, numerous automatic logging tools have been designed to assist developers in one of the logging tasks (e.g., providing suggestions on whether to log in try-catch blocks). These tools are useful in certain situations yet cannot provide a comprehensive logging solution in general. Moreover, although recent research has started to explore end-to-end logging, it is still largely constrained by the high cost of fine-tuning, hindering its practical usefulness in software development. To address these problems, this paper proposes UniLog, an automatic logging framework based on the in-context learning (ICL) paradigm of large language models (LLMs). Specifically, UniLog can generate an appropriate logging statement with only a prompt containing five demonstration examples without any model tuning. In addition, UniLog can further enhance its logging ability after warmup with only a few hundred random samples. We evaluated UniLog on a large dataset containing 12,012 code snippets extracted from 1,465 GitHub repositories. The results show that UniLog achieved the state-of-the-art performance in automatic logging: (1) 76.9\% accuracy in selecting logging positions, (2) 72.3\% accuracy in predicting verbosity levels, and (3) 27.1 BLEU-4 score in generating log messages. Meanwhile, UniLog requires less than 4\% of the parameter tuning time needed by fine-tuning the same LLM.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {14},
	numpages = {12},
	keywords = {logging, large language model, in-context learning},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3623298,
	author = {Huang, Yuchao and Wang, Junjie and Liu, Zhe and Wang, Yawen and Wang, Song and Chen, Chunyang and Hu, Yuanzhe and Wang, Qing},
	title = {CrashTranslator: Automatically Reproducing Mobile Application Crashes Directly from Stack Trace},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3623298},
	doi = {10.1145/3597503.3623298},
	abstract = {Crash reports are vital for software maintenance since they allow the developers to be informed of the problems encountered in the mobile application. Before fixing, developers need to reproduce the crash, which is an extremely time-consuming and tedious task. Existing studies conducted the automatic crash reproduction with the natural language described reproducing steps. Yet we find a non-neglectable portion of crash reports only contain the stack trace when the crash occurs. Such stack-trace-only crashes merely reveal the last GUI page when the crash occurs, and lack step-by-step guidance. Developers tend to spend more effort in understanding the problem and reproducing the crash, and existing techniques cannot work on this, thus calling for a greater need for automatic support. This paper proposes an approach named CrashTranslator to automatically reproduce mobile application crashes directly from the stack trace. It accomplishes this by leveraging a pre-trained Large Language Model to predict the exploration steps for triggering the crash, and designing a reinforcement learning based technique to mitigate the inaccurate prediction and guide the search holistically. We evaluate CrashTranslator on 75 crash reports involving 58 popular Android apps, and it successfully reproduces 61.3\% of the crashes, outperforming the state-of-the-art baselines by 109\% to 206\%. Besides, the average reproducing time is 68.7 seconds, outperforming the baselines by 302\% to 1611\%. We also evaluate the usefulness of CrashTranslator with promising results.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {18},
	numpages = {13},
	keywords = {bug reproduction, stack trace, mobile application testing},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3623306,
	author = {Guo, Qi and Cao, Junming and Xie, Xiaofei and Liu, Shangqing and Li, Xiaohong and Chen, Bihuan and Peng, Xin},
	title = {Exploring the Potential of ChatGPT in Automated Code Refinement: An Empirical Study},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3623306},
	doi = {10.1145/3597503.3623306},
	abstract = {Code review is an essential activity for ensuring the quality and maintainability of software projects. However, it is a time-consuming and often error-prone task that can significantly impact the development process. Recently, ChatGPT, a cutting-edge language model, has demonstrated impressive performance in various natural language processing tasks, suggesting its potential to automate code review processes. However, it is still unclear how well ChatGPT performs in code review tasks. To fill this gap, in this paper, we conduct the first empirical study to understand the capabilities of ChatGPT in code review tasks, specifically focusing on automated code refinement based on given code reviews. To conduct the study, we select the existing benchmark CodeReview and construct a new code review dataset with high quality. We use CodeReviewer, a state-of-the-art code review tool, as a baseline for comparison with ChatGPT. Our results show that ChatGPT outperforms CodeReviewer in code refinement tasks. Specifically, our results show that ChatGPT achieves higher EM and BLEU scores of 22.78 and 76.44 respectively, while the state-of-the-art method achieves only 15.50 and 62.88 on a high-quality code review dataset. We further identify the root causes for ChatGPT's underperformance and propose several strategies to mitigate these challenges. Our study provides insights into the potential of ChatGPT in automating the code review process, and highlights the potential research directions.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {34},
	numpages = {13},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3623316,
	author = {Yu, Hao and Shen, Bo and Ran, Dezhi and Zhang, Jiaxin and Zhang, Qi and Ma, Yuchi and Liang, Guangtai and Li, Ying and Wang, Qianxiang and Xie, Tao},
	title = {CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3623316},
	doi = {10.1145/3597503.3623316},
	abstract = {Code generation models based on the pre-training and fine-tuning paradigm have been increasingly attempted by both academia and industry, resulting in well-known industrial models such as Codex, CodeGen, and PanGu-Coder. To evaluate the effectiveness of these models, multiple existing benchmarks (e.g., HumanEval and AiXBench) are proposed, including only cases of generating a standalone function, i.e., a function that may invoke or access only built-in functions and standard libraries. However, non-standalone functions, which typically are not included in the existing benchmarks, constitute more than 70\% of the functions in popular open-source projects, and evaluating models' effectiveness on standalone functions cannot reflect these models' effectiveness on pragmatic code generation scenarios (i.e., code generation for real settings of open source or proprietary code).To help bridge the preceding gap, in this paper, we propose a benchmark named CoderEval, consisting of 230 Python and 230 Java code generation tasks carefully curated from popular real-world open-source projects and a self-contained execution platform to automatically assess the functional correctness of generated code. CoderEval supports code generation tasks from six levels of context dependency, where context refers to code elements such as types, APIs, variables, and consts defined outside the function under generation but within the dependent third-party libraries, current class, file, or project. CoderEval can be used to evaluate the effectiveness of models in generating code beyond only standalone functions. By evaluating three state-of-the-art code generation models (CodeGen, PanGu-Coder, and ChatGPT) on CoderEval and HumanEval, we find that the effectiveness of these models in generating standalone functions is substantially higher than that in generating non-standalone functions. Our analysis highlights the current progress and pinpoints future directions to further improve a model's effectiveness by leveraging contextual information for pragmatic code generation.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {37},
	numpages = {12},
	keywords = {code generation, large language models, benchmark},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3608134,
	author = {Geng, Mingyang and Wang, Shangwen and Dong, Dezun and Wang, Haotian and Li, Ge and Jin, Zhi and Mao, Xiaoguang and Liao, Xiangke},
	title = {Large Language Models are Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3608134},
	doi = {10.1145/3597503.3608134},
	abstract = {Code comment generation aims at generating natural language descriptions for a code snippet to facilitate developers' program comprehension activities. Despite being studied for a long time, a bottleneck for existing approaches is that given a code snippet, they can only generate one comment while developers usually need to know information from diverse perspectives such as what is the functionality of this code snippet and how to use it. To tackle this limitation, this study empirically investigates the feasibility of utilizing large language models (LLMs) to generate comments that can fulfill developers' diverse intents. Our intuition is based on the facts that (1) the code and its pairwise comment are used during the pre-training process of LLMs to build the semantic connection between the natural language and programming language, and (2) comments in the real-world projects, which are collected for the pre-training, usually contain different developers' intents. We thus postulate that the LLMs can already understand the code from different perspectives after the pre-training. Indeed, experiments on two large-scale datasets demonstrate the rationale of our insights: by adopting the in-context learning paradigm and giving adequate prompts to the LLM (e.g., providing it with ten or more examples), the LLM can significantly outperform a state-of-the-art supervised learning approach on generating comments with multiple intents. Results also show that customized strategies for constructing the prompts and post-processing strategies for reranking the results can both boost the LLM's performances, which shed light on future research directions for using LLMs to achieve comment generation.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {39},
	numpages = {13},
	keywords = {code summarization, large language model, in-context learning},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3608137,
	author = {Feng, Sidong and Chen, Chunyang},
	title = {Prompting Is All You Need: Automated Android Bug Replay with Large Language Models},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3608137},
	doi = {10.1145/3597503.3608137},
	abstract = {Bug reports are vital for software maintenance that allow users to inform developers of the problems encountered while using the software. As such, researchers have committed considerable resources toward automating bug replay to expedite the process of software maintenance. Nonetheless, the success of current automated approaches is largely dictated by the characteristics and quality of bug reports, as they are constrained by the limitations of manually-crafted patterns and pre-defined vocabulary lists. Inspired by the success of Large Language Models (LLMs) in natural language understanding, we propose AdbGPT, a new lightweight approach to automatically reproduce the bugs from bug reports through prompt engineering, without any training and hard-coding effort. AdbGPT leverages few-shot learning and chain-of-thought reasoning to elicit human knowledge and logical reasoning from LLMs to accomplish the bug replay in a manner similar to a developer. Our evaluations demonstrate the effectiveness and efficiency of our AdbGPT to reproduce 81.3\% of bug reports in 253.6 seconds, outperforming the state-of-the-art baselines and ablation studies. We also conduct a small-scale user study to confirm the usefulness of AdbGPT in enhancing developers' bug replay capabilities.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {67},
	numpages = {13},
	keywords = {automated bug replay, large language model, prompt engineering},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3623343,
	author = {Deng, Yinlin and Xia, Chunqiu Steven and Yang, Chenyuan and Zhang, Shizhuo Dylan and Yang, Shujing and Zhang, Lingming},
	title = {Large Language Models are Edge-Case Generators: Crafting Unusual Programs for Fuzzing Deep Learning Libraries},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3623343},
	doi = {10.1145/3597503.3623343},
	abstract = {Bugs in Deep Learning (DL) libraries may affect almost all downstream DL applications, and it is crucial to ensure the quality of such systems. It is challenging to generate valid input programs for fuzzing DL libraries, since the input programs need to satisfy both the syntax/semantics of the supported languages (e.g., Python) and the tensor/operator constraints for constructing valid computational graphs. Recently, the TitanFuzz work demonstrates that modern Large Language Models (LLMs) can be directly leveraged to implicitly learn all the language and DL computation constraints to generate valid programs for fuzzing DL libraries (and beyond). However, LLMs tend to generate ordinary programs following similar patterns/tokens with typical programs seen in their massive pre-training corpora (e.g., GitHub), while fuzzing favors unusual inputs that cover edge cases or are unlikely to be manually produced.To fill this gap, this paper proposes FuzzGPT, the first approach to priming LLMs to synthesize unusual programs for fuzzing. FuzzGPT is mainly built on the well-known hypothesis that historical bug-triggering programs may include rare/valuable code ingredients important for bug finding. Meanwhile, while traditional techniques leveraging such historical information require intensive human efforts to both design dedicated generators and ensure the syntactic/semantic validity of generated programs, FuzzGPT demonstrates that this process can be fully automated via the intrinsic capabilities of LLMs (including fine-tuning and in-context learning), while being generalizable and applicable to challenging domains. While FuzzGPT can be applied with different LLMs, this paper focuses on the powerful GPT-style models: Codex and CodeGen. Moreover, FuzzGPT also shows the potential of directly leveraging the instruction-following capability of the recent ChatGPT for effective fuzzing. The experimental study on two popular DL libraries (PyTorch and TensorFlow) shows that FuzzGPT can substantially outperform TitanFuzz, detecting 76 bugs, with 49 already confirmed as previously unknown bugs, including 11 high-priority bugs or security vulnerabilities.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {70},
	numpages = {13},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639085,
	author = {Chen, Junkai and Hu, Xing and Li, Zhenhao and Gao, Cuiyun and Xia, Xin and Lo, David},
	title = {Code Search is All You Need? Improving Code Suggestions with Code Search},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639085},
	doi = {10.1145/3597503.3639085},
	abstract = {Modern integrated development environments (IDEs) provide various automated code suggestion techniques (e.g., code completion and code generation) to help developers improve their efficiency. Such techniques may retrieve similar code snippets from the code base or leverage deep learning models to provide code suggestions. However, how to effectively enhance the code suggestions using code retrieval has not been systematically investigated. In this paper, we study and explore a retrieval-augmented framework for code suggestions. Specifically, our framework leverages different retrieval approaches and search strategies to search similar code snippets. Then the retrieved code is used to further enhance the performance of language models on code suggestions. We conduct experiments by integrating different language models into our framework and compare the results with their original models. We find that our framework noticeably improves the performance of both code completion and code generation by up to 53.8\% and 130.8\% in terms of BLEU-4, respectively. Our study highlights that integrating the retrieval process into code suggestions can improve the performance of code suggestions by a large margin.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {73},
	numpages = {13},
	keywords = {code suggestion, code search, language model},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639120,
	author = {Sun, Zhensu and Du, Xiaoning and Song, Fu and Wang, Shangwen and Li, Li},
	title = {When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639120},
	doi = {10.1145/3597503.3639120},
	abstract = {Leveraging recent advancements in large language models, modern neural code completion models have demonstrated the capability to generate highly accurate code suggestions. However, their massive size poses challenges in terms of computational costs and environmental impact, hindering their widespread adoption in practical scenarios. Dynamic inference emerges as a promising solution, as it allocates minimal computation during inference while maintaining the model's performance. In this research, we explore dynamic inference within the context of code completion. Initially, we conducted an empirical investigation on GPT-2, focusing on the inference capabilities of intermediate layers for code completion. We found that 54.4\% of tokens can be accurately generated using just the first layer, signifying significant computational savings potential. Moreover, despite using all layers, the model still fails to predict 14.5\% of tokens correctly, and the subsequent completions continued from them are rarely considered helpful, with only a 4.2\% Acceptance Rate. These findings motivate our exploration of dynamic inference in code completion and inspire us to enhance it with a decision-making mechanism that stops the generation of incorrect code. We thus propose a novel dynamic inference method specifically tailored for code completion models. This method aims not only to produce correct predictions with largely reduced computation but also to prevent incorrect predictions proactively. Our extensive evaluation shows that it can averagely skip 1.7 layers out of 16 layers in the models, leading to an 11.2\% speedup with only a marginal 1.1\% reduction in ROUGE-L.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {75},
	numpages = {12},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639133,
	author = {Al-Kaswan, Ali and Izadi, Maliheh and van Deursen, Arie},
	title = {Traces of Memorisation in Large Language Models for Code},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639133},
	doi = {10.1145/3597503.3639133},
	abstract = {Large language models have gained significant popularity because of their ability to generate human-like text and potential applications in various fields, such as Software Engineering. Large language models for code are commonly trained on large unsanitised corpora of source code scraped from the internet. The content of these datasets is memorised and can be extracted by attackers with data extraction attacks. In this work, we explore memorisation in large language models for code and compare the rate of memorisation with large language models trained on natural language. We adopt an existing benchmark for natural language and construct a benchmark for code by identifying samples that are vulnerable to attack. We run both benchmarks against a variety of models, and perform a data extraction attack. We find that large language models for code are vulnerable to data extraction attacks, like their natural language counterparts. From the training data that was identified to be potentially extractable we were able to extract 47\% from a CodeGen-Mono-16B code completion model. We also observe that models memorise more, as their parameter count grows, and that their pre-training data are also vulnerable to attack. We also find that data carriers are memorised at a higher rate than regular code or documentation and that different model architectures memorise different samples. Data leakage has severe outcomes, so we urge the research community to further investigate the extent of this phenomenon using a wider range of models and extraction techniques in order to build safeguards to mitigate this issue.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {78},
	numpages = {12},
	keywords = {large language models, privacy, memorisation, data leakage},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639138,
	author = {Izadi, Maliheh and Katzy, Jonathan and Van Dam, Tim and Otten, Marc and Popescu, Razvan Mihai and Van Deursen, Arie},
	title = {Language Models for Code Completion: A Practical Evaluation},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639138},
	doi = {10.1145/3597503.3639138},
	abstract = {Transformer-based language models for automatic code completion have shown great promise so far, yet the evaluation of these models rarely uses real data. This study provides both quantitative and qualitative assessments of three public code language models when completing real-world code. We first developed an open-source IDE extension, Code4Me, for the online evaluation of the models. We collected real auto-completion usage data for over a year from more than 1200 users, resulting in over 600K valid completions. These models were then evaluated using six standard metrics across twelve programming languages. Next, we conducted a qualitative study of 1690 real-world completion requests to identify the reasons behind the poor model performance. A comparative analysis of the models' performance in online and offline settings was also performed, using benchmark synthetic datasets and two masking strategies.Our findings suggest that while developers utilize code completion across various languages, the best results are achieved for mainstream languages such as Python and Java. InCoder outperformed the other models across all programming languages, highlighting the significance of training data and objectives. Our study also revealed that offline evaluations do not accurately reflect real-world scenarios. Upon qualitative analysis of the models' predictions, we found that 66.3\% of failures were due to models' limitations, 24.4\% occurred due to inappropriate model usage in a development context, and 9.3\% were valid requests that developers overwrote. Given these findings, we propose several strategies to overcome the current limitations. These include refining training objectives, improving resilience to typographical errors, adopting hybrid approaches, and enhancing implementations and usability.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {79},
	numpages = {13},
	keywords = {automatic code completion, transformers, language models, IDE, evaluation, open source, InCoder, UniXcoder, CodeGPT},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639219,
	author = {Du, Xueying and Liu, Mingwei and Wang, Kaixin and Wang, Hanlin and Liu, Junwei and Chen, Yixuan and Feng, Jiayi and Sha, Chaofeng and Peng, Xin and Lou, Yiling},
	title = {Evaluating Large Language Models in Class-Level Code Generation},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639219},
	doi = {10.1145/3597503.3639219},
	abstract = {Recently, many large language models (LLMs) have been proposed, showing advanced proficiency in code generation. Meanwhile, many efforts have been dedicated to evaluating LLMs on code generation benchmarks such as HumanEval. Although being very helpful for comparing different LLMs, existing evaluation focuses on a simple code generation scenario (i.e., function-level or statement-level code generation), which mainly asks LLMs to generate one single code unit (e.g., a function or a statement) for the given natural language description. Such evaluation focuses on generating independent and often small-scale code units, thus leaving it unclear how LLMs perform in real-world software development scenarios.To fill this knowledge gap, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e., class-level code generation. Compared with existing code generation benchmarks, it better reflects real-world software development scenarios due to it comprising broader contextual dependencies and multiple, interdependent units of code. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on the new benchmark ClassEval, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we find that all LLMs perform much worse on class-level code generation compared to the method-level. While GPT models still dominate other LLMs on class-level code generation, the performance rankings of other models on method-level code generation no longer holds for class-level code generation. Besides, most models (except GPT models) perform better when generating the class method by method; and they have the limited ability of generating dependent code. Based on our findings, we call for software engineering (SE) researchers' expertise to build more LLM benchmarks based on practical and complicated software development scenarios.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {81},
	numpages = {13},
	keywords = {class-level code generation, large language model, benchmark},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639226,
	author = {Pan, Rangeet and Ibrahimzada, Ali Reza and Krishna, Rahul and Sankar, Divya and Wassi, Lambert Pouguem and Merler, Michele and Sobolev, Boris and Pavuluri, Raju and Sinha, Saurabh and Jabbarvand, Reyhaneh},
	title = {Lost in Translation: A Study of Bugs Introduced by Large Language Models while Translating Code},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639226},
	doi = {10.1145/3597503.3639226},
	abstract = {Code translation aims to convert source code from one programming language (PL) to another. Given the promising abilities of large language models (LLMs) in code synthesis, researchers are exploring their potential to automate code translation. The prerequisite for advancing the state of LLM-based code translation is to understand their promises and limitations over existing techniques. To that end, we present a large-scale empirical study to investigate the ability of general LLMs and code LLMs for code translation across pairs of different languages, including C, C++, Go, Java, and Python. Our study, which involves the translation of 1,700 code samples from three benchmarks and two real-world projects, reveals that LLMs are yet to be reliably used to automate code translation---with correct translations ranging from 2.1\% to 47.3\% for the studied LLMs. Further manual investigation of unsuccessful translations identifies 15 categories of translation bugs. We also compare LLM-based code translation with traditional non-LLM-based approaches. Our analysis shows that these two classes of techniques have their own strengths and weaknesses. Finally, insights from our study suggest that providing more context to LLMs during translation can help them produce better results. To that end, we propose a prompt-crafting approach based on the symptoms of erroneous translations; this improves the performance of LLM-based code translation by 5.5\% on average. Our study is the first of its kind, in terms of scale and breadth, that provides insights into the current limitations of LLMs in code translation and opportunities for improving them. Our dataset---consisting of 1,700 code samples in five PLs with 10K+ tests, 43K+ translated code, 1,748 manually labeled bugs, and 1,365 bug-fix pairs---can help drive research in this area.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {82},
	numpages = {13},
	keywords = {code translation, bug taxonomy, llm},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639184,
	author = {Chow, Yiu Wai and Di Grazia, Luca and Pradel, Michael},
	title = {PyTy: Repairing Static Type Errors in Python},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639184},
	doi = {10.1145/3597503.3639184},
	abstract = {Gradual typing enables developers to annotate types of their own choosing, offering a flexible middle ground between no type annotations and a fully statically typed language. As more and more code bases get type-annotated, static type checkers detect an increasingly large number of type errors. Unfortunately, fixing these errors requires manual effort, hampering the adoption of gradual typing in practice. This paper presents PyTy, an automated program repair approach targeted at statically detectable type errors in Python. The problem of repairing type errors deserves specific attention because it exposes particular repair patterns, offers a warning message with hints about where and how to apply a fix, and because gradual type checking serves as an automatic way to validate fixes. We addresses this problem through three contributions: (i) an empirical study that investigates how developers fix Python type errors, showing a diverse set of fixing strategies with some recurring patterns; (ii) an approach to automatically extract type error fixes, which enables us to create a dataset of 2,766 error-fix pairs from 176 GitHub repositories, named PyTyDefects; (iii) the first learning-based repair technique for fixing type errors in Python. Motivated by the relative data scarcity of the problem, the neural model at the core of PyTy is trained via cross-lingual transfer learning. Our evaluation shows that PyTy offers fixes for ten frequent categories of type errors, successfully addressing 85.4\% of 281 real-world errors. This effectiveness outperforms state-of-the-art large language models asked to repair type errors (by 2.1x) and complements a previous technique aimed at type errors that manifest at runtime. Finally, 20 out of 30 pull requests with PyTy-suggested fixes have been merged by developers, showing the usefulness of PyTy in practice.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {87},
	numpages = {13},
	keywords = {automatic program repair, type annotation, transfer learning},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639081,
	author = {Jiang, Yuxuan and Zhang, Chaoyun and He, Shilin and Yang, Zhihao and Ma, Minghua and Qin, Si and Kang, Yu and Dang, Yingnong and Rajmohan, Saravan and Lin, Qingwei and Zhang, Dongmei},
	title = {Xpert: Empowering Incident Management with Query Recommendations via Large Language Models},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639081},
	doi = {10.1145/3597503.3639081},
	abstract = {Large-scale cloud systems play a pivotal role in modern IT infrastructure. However, incidents occurring within these systems can lead to service disruptions and adversely affect user experience. To swiftly resolve such incidents, on-call engineers depend on crafting domain-specific language (DSL) queries to analyze telemetry data. However, writing these queries can be challenging and time-consuming. This paper presents a thorough empirical study on the utilization of queries of KQL, a DSL employed for incident management in a large-scale cloud management system at Microsoft. The findings obtained underscore the importance and viability of KQL queries recommendation to enhance incident management.Building upon these valuable insights, we introduce Xpert, an end-to-end machine learning framework that automates KQL recommendation process. By leveraging historical incident data and large language models, Xpert generates customized KQL queries tailored to new incidents. Furthermore, Xpert incorporates a novel performance metric called Xcore, enabling a thorough evaluation of query quality from three comprehensive perspectives. We conduct extensive evaluations of Xpert, demonstrating its effectiveness in offline settings. Notably, we deploy Xpert in the real production environment of a large-scale incident management system in Microsoft, validating its efficiency in supporting incident management. To the best of our knowledge, this paper represents the first empirical study of its kind, and Xpert stands as a pioneering DSL query recommendation framework designed for incident management.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {92},
	numpages = {13},
	keywords = {incident management, query generation, large language model},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639157,
	author = {Su, Yanqi and Liao, Dianshu and Xing, Zhenchang and Huang, Qing and Xie, Mulong and Lu, Qinghua and Xu, Xiwei},
	title = {Enhancing Exploratory Testing by Large Language Model and Knowledge Graph},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639157},
	doi = {10.1145/3597503.3639157},
	abstract = {Exploratory testing leverages the tester's knowledge and creativity to design test cases for effectively uncovering system-level bugs from the end user's perspective. Researchers have worked on test scenario generation to support exploratory testing based on a system knowledge graph, enriched with scenario and oracle knowledge from bug reports. Nevertheless, the adoption of this approach is hindered by difficulties in handling bug reports of inconsistent quality and varied expression styles, along with the infeasibility of the generated test scenarios. To overcome these limitations, we utilize the superior natural language understanding (NLU) capabilities of Large Language Models (LLMs) to construct a System KG of User Tasks and Failures (SysKG-UTF). Leveraging the system and bug knowledge from the KG, along with the logical reasoning capabilities of LLMs, we generate test scenarios with high feasibility and coherence. Particularly, we design chain-of-thought (CoT) reasoning to extract human-like knowledge and logical reasoning from LLMs, simulating a developer's process of validating test scenario feasibility. Our evaluation shows that our approach significantly enhances the KG construction, particularly for bug reports with low quality. Furthermore, our approach generates test scenarios with high feasibility and coherence. The user study further proves the effectiveness of our generated test scenarios in supporting exploratory testing. Specifically, 8 participants find 36 bugs from 8 seed bugs in two hours using our test scenarios, a significant improvement over the 21 bugs found by the state-of-the-art baseline.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {98},
	numpages = {12},
	keywords = {exploratory testing, knowledge graph, AI chain, prompt engineering},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639180,
	author = {Liu, Zhe and Chen, Chunyang and Wang, Junjie and Chen, Mengzhuo and Wu, Boyu and Che, Xing and Wang, Dandan and Wang, Qing},
	title = {Make LLM a Testing Expert: Bringing Human-like Interaction to Mobile GUI Testing via Functionality-aware Decisions},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639180},
	doi = {10.1145/3597503.3639180},
	abstract = {Automated Graphical User Interface (GUI) testing plays a crucial role in ensuring app quality, especially as mobile applications have become an integral part of our daily lives. Despite the growing popularity of learning-based techniques in automated GUI testing due to their ability to generate human-like interactions, they still suffer from several limitations, such as low testing coverage, inadequate generalization capabilities, and heavy reliance on training data. Inspired by the success of Large Language Models (LLMs) like ChatGPT in natural language understanding and question answering, we formulate the mobile GUI testing problem as a Q&amp;A task. We propose GPTDroid, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process. Within this framework, we have also introduced a functionality-aware memory prompting mechanism that equips the LLM with the ability to retain testing knowledge of the whole process and conduct long-term, functionality-based reasoning to guide exploration. We evaluate it on 93 apps from Google Play and demonstrate that it outperforms the best baseline by 32\% in activity coverage, and detects 31\% more bugs at a faster rate. Moreover, GPTDroid identifies 53 new bugs on Google Play, of which 35 have been confirmed and fixed.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {100},
	numpages = {13},
	keywords = {automated GUI testing, large language model},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639121,
	author = {Xia, Chunqiu Steven and Paltenghi, Matteo and Le Tian, Jia and Pradel, Michael and Zhang, Lingming},
	title = {Fuzz4All: Universal Fuzzing with Large Language Models},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639121},
	doi = {10.1145/3597503.3639121},
	abstract = {Fuzzing has achieved tremendous success in discovering bugs and vulnerabilities in various software systems. Systems under test (SUTs) that take in programming or formal language as inputs, e.g., compilers, runtime engines, constraint solvers, and software libraries with accessible APIs, are especially important as they are fundamental building blocks of software development. However, existing fuzzers for such systems often target a specific language, and thus cannot be easily applied to other languages or even other versions of the same language. Moreover, the inputs generated by existing fuzzers are often limited to specific features of the input language, and thus can hardly reveal bugs related to other or new features. This paper presents Fuzz4All, the first fuzzer that is universal in the sense that it can target many different input languages and many different features of these languages. The key idea behind Fuzz4All is to leverage large language models (LLMs) as an input generation and mutation engine, which enables the approach to produce diverse and realistic inputs for any practically relevant language. To realize this potential, we present a novel autoprompting technique, which creates LLM prompts that are well-suited for fuzzing, and a novel LLM-powered fuzzing loop, which iteratively updates the prompt to create new fuzzing inputs. We evaluate Fuzz4All on nine systems under test that take in six different languages (C, C++, Go, SMT2, Java, and Python) as inputs. The evaluation shows, across all six languages, that universal fuzzing achieves higher coverage than existing, language-specific fuzzers. Furthermore, Fuzz4All has identified 98 bugs in widely used systems, such as GCC, Clang, Z3, CVC5, OpenJDK, and the Qiskit quantum computing platform, with 64 bugs already confirmed by developers as previously unknown.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {126},
	numpages = {13},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639118,
	author = {Liu, Zhe and Chen, Chunyang and Wang, Junjie and Chen, Mengzhuo and Wu, Boyu and Tian, Zhilin and Huang, Yuekai and Hu, Jun and Wang, Qing},
	title = {Testing the Limits: Unusual Text Inputs Generation for Mobile App Crash Detection with Large Language Model},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639118},
	doi = {10.1145/3597503.3639118},
	abstract = {Mobile applications have become a ubiquitous part of our daily life, providing users with access to various services and utilities. Text input, as an important interaction channel between users and applications, plays an important role in core functionality such as search queries, authentication, messaging, etc. However, certain special text (e.g., -18 for Font Size) can cause the app to crash, and generating diversified unusual inputs for fully testing the app is highly demanded. Nevertheless, this is also challenging due to the combination of explosion dilemma, high context sensitivity, and complex constraint relations. This paper proposes InputBlaster which leverages the LLM to automatically generate unusual text inputs for mobile app crash detection. It formulates the unusual inputs generation problem as a task of producing a set of test generators, each of which can yield a batch of unusual text inputs under the same mutation rule. In detail, InputBlaster leverages LLM to produce the test generators together with the mutation rules serving as the reasoning chain, and utilizes the in-context learning schema to demonstrate the LLM with examples for boosting the performance. InputBlaster is evaluated on 36 text input widgets with cash bugs involving 31 popular Android apps, and results show that it achieves 78\% bug detection rate, with 136\% higher than the best baseline. Besides, we integrate it with the automated GUI testing tool and detect 37 unseen crashes in real-world apps.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {137},
	numpages = {12},
	keywords = {Android GUI testing, large language model, in-context learning},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639210,
	author = {Fu, Jingzhou and Liang, Jie and Wu, Zhiyong and Jiang, Yu},
	title = {Sedar: Obtaining High-Quality Seeds for DBMS Fuzzing via Cross-DBMS SQL Transfer},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639210},
	doi = {10.1145/3597503.3639210},
	abstract = {Effective DBMS fuzzing relies on high-quality initial seeds, which serve as the starting point for mutation. These initial seeds should incorporate various DBMS features to explore the state space thoroughly. While built-in test cases are typically used as initial seeds, many DBMSs lack comprehensive test cases, making it difficult to apply state-of-the-art fuzzing techniques directly.To address this, we propose Sedar which produces initial seeds for a target DBMS by transferring test cases from other DBMSs. The underlying insight is that many DBMSs share similar functionalities, allowing seeds that cover deep execution paths in one DBMS to be adapted for other DBMSs. The challenge lies in converting these seeds to a format supported by the grammar of the target database. Sedar follows a three-step process to generate seeds. First, it executes existing SQL test cases within the DBMS they were designed for and captures the schema information during execution. Second, it utilizes large language models (LLMs) along with the captured schema information to guide the generation of new test cases based on the responses from the LLM. Lastly, to ensure that the test cases can be properly parsed and mutated by fuzzers, Sedar temporarily comments out unparsable sections for the fuzzers and uncomments them after mutation. We integrate Sedar into the DBMS fuzzers Sqirrel and Griffin, targeting DBMSs such as Virtuoso, MonetDB, DuckDB, and ClickHouse. Evaluation results demonstrate significant improvements in both fuzzers. Specifically, compared to Sqirrel and Griffin with non-transferred seeds, Sedar enhances code coverage by 72.46\%-214.84\% and 21.40\%-194.46\%; compared to Sqirrel and Griffin with native test cases of these DBMSs as initial seeds, incorporating the transferred seeds of Sedar results in an improvement in code coverage by 4.90\%-16.20\% and 9.73\%-28.41\%. Moreover, Sedar discovered 70 new vulnerabilities, with 60 out of them being uniquely found by Sedar with transferred seeds, and 19 of them have been assigned with CVEs.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {146},
	numpages = {12},
	keywords = {DBMS fuzzing, initial seeds, vulnerability detection},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639116,
	author = {Nong, Yu and Fang, Richard and Yi, Guangbei and Zhao, Kunsong and Luo, Xiapu and Chen, Feng and Cai, Haipeng},
	title = {VGX: Large-Scale Sample Generation for Boosting Learning-Based Software Vulnerability Analyses},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639116},
	doi = {10.1145/3597503.3639116},
	abstract = {Accompanying the successes of learning-based defensive software vulnerability analyses is the lack of large and quality sets of labeled vulnerable program samples, which impedes further advancement of those defenses. Existing automated sample generation approaches have shown potentials yet still fall short of practical expectations due to the high noise in the generated samples. This paper proposes VGX, a new technique aimed for large-scale generation of high-quality vulnerability datasets. Given a normal program, VGX identifies the code contexts in which vulnerabilities can be injected, using a customized Transformer featured with a new value-flow-based position encoding and pre-trained against new objectives particularly for learning code structure and context. Then, VGX materializes vulnerability-injection code editing in the identified contexts using patterns of such edits obtained from both historical fixes and human knowledge about real-world vulnerabilities.Compared to four state-of-the-art (SOTA) (i.e., pattern-, Transformer-, GNN-, and pattern+Transformer-based) baselines, VGX achieved 99.09--890.06\% higher F1 and 22.45\%-328.47\% higher label accuracy. For in-the-wild sample production, VGX generated 150,392 vulnerable samples, from which we randomly chose 10\% to assess how much these samples help vulnerability detection, localization, and repair. Our results show SOTA techniques for these three application tasks achieved 19.15--330.80\% higher F1, 12.86--19.31\% higher top-10 accuracy, and 85.02--99.30\% higher top-50 accuracy, respectively, by adding those samples to their original training data. These samples also helped a SOTA vulnerability detector discover 13 more real-world vulnerabilities (CVEs) in critical systems (e.g., Linux kernel) that would be missed by the original model.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {149},
	numpages = {13},
	keywords = {vulnerability dataset, vulnerability injection, data quality, vulnerability analysis, deep learning, program generation},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639223,
	author = {Imran, Mia Mohammad and Chatterjee, Preetha and Damevski, Kostadin},
	title = {Uncovering the Causes of Emotions in Software Developer Communication Using Zero-shot LLMs},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639223},
	doi = {10.1145/3597503.3639223},
	abstract = {Understanding and identifying the causes behind developers' emotions (e.g., Frustration caused by 'delays in merging pull requests') can be crucial towards finding solutions to problems and fostering collaboration in open-source communities. Effectively identifying such information in the high volume of communications across the different project channels, such as chats, emails, and issue comments, requires automated recognition of emotions and their causes. To enable this automation, large-scale software engineering-specific datasets that can be used to train accurate machine learning models are required. However, such datasets are expensive to create with the variety and informal nature of software projects' communication channels.In this paper, we explore zero-shot LLMs that are pre-trained on massive datasets but without being fine-tuned specifically for the task of detecting emotion causes in software engineering: ChatGPT, GPT-4, and flan-alpaca. Our evaluation indicates that these recently available models can identify emotion categories when given detailed emotions, although they perform worse than the top-rated models. For emotion cause identification, our results indicate that zero-shot LLMs are effective at recognizing the correct emotion cause with a BLEU-2 score of 0.598. To highlight the potential use of these techniques, we conduct a case study of the causes of Frustration in the last year of development of a popular open-source project, revealing several interesting insights.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {182},
	numpages = {13},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639155,
	author = {Xu, Junjielong and Yang, Ruichun and Huo, Yintong and Zhang, Chengyu and He, Pinjia},
	title = {DivLog: Log Parsing with Prompt Enhanced In-Context Learning},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639155},
	doi = {10.1145/3597503.3639155},
	abstract = {Log parsing, which involves log template extraction from semi-structured logs to produce structured logs, is the first and the most critical step in automated log analysis. However, current log parsers suffer from limited effectiveness for two reasons. First, traditional data-driven log parsers solely rely on heuristics or handcrafted features designed by domain experts, which may not consistently perform well on logs from diverse systems. Second, existing supervised log parsers require model tuning, which is often limited to fixed training samples and causes sub-optimal performance across the entire log source. To address this limitation, we propose DivLog, an effective log parsing framework based on the in-context learning (ICL) ability of large language models (LLMs). Specifically, before log parsing, DivLog samples a small amount of offline logs as candidates by maximizing their diversity. Then, during log parsing, DivLog selects five appropriate labeled candidates as examples for each target log and constructs them into a prompt. By mining the semantics of examples in the prompt, DivLog generates a target log template in a training-free manner. In addition, we design a straightforward yet effective prompt format to extract the output and enhance the quality of the generated log templates. We conducted experiments on 16 widely-used public datasets. The results show that DivLog achieves (1) 98.1\% Parsing Accuracy, (2) 92.1\% Precision Template Accuracy, and (3) 92.9\% Recall Template Accuracy on average, exhibiting state-of-the-art performance.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {199},
	numpages = {12},
	keywords = {log parsing, large language model, in-context learning},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639183,
	author = {Ahmed, Toufique and Pai, Kunal Suresh and Devanbu, Premkumar and Barr, Earl},
	title = {Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639183},
	doi = {10.1145/3597503.3639183},
	abstract = {Large Language Models (LLM) are a new class of computation engines, "programmed" via prompt engineering. Researchers are still learning how to best "program" these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously collect semantics facts, from the code, while working. Mostly these are shallow, simple facts arising from a quick read. For a function, such facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc.One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them implicitly capable of doing this simple level of "code analysis" and extracting such information, while processing code: but are they, really? If they aren't, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether automatically augmenting an LLM's prompt with semantic facts explicitly, actually helps.Prior work shows that LLM performance on code summarization benefits from embedding a few code \&amp; summary exemplars in the prompt, before the code to be summarized. While summarization performance has steadily progressed since the early days, there is still room for improvement: LLM performance on code summarization still lags its performance on natural-language tasks like translation and text summarization.We find that adding semantic facts to the code in the prompt actually does help! This approach improves performance in several different settings suggested by prior work, including for three different Large Language Models. In most cases, we see improvements, as measured by a range of commonly-used metrics; for the PHP language in the challenging CodeSearchNet dataset, this augmentation actually yields performance surpassing 30 BLEU1. In addition, we have also found that including semantic facts yields a substantial enhancement in LLMs' line completion performance.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {220},
	numpages = {13},
	keywords = {LLM, code summarization, program analysis, prompt engineering},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639135,
	author = {Yang, Yanming and Hu, Xing and Xia, Xin and Lo, David and Yang, Xiaohu},
	title = {Streamlining Java Programming: Uncovering Well-Formed Idioms with IdioMine},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639135},
	doi = {10.1145/3597503.3639135},
	abstract = {Code idioms are commonly used patterns, techniques, or practices that aid in solving particular problems or specific tasks across multiple software projects. They can improve code quality, performance, and maintainability, and also promote program standardization and reuse across projects. However, identifying code idioms is significantly challenging, as existing studies have still suffered from three main limitations. First, it is difficult to recognize idioms that span non-contiguous code lines. Second, identifying idioms with intricate data flow and code structures can be challenging. Moreover, they only extract dataset-specific idioms, so common idioms or well-established code/design patterns that are rarely found in datasets cannot be identified.To overcome these limitations, we propose a novel approach, named IdioMine, to automatically extract generic and specific idioms from both Java projects and libraries. We perform program analysis on Java functions to transform them into concise PDGs, for integrating the data flow and control flow of code fragments. We then develop a novel chain structure, Data-driven Control Chain (DCC), to extract sub-idioms that possess contiguous semantic meanings from PDGs. After that, we utilize GraphCodeBERT to generate code embeddings of these sub-idioms and perform density-based clustering to obtain frequent sub-idioms. We use heuristic rules to identify interrelated sub-idioms among the frequent ones. Finally, we employ ChatGPT to synthesize interrelated sub-idioms into potential code idioms and infer real idioms from them.We conduct well-designed experiments and a user study to evaluate IdioMine's correctness and the practical value of the extracted idioms. Our experimental results show that IdioMine effectively extracts more idioms with better performance in most metrics. We compare our approach with Haggis and ChatGPT, IdioMine outperforms them by 22.8\% and 35.5\% in Idiom Set Precision (ISP) and by 9.7\% and 22.9\% in Idiom Coverage (IC) when extracting idioms from libraries. IdioMine also extracts almost twice the size of idioms than the baselines, exhibiting its ability to identify complete idioms. Our user study indicates that idioms extracted by IdioMine are well-formed and semantically clear. Moreover, we conduct a qualitative and quantitative analysis to investigate the primary functionalities of IdioMine's extracted idioms from various projects and libraries.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {230},
	numpages = {12},
	keywords = {code idiom mining, code pattern, large language model (LLM), clustering},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3663529.3663829,
	author = {Toslali, Mert and Snible, Edward and Chen, Jing and Cha, Alan and Singh, Sandeep and Kalantar, Michael and Parthasarathy, Srinivasan},
	title = {AgraBOT: Accelerating Third-Party Security Risk Management in Enterprise Setting through Generative AI},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3663829},
	doi = {10.1145/3663529.3663829},
	abstract = {In the contemporary business landscape, organizations often rely on third-party services for many functions, including IT services, cloud computing, and business processes. To identify potential security risks, organizations conduct rigorous assessments before engaging with third-party vendors, referred to as Third-Party Security Risk Management (TPSRM). Traditionally, TPSRM assessments are executed manually by human experts and involve scrutinizing various third-party documents such as System and Organization Controls Type 2 (SOC 2) reports and reviewing comprehensive questionnaires along with the security policy and procedures of vendors—a process that is time-intensive and inherently lacks scalability. 
 
 
 
AgraBOT, a Retrieval Augmented Generation (RAG) framework, can assist TPSRM assessors by expediting TPSRM assessments and reducing the time required from days to mere minutes. AgraBOT utilizes cutting-edge AI techniques, including information retrieval (IR), large language models (LLMs), multi-stage ranking, prompt engineering, and in-context learning to accurately generate relevant answers from third-party documents to conduct assessments. We evaluate AgraBOT on seven real TPSRM assessments, consisting of 373 question-answer pairs, and attain an F1 score of 0.85.},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {74–79},
	numpages = {6},
	keywords = {AI, Document Understanding, LLM, RAG, TPSRM},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@inproceedings{10.1145/3663529.3663836,
	author = {Dunay, Omer and Cheng, Daniel and Tait, Adam and Thakkar, Parth and Rigby, Peter C. and Chiu, Andy and Ahmad, Imad and Ganesan, Arun and Maddila, Chandra and Murali, Vijayaraghavan and Tayyebi, Ali and Nagappan, Nachiappan},
	title = {Multi-line AI-Assisted Code Authoring},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3663836},
	doi = {10.1145/3663529.3663836},
	abstract = {CodeCompose is an AI-assisted code authoring tool powered by large language models (LLMs) that provides inline suggestions all developers at Meta. In this paper, we present how we scaled the product from displaying single-line suggestions to multi-line suggestions. This evolution required us to overcome several unique challenges in improving the usability of these suggestions for developers.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
First, we discuss how multi-line suggestions can have a "jarring" effect, as the LLM’s suggestions constantly move around the developer’s existing code, which would otherwise result in decreased productivity and satisfaction.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Second, multi-line suggestions take significantly longer to generate; hence we present several innovative investments we made to reduce the perceived latency for users. These model-hosting optimizations sped up multi-line suggestion latency by 2.5x.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Finally, we conduct experiments on 10’s of thousands of engineers to understand how multi-line suggestions impact the user experience and contrast this with single-line suggestions. Our experiments reveal that (i) multi-line suggestions account for 42\% of total characters accepted (despite only accounting for 16\% for dis- played suggestions) (ii) multi-line suggestions almost doubled the percentage of keystrokes saved for users from 9\% to 17\%. Multi-line CodeCompose has been rolled out to all engineers at Meta, and less than 1\% of engineers have opted out of multi-line suggestions.},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {150–160},
	numpages = {11},
	keywords = {AI, Developer productivity, LLM code authoring, Neural code completion, Program synthesis, Responsiveness, User experience},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@inproceedings{10.1145/3663529.3663839,
	author = {Alshahwan, Nadia and Chheda, Jubin and Finogenova, Anastasia and Gokkaya, Beliz and Harman, Mark and Harper, Inna and Marginean, Alexandru and Sengupta, Shubho and Wang, Eddy},
	title = {Automated Unit Test Improvement using Large Language Models at Meta},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3663839},
	doi = {10.1145/3663529.3663839},
	abstract = {This paper describes Meta’s TestGen-LLM tool, which uses LLMs to automatically improve existing human-written tests.     TestGen-LLM verifies that its generated test classes successfully clear a set of filters that assure measurable improvement over the original test suite, thereby eliminating problems due to LLM hallucination.    We describe the deployment of TestGen-LLM at Meta test-a-thons for the Instagram and Facebook platforms.     In an evaluation on Reels and Stories products for Instagram,     75\% of TestGen-LLM’s test cases built correctly, 57\% passed reliably, and 25\% increased coverage.    During Meta’s Instagram and Facebook test-a-thons, it improved 11.5\% of all classes to which it was applied, with 73\% of its recommendations being accepted for production deployment by Meta software engineers.    We believe this is the first report on industrial scale deployment of LLM-generated code backed by such assurances of code improvement.},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {185–196},
	numpages = {12},
	keywords = {Automated Test Generation, Genetic Improvement, LLMs, Large Language Models, Unit Testing},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@inproceedings{10.1145/3663529.3663841,
	author = {Roy, Devjeet and Zhang, Xuchao and Bhave, Rashi and Bansal, Chetan and Las-Casas, Pedro and Fonseca, Rodrigo and Rajmohan, Saravan},
	title = {Exploring LLM-Based Agents for Root Cause Analysis},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3663841},
	doi = {10.1145/3663529.3663841},
	abstract = {The growing complexity of cloud based software systems has resulted in incident management becoming an integral part of the software development lifecycle. Root cause analysis (RCA), a critical part of the incident management process, is a demanding task for on-call engineers, requiring deep domain knowledge and extensive experience with a team’s specific services. Automation of RCA can result in significant savings of time, and ease the burden of incident management on on-call engineers. Recently, researchers have utilized Large Language Models (LLMs) to perform RCA, and have demonstrated promising results. However, these approaches are not able to dynamically collect additional diagnostic information such as incident related logs, metrics or databases, severely restricting their ability to diagnose root causes. In this work, we explore the use of LLM based agents for RCA to address this limitation. We present a thorough empirical evaluation of a ReAct agent equipped with retrieval tools, on an out-of-distribution dataset of production incidents collected at a large IT corporation. Results show that ReAct performs competitively with strong retrieval and reasoning baselines, but with highly increased factual accuracy. We then extend this evaluation by incorporating discussions associated with incident reports as additional inputs for the models, which surprisingly does not yield significant performance improvements. Lastly, we conduct a case study with a team at Microsoft to equip the ReAct agent with tools that give it access to external diagnostic services that are used by the team for manual RCA. Our results show how agents can overcome the limitations of prior work, and practical considerations for implementing such a system in practice.},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {208–219},
	numpages = {12},
	keywords = {AIOps, Cloud Computing, Incident Management, Root Cause Analysis},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@inproceedings{10.1145/3663529.3663842,
	author = {Wu, Shengnan and Hu, Yongxiang and Wang, Yingchuan and Gu, Jiazhen and Meng, Jin and Fan, Liujie and Luan, Zhongshi and Wang, Xin and Zhou, Yangfan},
	title = {Combating Missed Recalls in E-commerce Search: A CoT-Prompting Testing Approach},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3663842},
	doi = {10.1145/3663529.3663842},
	abstract = {Search components in e-commerce apps, often complex AI-based systems, are prone to bugs that can lead to missed recalls—situations where items that should be listed in search results aren't. This can frustrate shop owners and harm the app's profitability. However, testing for missed recalls is challenging due to difficulties in generating user-aligned test cases and the absence of oracles. In this paper, we introduce mrDetector, the first automatic testing approach specifically for missed recalls. To tackle the test case generation challenge, we use findings from how users construct queries during searching to create a CoT prompt to generate user-aligned queries by LLM. In addition, we learn from users who create multiple queries for one shop and compare search results, and provide a test oracle through a metamorphic relation. Extensive experiments using open access data demonstrate that mrDetector outperforms all baselines with the lowest false positive ratio. Experiments with real industrial data show that mrDetector discovers over one hundred missed recalls with only 17 false positives.},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {220–231},
	numpages = {12},
	keywords = {LLM, Metamorphic Testing, Search Components},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@inproceedings{10.1145/3663529.3663846,
	author = {Zhang, Xuchao and Ghosh, Supriyo and Bansal, Chetan and Wang, Rujia and Ma, Minghua and Kang, Yu and Rajmohan, Saravan},
	title = {Automated Root Causing of Cloud Incidents using In-Context Learning with GPT-4},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3663846},
	doi = {10.1145/3663529.3663846},
	abstract = {Root Cause Analysis (RCA) plays a pivotal role in the incident diagnosis process for cloud services, requiring on-call engineers to identify the primary issues and implement corrective actions to prevent future recurrences. Improving the incident RCA process is vital for minimizing service downtime, customer impact and manual toil. Recent advances in artificial intelligence have introduced state-of-the-art Large Language Models (LLMs) like GPT-4, which have proven effective in tackling various AIOps problems, ranging from code authoring to incident management. Nonetheless, the GPT-4 model’s immense size presents challenges when trying to fine-tune it on user data because of the significant GPU resource demand and the necessity for continuous model fine-tuning with the emergence of new data. To address the high cost of fine-tuning LLM, we propose an in-context learning approach for automated root causing, which eliminates the need for fine-tuning. We conduct extensive study over 100,000 production incidents from Microsoft, comparing several large language models using multiple metrics. The results reveal that our in-context learning approach outperforms the previous fine-tuned large language models such as GPT-3 by an average of 24.8\% across all metrics, with an impressive 49.7\% improvement over the zero-shot model. Moreover, human evaluation involving actual incident owners demonstrates its superiority over the fine-tuned model, achieving a 43.5\% improvement in correctness and an 8.7\% enhancement in readability. The impressive results demonstrate the viability of utilizing a vanilla GPT model for the RCA task, thereby avoiding the high computational and maintenance costs associated with a fine-tuned model.},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {266–277},
	numpages = {12},
	keywords = {In-context Learning, Incident Diagnosis, Large Language Model, Root Cause Analysis},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@inproceedings{10.1145/3663529.3663855,
	author = {Sarda, Komal and Namrud, Zakeya and Litoiu, Marin and Shwartz, Larisa and Watts, Ian},
	title = {Leveraging Large Language Models for the Auto-remediation of Microservice Applications: An Experimental Study},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3663855},
	doi = {10.1145/3663529.3663855},
	abstract = {Runtime auto-remediation is crucial for ensuring the reliability and efficiency of distributed systems, especially within complex microservice-based applications. However, the complexity of modern microservice deployments often surpasses the capabilities of traditional manual remediation and existing autonomic computing methods. Our proposed solution harnesses large language models (LLMs) to generate and execute Ansible playbooks automatically to address issues within these complex environments. Ansible playbooks, a widely adopted markup language for IT task automation, facilitate critical actions such as addressing network failures, resource constraints, configuration errors, and application bugs prevalent in managing microservices. We apply in-context learning on pre-trained LLMs using our custom-made Ansible-based remediation dataset, equipping these models to comprehend diverse remediation tasks within microservice environments. Then, these tuned LLMs efficiently generate precise Ansible scripts tailored to specific issues encountered, surpassing current state-of-the-art techniques with high functional correctness (95.45\%) and average correctness (98.86\%).},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {358–369},
	numpages = {12},
	keywords = {Ansible, Auto-remediation, Autonomic computing, Cloud native applications, Code generation, Kubernetes, Large language models, Microservices, Prompt engineering, Real-time faults, Self-adaptive software},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@inproceedings{10.1145/3663529.3663861,
	author = {Goel, Drishti and Husain, Fiza and Singh, Aditya and Ghosh, Supriyo and Parayil, Anjaly and Bansal, Chetan and Zhang, Xuchao and Rajmohan, Saravan},
	title = {X-Lifecycle Learning for Cloud Incident Management using LLMs},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3663861},
	doi = {10.1145/3663529.3663861},
	abstract = {Incident management for large cloud services is a complex and tedious process that requires a significant amount of manual effort from on-call engineers (OCEs). OCEs typically leverage data from different stages of the software development lifecycle [SDLC] (e.g., codes, configuration, monitor data, service properties, service dependencies, trouble-shooting documents, etc.) to generate insights for detection, root cause analysis and mitigation of incidents. Recent advancements in large language models [LLMs] (e.g., ChatGPT, GPT-4, Gemini) have created opportunities to automatically generate contextual recommendations for the OCEs, assisting them in quickly identifying and mitigating critical issues. However, existing research typically takes a silo-ed view of solving a certain task in incident management by leveraging data from a single stage of the SDLC. In this paper, we demonstrate that augmenting additional contextual data from different stages of the SDLC improves the performance of two critically important and practically challenging tasks: (1) automatically generating root cause recommendations for dependency failure related incidents, and (2) identifying the ontology of service monitors used for automatically detecting incidents. By leveraging a dataset of 353 incidents and 260 monitors from Microsoft, we demonstrate that augmenting contextual information from different stages of the SDLC improves the performance over state-of-the-art methods.},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {417–428},
	numpages = {12},
	keywords = {Cloud Services, Large language models, Monitor management, Reliability, Root-cause analysis},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@inproceedings{10.1145/3663529.3663801,
	author = {Chen, Yinghao and Hu, Zehao and Zhi, Chen and Han, Junxiao and Deng, Shuiguang and Yin, Jianwei},
	title = {ChatUniTest: A Framework for LLM-Based Test Generation},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3663801},
	doi = {10.1145/3663529.3663801},
	abstract = {Unit testing is an essential yet frequently arduous task. Various automated unit test generation tools have been introduced to mitigate this challenge. Notably, methods based on large language models (LLMs) have garnered considerable attention and exhibited promising results in recent years. Nevertheless, LLM-based tools encounter limitations in generating accurate unit tests. This paper presents ChatUniTest, an LLM-based automated unit test generation framework. ChatUniTest incorporates an adaptive focal context mechanism to encompass valuable context in prompts and adheres to a generation-validation-repair mechanism to rectify errors in generated unit tests.
 
Subsequently, we have developed ChatUniTest Core, a common library that implements core workflow, complemented by the ChatUniTest Toolchain, a suite of seamlessly integrated tools enhancing the capabilities of ChatUniTest. Our effectiveness evaluation reveals that ChatUniTest outperforms TestSpark and EvoSuite in half of the evaluated projects, achieving the highest overall line coverage.
 
Furthermore, insights from our user study affirm that ChatUniTest delivers substantial value to various stakeholders in the software testing domain.
 
ChatUniTest is available at https://github.com/ZJU-ACES-ISE/ChatUniTest, and the demo video is available at https://www.youtube.com/watch?v=GmfxQUqm2ZQ.},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {572–576},
	numpages = {5},
	keywords = {Automatic Unit Testing Generation, Large Language Models},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@inproceedings{10.1145/3663529.3663868,
	author = {Liu, Haoran and Jia, Zhouyang and Zhou, Huiping and Zhou, Haifang and Li, Shanshan},
	title = {Go the Extra Mile: Fixing Propagated Error-Handling Bugs},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3663868},
	doi = {10.1145/3663529.3663868},
	abstract = {Error handling bugs are widespread in software, compromising its reliability. In C/C++ environments, error-handling bugs are often propagated to multiple functions through return values.
 
 
 
This paper introduces EH-Fixer, a conversation-based automated method for fixing propagated error-handling (PEH) bugs. EH-Fixer employs LLM in a conversation style, utilizing information retrieval to address PEH bugs. We constructed a dataset containing 30 PEH bugs and evaluated EH-Fixer against two state-of-the-art approaches. Preliminary results indicate that EH-Fixer successfully fixed 19 more PEH bugs than existing approaches.},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {661–662},
	numpages = {2},
	keywords = {Automatic Program Repair, Error-Handling Bug},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@inproceedings{10.1145/3663529.3663869,
	author = {Chavan, Sagar Bhikan and Mondal, Shouvick},
	title = {Do Large Language Models Recognize Python Identifier Swaps in Their Generated Code?},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3663869},
	doi = {10.1145/3663529.3663869},
	abstract = {Large Language Models (LLMs) have transformed natural language processing and generation activities in recent years. However, as the scale and complexity of these models grow, their ability to write correct and secure code has come under scrutiny. In our research, we delve into the critical examination of LLMs including ChatGPT-3.5, legacy Bard, and Gemini Pro, and their proficiency in generating accurate and secure code, particularly focusing on the occurrence of identifier swaps within the code they produce. Our methodology encompasses the creation of a diverse dataset comprising a range of coding tasks designed to challenge the code generation capabilities of these models. Further, we employ Pylint for an extensive code quality assessment and undertake a manual multi-turn prompted “Python identifier-swap” test session to evaluate the models’ ability to maintain context and coherence over sequential coding prompts. Our preliminary findings indicate a concern for developers: LLMs capable of generating better quality codes can perform worse when queried to recognize identifier swaps.},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {663–664},
	numpages = {2},
	keywords = {Gemini Pro, LLMs, Python Identifier Swap},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@inproceedings{10.1145/3663529.3663873,
	author = {Patel, Hetvi and Shah, Kevin Amit and Mondal, Shouvick},
	title = {Do Large Language Models Generate Similar Codes from Mutated Prompts? A Case Study of Gemini Pro},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3663873},
	doi = {10.1145/3663529.3663873},
	abstract = {In this work, we delve into the domain of source code similarity detection using Large Language Models (LLMs). Our investigation is motivated by the necessity to identify similarities among different pieces of source code, a critical aspect for tasks such as plagiarism detection and code reuse. We specifically focus on exploring the effectiveness of leveraging LLMs for this purpose. To achieve this, we utilized the LLMSecEval dataset, comprising 150 NL prompts for code generation across two languages: C and Python, and employed radamsa, a mutation-based input generator, to create 26 different mutations per NL prompt. Next, using the Gemini Pro LLM, we generated code for the original and mutated NL prompts. Finally, we detect code similarities using the recently proposed CodeBERTScore metric that utilizes the CodeBERT LLM. Our experiment aims to uncover the extent to which LLMs can consistently generate similar code despite mutations in the input NL prompts, providing insights into the robustness and generalizability of LLMs in understanding and comparing code syntax and semantics.},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {671–672},
	numpages = {2},
	keywords = {Gemini Pro, LLMs, NL Prompt Mutation, Source Code Similarity},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@inproceedings{10.1145/3663529.3664463,
	author = {Souza, D\'{e}bora},
	title = {Comparing Gemini Pro and GPT-3.5 in Algorithmic Problems},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3664463},
	doi = {10.1145/3663529.3664463},
	abstract = {The GPT-3.5 and Gemini Pro models can help generating code based on the natural language prompts they receive. However, it’s not certain the strengths and weaknesses of each model. We compare them with 100 programming problems across various difficulty levels, GPT-3.5 outperforms Gemini Pro by 30\%, highlighting their utility for programmers despite neither achieving 100\% accuracy.},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {698–700},
	numpages = {3},
	keywords = {Algorithmic Problems, GPT-3.5, Gemini Pro, LLMs},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@article{10329992,
	author = {Schafer, Max and Nadi, Sarah and Eghbali, Aryaz and Tip, Frank},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation }},
	year = {2024},
	volume = {50},
	number = {01},
	ISSN = {1939-3520},
	pages = {85-105},
	abstract = { Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. Large Language Models (LLMs) have recently been applied to various aspects of software development, including their suggested use for automated generation of unit tests, but while requiring additional training or few-shot learning on examples of existing tests. This paper presents a large-scale empirical evaluation on the effectiveness of LLMs for automated unit test generation without requiring additional training or manual effort. Concretely, we consider an approach where the LLM is provided with prompts that include the signature and implementation of a function under test, along with usage examples extracted from documentation. Furthermore, if a generated test fails, our approach attempts to generate a new test that fixes the problem by re-prompting the model with the failing test and error message. We implement our approach in TestPilot, an adaptive LLM-based test generation tool for JavaScript that automatically generates unit tests for the methods in a given project's API. We evaluate TestPilot using OpenAI's gpt3.5-turbo LLM on 25 npm packages with a total of 1,684 API functions. The generated tests achieve a median statement coverage of 70.2% and branch coverage of 52.8%. In contrast, the state-of-the feedback-directed JavaScript test generation technique, Nessie, achieves only 51.3% statement coverage and 25.6% branch coverage. Furthermore, experiments with excluding parts of the information included in the prompts show that all components contribute towards the generation of effective test suites. We also find that 92.8% of TestPilot's generated tests have $\leq$ 50% similarity with existing tests (as measured by normalized edit distance), with none of them being exact copies. Finally, we run TestPilot with two additional LLMs, OpenAI's older code-cushman-002 LLM and StarCoder, an LLM for which the training process is publicly documented. Overall, we observed similar results with the former (68.2% median statement coverage), and somewhat worse results with the latter (54.0% median statement coverage), suggesting that the effectiveness of the approach is influenced by the size and training set of the LLM, but does not fundamentally depend on the specific model. },
	keywords = {Training;Test pattern generators;Documentation;Codes;Source coding;Software;Electronic mail},
	doi = {10.1109/TSE.2023.3334955},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3334955},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {jan}
}


@article{10378848,
	author = {Tufano, Rosalia and Dabic, Ozren and Mastropaolo, Antonio and Ciniselli, Matteo and Bavota, Gabriele},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ Code Review Automation: Strengths and Weaknesses of the State of the Art }},
	year = {2024},
	volume = {50},
	number = {02},
	ISSN = {1939-3520},
	pages = {338-353},
	abstract = { The automation of code review has been tackled by several researchers with the goal of reducing its cost. The adoption of deep learning in software engineering pushed the automation to new boundaries, with techniques imitating developers in generative tasks, such as commenting on a code change as a reviewer would do or addressing a reviewer's comment by modifying code. The performance of these techniques is usually assessed through quantitative metrics, e.g., the percentage of instances in the test set for which correct predictions are generated, leaving many open questions on the techniques’ capabilities. For example, knowing that an approach is able to correctly address a reviewer's comment in 10% of cases is of little value without knowing what was asked by the reviewer: What if in all successful cases the code change required to address the comment was just the removal of an empty line? In this paper we aim at characterizing the cases in which three code review automation techniques tend to succeed or fail in the two above-described tasks. The study has a strong qualitative focus, with $\sim$∼105 man-hours of manual inspection invested in manually analyzing correct and wrong predictions generated by the three techniques, for a total of 2,291 inspected predictions. The output of this analysis are two taxonomies reporting, for each of the two tasks, the types of code changes on which the experimented techniques tend to succeed or to fail, pointing to areas for future work. A result of our manual analysis was also the identification of several issues in the datasets used to train and test the experimented techniques. Finally, we assess the importance of researching in techniques specialized for code review automation by comparing their performance with ChatGPT, a general purpose large language model, finding that ChatGPT struggles in commenting code as a human reviewer would do. },
	keywords = {Codes;Task analysis;Automation;Software;Java;Transformers;Costs},
	doi = {10.1109/TSE.2023.3348172},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3348172},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {feb}
}


@article{10433002,
	author = {Zhang, Yuxia and Qiu, Zhiqing and Stol, Klaas-Jan and Zhu, Wenhui and Zhu, Jiaxin and Tian, Yingchen and Liu, Hui},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ Automatic Commit Message Generation: A Critical Review and Directions for Future Work }},
	year = {2024},
	volume = {50},
	number = {04},
	ISSN = {1939-3520},
	pages = {816-835},
	abstract = { Commit messages are critical for code comprehension and software maintenance. Writing a high-quality message requires skill and effort. To support developers and reduce their effort on this task, several approaches have been proposed to automatically generate commit messages. Despite the promising performance reported, we have identified three significant and prevalent threats in these automated approaches: 1) the datasets used to train and evaluate these approaches contain a considerable amount of ‘noise’; 2) current approaches only consider commits of a limited diff size; and 3) current approaches can only generate the subject of a commit message, not the message body. The first limitation may let the models ‘learn’ inappropriate messages in the training stage, and also lead to inflated performance results in their evaluation. The other two threats can considerably weaken the practical usability of these approaches. Further, with the rapid emergence of large language models (LLMs) that show superior performance in many software engineering tasks, it is worth asking: can LLMs address the challenge of long diffs and whole message generation? This article first reports the results of an empirical study to assess the impact of these three threats on the performance of the state-of-the-art auto generators of commit messages. We collected commit data of the Top 1,000 most-starred Java projects in GitHub and systematically removed noisy commits with bot-submitted and meaningless messages. We then compared the performance of four approaches representative of the state-of-the-art before and after the removal of noisy messages, or with different lengths of commit diffs. We also conducted a qualitative survey with developers to investigate their perspectives on simply generating message subjects. Finally, we evaluate the performance of two representative LLMs, namely UniXcoder and ChatGPT, in generating more practical commit messages. The results demonstrate that generating commit messages is of great practical value, considerable work is needed to mature the current state-of-the-art, and LLMs can be an avenue worth trying to address the current limitations. Our analyses provide insights for future work to achieve better performance in practice. },
	keywords = {Codes;Chatbots;Task analysis;Noise measurement;Machine translation;Information retrieval;Software maintenance},
	doi = {10.1109/TSE.2024.3364675},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3364675},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {apr}
}


@article{10485640,
	author = {Tang, Yutian and Liu, Zhijie and Zhou, Zhichao and Luo, Xiapu},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ ChatGPT vs SBST: A Comparative Assessment of Unit Test Suite Generation }},
	year = {2024},
	volume = {50},
	number = {06},
	ISSN = {1939-3520},
	pages = {1340-1359},
	abstract = { Recent advancements in large language models (LLMs) have demonstrated exceptional success in a wide range of general domain tasks, such as question answering and following instructions. Moreover, LLMs have shown potential in various software engineering applications. In this study, we present a systematic comparison of test suites generated by the ChatGPT LLM and the state-of-the-art SBST tool EvoSuite. Our comparison is based on several critical factors, including correctness, readability, code coverage, and bug detection capability. By highlighting the strengths and weaknesses of LLMs (specifically ChatGPT) in generating unit test cases compared to EvoSuite, this work provides valuable insights into the performance of LLMs in solving software engineering problems. Overall, our findings underscore the potential of LLMs in software engineering and pave the way for further research in this area. },
	keywords = {Chatbots;Codes;Task analysis;Software;Question answering (information retrieval);Computer bugs;Benchmark testing},
	doi = {10.1109/TSE.2024.3382365},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3382365},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {jun}
}


@article{10507163,
	author = {Liu, Zhijie and Tang, Yutian and Luo, Xiapu and Zhou, Yuming and Zhang, Liang Feng},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ No Need to Lift a Finger Anymore? Assessing the Quality of Code Generation by ChatGPT }},
	year = {2024},
	volume = {50},
	number = {06},
	ISSN = {1939-3520},
	pages = {1548-1584},
	abstract = { Large language models (LLMs) have demonstrated impressive capabilities across various natural language processing (NLP) tasks, such as machine translation, question answering, summarization, and so on. Additionally, LLMs are also highly valuable in supporting software engineering tasks, particularly in the field of code generation. Automatic code generation is a process of automatically generating source code or executable code based on given specifications or requirements, improving developer productivity. In this study, we perform a systematic empirical assessment to the quality of code generation using ChatGPT, a recent state-of-the-art product LLM. We leverage 728 algorithm problems in five languages (i.e., C, C++, Java, Python, and JavaScript) and 18 CWEs with 54 code scenarios for the code generation task. Our evaluation encompasses a comprehensive analysis of code snippets generated by ChatGPT, focusing on three critical aspects: correctness, complexity, and security. We also specifically investigate ChatGPT's ability to engage in multi-round fixing process (i.e., ChatGPT's dialog ability, chatting between users and ChatGPT for fixing generated buggy code) of facilitating code generation. By delving into the generated code and examining the experimental results, this work provides valuable insights into the performance of ChatGPT in tackling code generation tasks over the three critical aspects. The experimental results demonstrate that (1) ChatGPT is better at generating functionally correct code for problems before 2021 in different languages than problems after 2021 with $48.14\%$48.14% advantage in Accepted rate on judgment platform, but ChatGPT's ability to directly fix erroneous code with multi-round fixing process to achieve correct functionality is relatively weak; (2) the distribution of cyclomatic and cognitive complexity levels for code snippets in different languages varies. Furthermore, the multi-round fixing process with ChatGPT generally preserves or increases the complexity levels of code snippets; (3) in algorithm scenarios with languages of C, C++, and Java, and CWE scenarios with languages of C and Python3, the code generated by ChatGPT has relevant vulnerabilities. However, the multi-round fixing process for vulnerable code snippets demonstrates promising results, with more than $89\%$89% of vulnerabilities successfully addressed; and (4) code generation may be affected by ChatGPT's non-determinism factor, resulting in variations of code snippets in functional correctness, complexity, and security. Overall, our findings uncover potential issues and limitations that arise in the ChatGPT-based code generation and lay the groundwork for improving AI and LLM-based code generation techniques. },
	keywords = {Codes;Chatbots;Task analysis;Complexity theory;Security;Transformers;Electronic mail},
	doi = {10.1109/TSE.2024.3392499},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3392499},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {jun}
}


@article{10482873,
	author = {Liu, Changshu and Cetin, Pelin and Patodia, Yogesh and Ray, Baishakhi and Chakraborty, Saikat and Ding, Yangruibo},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ Automated Code Editing With Search-Generate-Modify }},
	year = {2024},
	volume = {50},
	number = {07},
	ISSN = {1939-3520},
	pages = {1675-1686},
	abstract = { Code editing is essential in evolving software development. In literature, several automated code editing tools are proposed, which leverage Information Retrieval-based techniques and Machine Learning-based code generation and code editing models. Each technique comes with its own promises and perils, and for this reason, they are often used together to complement their strengths and compensate for their weaknesses. This paper proposes a hybrid approach to better synthesize code edits by leveraging the power of code search, generation, and modification. Our key observation is that a patch that is obtained by search & retrieval, even if incorrect, can provide helpful guidance to a code generation model. However, a retrieval-guided patch produced by a code generation model can still be a few tokens off from the intended patch. Such generated patches can be slightly modified to create the intended patches. We developed a novel tool to solve this challenge: SarGaM, which is designed to follow a real developer's code editing behavior. Given an original code version, the developer may search for the related patches, generate or write the code, and then modify the generated code to adapt it to the right context. Our evaluation of SarGaM on edit generation shows superior performance w.r.t. the current state-of-the-art techniques. SarGaM also shows its effectiveness on automated program repair tasks. },
	keywords = {Codes;Transformers;Decoding;Task analysis;Maintenance engineering;Context modeling;Training},
	doi = {10.1109/TSE.2024.3376387},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3376387},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {jul}
}


@article{10521881,
	author = {Tu, Haoxin and Zhou, Zhide and Jiang, He and Yusuf, Imam Nur Bani and Li, Yuxian and Jiang, Lingxiao},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ Isolating Compiler Bugs by Generating Effective Witness Programs With Large Language Models }},
	year = {2024},
	volume = {50},
	number = {07},
	ISSN = {1939-3520},
	pages = {1768-1788},
	abstract = { Compiler bugs pose a significant threat to safety-critical applications, and promptly as well as effectively isolating these bugs is crucial for assuring the quality of compilers. However, the limited availability of debugging information on reported bugs complicates the compiler bug isolation task. Existing compiler bug isolation approaches convert the problem into a test program mutation problem, but they are still limited by ineffective mutation strategies or high human effort requirements. Drawing inspiration from the recent progress of pre-trained Large Language Models (LLMs), such as ChatGPT, in code generation, we propose a new approach named LLM4CBI to utilize LLMs to generate effective test programs for compiler bug isolation. However, using LLMs directly for test program mutation may not yield the desired results due to the challenges associated with formulating precise prompts and selecting specialized prompts. To overcome the challenges, three new components are designed in LLM4CBI. First, LLM4CBI utilizes a program complexity-guided prompt production component, which leverages data and control flow analysis to identify the most valuable variables and locations in programs for mutation. Second, LLM4CBI employs a memorized prompt selection component, which adopts reinforcement learning to select specialized prompts for mutating test programs continuously. Third, a test program validation component is proposed to select specialized feedback prompts to avoid repeating the same mistakes during the mutation process. Compared with the state-of-the-art approaches (DiWi and RecBi) over 120 real bugs from the two most popular compilers, namely GCC and LLVM, our evaluation demonstrates the advantages of LLM4CBI: It can isolate 69.70%/21.74% and 24.44%/8.92% more bugs than DiWi and RecBi within Top-1/Top-5 ranked results. Additionally, we demonstrate that the LLMs component (i.e., GPT-3.5) used in LLM4CBI can be easily replaced by other LLMs while still achieving reasonable results in comparison to related studies. },
	keywords = {Computer bugs;Program processors;Task analysis;Codes;Reinforcement learning;Production;Mathematical models},
	doi = {10.1109/TSE.2024.3397822},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3397822},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {jul}
}


@article{10584357,
	author = {Fang, Chunrong and Sun, Weisong and Chen, Yuchen and Chen, Xiao and Wei, Zhao and Zhang, Quanjun and You, Yudu and Luo, Bin and Liu, Yang and Chen, Zhenyu},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ Esale: Enhancing Code-Summary Alignment Learning for Source Code Summarization }},
	year = {2024},
	volume = {50},
	number = {08},
	ISSN = {1939-3520},
	pages = {2077-2095},
	abstract = { (Source) code summarization aims to automatically generate succinct natural language summaries for given code snippets. Such summaries play a significant role in promoting developers to understand and maintain code. Inspired by neural machine translation, deep learning-based code summarization techniques widely adopt an encoder-decoder framework, where the encoder transforms given code snippets into context vectors, and the decoder decodes context vectors into summaries. Recently, large-scale pre-trained models for source code (e.g., CodeBERT and UniXcoder) are equipped with encoders capable of producing general context vectors and have achieved substantial improvements on the code summarization task. However, although they are usually trained mainly on code-focused tasks and can capture general code features, they still fall short in capturing specific features that need to be summarized. In a nutshell, they fail to learn the alignment between code snippets and summaries (code-summary alignment for short). In this paper, we propose a novel approach to improve code summarization based on summary-focused tasks. Specifically, we exploit a multi-task learning paradigm to train the encoder on three summary-focused tasks to enhance its ability to learn code-summary alignment, including unidirectional language modeling (ULM), masked language modeling (MLM), and action word prediction (AWP). Unlike pre-trained models that mainly predict masked tokens in code snippets, we design ULM and MLM to predict masked words in summaries. Intuitively, predicting words based on given code snippets would help learn the code-summary alignment. In addition, existing work shows that AWP affects the prediction of the entire summary. Therefore, we further introduce the domain-specific task AWP to enhance the ability of the encoder to learn the alignment between action words and code snippets. We evaluate the effectiveness of our approach, called Esale, by conducting extensive experiments on four datasets, including two widely used datasets JCSD and PCSD, a cross-project Java dataset CPJD, and a multilingual language dataset CodeSearchNet. Experimental results show that Esale significantly outperforms state-of-the-art baselines in all three widely used metrics, including BLEU, METEOR, and ROUGE-L. Moreover, the human evaluation proves that the summaries generated by Esale are more informative and closer to the ground-truth summaries. },
	keywords = {Codes;Task analysis;Vectors;Predictive models;Source coding;Decoding;Software},
	doi = {10.1109/TSE.2024.3422274},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3422274},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {aug}
}


@article{10606356,
	author = {Fakhoury, Sarah and Naik, Aaditya and Sakkas, Georgios and Chakraborty, Saikat and Lahiri, Shuvendu K.},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ LLM-Based Test-Driven Interactive Code Generation: User Study and Empirical Evaluation }},
	year = {2024},
	volume = {50},
	number = {09},
	ISSN = {1939-3520},
	pages = {2254-2268},
	abstract = { Large language models (LLMs) have shown great potential in automating significant aspects of coding by producing natural code from informal natural language (NL) intent. However, given NL is informal, it does not lend easily to checking that the generated code correctly satisfies the user intent. In this paper, we propose a novel interactive workflow TiCoder for guided intent clarification (i.e., partial formalization) through tests to support the generation of more accurate code suggestions. Through a mixed methods user study with 15 programmers, we present an empirical evaluation of the effectiveness of the workflow to improve code generation accuracy. We find that participants using the proposed workflow are significantly more likely to correctly evaluate AI generated code, and report significantly less task-induced cognitive load. Furthermore, we test the potential of the workflow at scale with four different state-of-the-art LLMs on two python datasets, using an idealized proxy for a user feedback. We observe an average absolute improvement of 45.97% in the pass@1 code generation accuracy for both datasets and across all LLMs within 5 user interactions, in addition to the automatic generation of accompanying unit tests. },
	keywords = {Codes;Accuracy;Natural languages;Artificial intelligence;Python;Task analysis;Benchmark testing},
	doi = {10.1109/TSE.2024.3428972},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3428972},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {sep}
}


@article{10609742,
	author = {Shin, Jiho and Hemmati, Hadi and Wei, Moshi and Wang, Song},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ Assessing Evaluation Metrics for Neural Test Oracle Generation }},
	year = {2024},
	volume = {50},
	number = {09},
	ISSN = {1939-3520},
	pages = {2337-2349},
	abstract = { Recently, deep learning models have shown promising results in test oracle generation. Neural Oracle Generation (NOG) models are commonly evaluated using static (automatic) metrics which are mainly based on textual similarity of the output, e.g. BLEU, ROUGE-L, METEOR, and Accuracy. However, these textual similarity metrics may not reflect the testing effectiveness of the generated oracle within a test suite, which is often measured by dynamic (execution-based) test adequacy metrics such as code coverage and mutation score. In this work, we revisit existing oracle generation studies plus gpt-3.5 to empirically investigate the current standing of their performance in textual similarity and test adequacy metrics. Specifically, we train and run four state-of-the-art test oracle generation models on seven textual similarity and two test adequacy metrics for our analysis. We apply two different correlation analyses between these two different sets of metrics. Surprisingly, we found no significant correlation between the textual similarity metrics and test adequacy metrics. For instance, gpt-3.5 on the jackrabbit-oak project had the highest performance on all seven textual similarity metrics among the studied NOGs. However, it had the lowest test adequacy metrics compared to all the studied NOGs. We further conducted a qualitative analysis to explore the reasons behind our observations. We found that oracles with high textual similarity metrics but low test adequacy metrics tend to have complex or multiple chained method invocations within the oracle's parameters, making them hard for the model to generate completely, affecting the test adequacy metrics. On the other hand, oracles with low textual similarity metrics but high test adequacy metrics tend to have to call different assertion types or a different method that functions similarly to the ones in the ground truth. Overall, this work complements prior studies on test oracle generation with an extensive performance evaluation on textual similarity and test adequacy metrics and provides guidelines for better assessment of deep learning applications in software test generation in the future. },
	keywords = {Measurement;Correlation;Testing;Software;Codes;Accuracy;Electronic mail},
	doi = {10.1109/TSE.2024.3433463},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3433463},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {sep}
}


@article{10634302,
	author = {Yang, Guang and Zhou, Yu and Chen, Xiang and Zhang, Xiangyu and Zhuo, Terry Yue and Chen, Taolue},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ Chain-of-Thought in Neural Code Generation: From and for Lightweight Language Models }},
	year = {2024},
	volume = {50},
	number = {09},
	ISSN = {1939-3520},
	pages = {2437-2457},
	abstract = { Large Language Models (LLMs) have demonstrated remarkable potential in code generation. The integration of Chain of Thought (CoT) reasoning can further boost their performance. However, current CoT methods often require manual writing or LLMs with over 100 billion parameters to generate, impeding their applicability in resource-constrained scenarios. In this study, we investigate lightweight Language Models ($\ell$ℓLMs), which are defined to have fewer than 10 billion parameters. Empirically, we find that most $\ell$ℓLMs cannot generate high-quality CoTs when prompted by the few-shot method, but can take advantage of high-quality CoTs generated elsewhere to improve their performance in code generation. Based on these findings, we design a novel approach COTTON which can leverage $\ell$ℓLMs to automatically generate CoTs for code generation. We synthesize new datasets and conduct extensive experiments on various benchmarks. The results show that the CoTs generated by COTTON outperform the baselines in terms of automated and human evaluation metrics. In particular, the CoTs generated by COTTON boost various $\ell$ℓLMs to achieve higher performance gains than those generated by LLMs such as ChatGLM (130B), and are competitive with those generated by Gemini and gpt-3.5-turbo. The results also reveal that COTTON not only improves the performance of $\ell$ℓLMs, but also enhances the performance of LLMs. Our study showcases the potential of $\ell$ℓLMs in software engineering applications. },
	keywords = {Codes;Cotton;Task analysis;Computational modeling;Benchmark testing;Training;Software engineering},
	doi = {10.1109/TSE.2024.3440503},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3440503},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {sep}
}


@article{10636040,
	author = {Zhou, Ziyi and Li, Mingchen and Yu, Huiqun and Fan, Guisheng and Yang, Penghui and Huang, Zijie},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ Learning to Generate Structured Code Summaries From Hybrid Code Context }},
	year = {2024},
	volume = {50},
	number = {10},
	ISSN = {1939-3520},
	pages = {2512-2528},
	abstract = { Code summarization aims to automatically generate natural language descriptions for code, and has become a rapidly expanding research area in the past decades. Unfortunately, existing approaches mainly focus on the “one-to-one” mapping from methods to short descriptions, which hinders them from becoming practical tools: 1) The program context is ignored, so they have difficulty in predicting keywords outside the target method; 2) They are typically trained to generate brief function descriptions with only one sentence in length, and therefore have difficulty in providing specific information. These drawbacks are partially due to the limitations of public code summarization datasets. In this paper, we first build a large code summarization dataset including different code contexts and summary content annotations, and then propose a deep learning framework that learns to generate structured code summaries from hybrid program context, named StructCodeSum. It provides both an LLM-based approach and a lightweight approach which are suitable for different scenarios. Given a target method, StructCodeSum predicts its function description, return description, parameter description, and usage description through hybrid code context, and ultimately builds a Javadoc-style code summary. The hybrid code context consists of path context, class context, documentation context and call context of the target method. Extensive experimental results demonstrate: 1) The hybrid context covers more than 70% of the summary tokens in average and significantly boosts the model performance; 2) When generating function descriptions, StructCodeSum outperforms the state-of-the-art approaches by a large margin; 3) According to human evaluation, the quality of the structured summaries generated by our approach is better than the documentation generated by Code Llama. },
	keywords = {Codes;Hybrid power systems;Documentation;Task analysis;Annotations;Source coding;Context modeling},
	doi = {10.1109/TSE.2024.3439562},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3439562},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {oct}
}


@article{10664637,
	author = {Kang, Sungmin and Yoon, Juyeon and Askarbekkyzy, Nargiz and Yoo, Shin},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ Evaluating Diverse Large Language Models for Automatic and General Bug Reproduction }},
	year = {2024},
	volume = {50},
	number = {10},
	ISSN = {1939-3520},
	pages = {2677-2694},
	abstract = { Bug reproduction is a critical developer activity that is also challenging to automate, as bug reports are often in natural language and thus can be difficult to transform to test cases consistently. As a result, existing techniques mostly focused on crash bugs, which are easier to automatically detect and verify. In this work, we overcome this limitation by using large language models (LLMs), which have been demonstrated to be adept at natural language processing and code generation. By prompting LLMs to generate bug-reproducing tests, and via a post-processing pipeline to automatically identify promising generated tests, our proposed technique Libro could successfully reproduce about one-third of all bugs in the widely used Defects4J benchmark. Furthermore, our extensive evaluation on 15 LLMs, including 11 open-source LLMs, suggests that open-source LLMs also demonstrate substantial potential, with the StarCoder LLM achieving 70% of the reproduction performance of the closed-source OpenAI LLM code-davinci-002 on the large Defects4J benchmark, and 90% of performance on a held-out bug dataset likely not part of any LLM's training data. In addition, our experiments on LLMs of different sizes show that bug reproduction using Libro improves as LLM size increases, providing information as to which LLMs can be used with the Libro pipeline. },
	keywords = {Computer bugs;Codes;Pipelines;Large language models;Debugging;Java;Computational modeling},
	doi = {10.1109/TSE.2024.3450837},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3450837},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {oct}
}


@article{10538301,
	author = {Wang, Haoye and Gao, Zhipeng and Hu, Xing and Lo, David and Grundy, John and Wang, Xinyu},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ Just-In-Time TODO-Missed Commits Detection }},
	year = {2024},
	volume = {50},
	number = {11},
	ISSN = {1939-3520},
	pages = {2732-2752},
	abstract = { TODO comments play an important role in helping developers to manage their tasks and communicate with other team members. TODO comments are often introduced by developers as a type of technical debt, such as a reminder to add/remove features or a request to optimize the code implementations. These can all be considered as notifications for developers to revisit regarding the current suboptimal solutions. TODO comments often bring short-term benefits – higher productivity or shorter development cost – and indicate attention needs to be paid for the long-term software quality. Unfortunately, due to their lack of knowledge or experience and/or the time constraints, developers sometimes may forget or even not be aware of suboptimal implementations. The loss of the TODO comments for these suboptimal solutions may hurt the software quality and reliability in the long-term. Therefore it is beneficial to remind the developers of the suboptimal solutions whenever they change the code. In this work, we refer this problem to the task of detecting TODO-missed commits, and we propose a novel approach named TDReminder (TODO comment Reminder) to address the task. With the help of TDReminder, developers can identify possible missing TODO commits just-in-time when submitting a commit. Our approach has two phases: offline training and online inference. We first embed code change and commit message into contextual vector representations using two neural encoders respectively. The association between these representations is learned by our model automatically. In the online inference phase, TDReminder leverages the trained model to compute the likelihood of a commit being a TODO-missed commit. We evaluate TDReminder on datasets crawled from 10k popular Python and Java repositories in GitHub respectively. Our experimental results show that TDReminder outperforms a set of benchmarks by a large margin in TODO-missed commits detection. Moreover, to better help developers use TDReminder in practice, we have incorporated Large Language Models (LLMs) with our approach to provide explainable recommendations. The user study shows that our tool can effectively inform developers not only “when” to add TODOs, but also “where” and “what” TODOs should be added, verifying the value of our tool in practical application. },
	keywords = {Codes;Task analysis;Training;Python;Stars;Software quality;Software development management},
	doi = {10.1109/TSE.2024.3405005},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3405005},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {nov}
}


@article{10707668,
	author = {Li, Yichen and Huo, Yintong and Jiang, Zhihan and Zhong, Renyi and He, Pinjia and Su, Yuxin and Briand, Lionel C. and Lyu, Michael R.},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ Exploring the Effectiveness of LLMs in Automated Logging Statement Generation: An Empirical Study }},
	year = {2024},
	volume = {50},
	number = {12},
	ISSN = {1939-3520},
	pages = {3188-3207},
	abstract = { Automated logging statement generation supports developers in documenting critical software runtime behavior. While substantial recent research has focused on retrieval-based and learning-based methods, results suggest they fail to provide appropriate logging statements in real-world complex software. Given the great success in natural language generation and programming language comprehension, large language models (LLMs) might help developers generate logging statements, but this has not yet been investigated. To fill the gap, this paper performs the first study on exploring LLMs for logging statement generation. We first build a logging statement generation dataset, LogBench, with two parts: (1) LogBench-O: 3,870 methods with 6,849 logging statements collected from GitHub repositories, and (2) LogBench-T: the transformed unseen code from LogBench-O. Then, we leverage LogBench to evaluate the effectiveness and generalization capabilities (using LogBench-T) of 13 top-performing LLMs, from 60M to 405B parameters. In addition, we examine the performance of these LLMs against classical retrieval-based and machine learning-based logging methods from the era preceding LLMs. Specifically, we evaluate the logging effectiveness of LLMs by studying their ability to determine logging ingredients and the impact of prompts and external program information. We further evaluate LLM's logging generalization capabilities using unseen data (LogBench-T) derived from code transformation techniques. While existing LLMs deliver decent predictions on logging levels and logging variables, our study indicates that they only achieve a maximum BLEU score of 0.249, thus calling for improvements. The paper also highlights the importance of prompt constructions and external factors (e.g., programming contexts and code comments) for LLMs’ logging performance. In addition, we observed that existing LLMs show a significant performance drop (8.2%-16.2% decrease) when dealing with logging unseen code, revealing their unsatisfactory generalization capabilities. Based on these findings, we identify five implications and provide practical advice for future logging research. Our empirical analysis discloses the limitations of current logging approaches while showcasing the potential of LLM-based logging tools, and provides actionable guidance for building more practical models. },
	keywords = {Codes;Software;Natural languages;Runtime;Generators;Training data;Software development management;Measurement;Large language models;Analytical models},
	doi = {10.1109/TSE.2024.3475375},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3475375},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {dec}
}


@article{10713474,
	author = {Xue, Pengyu and Wu, Linhao and Yu, Zhongxing and Jin, Zhi and Yang, Zhen and Li, Xinyi and Yang, Zhenyu and Tan, Yue},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ Automated Commit Message Generation With Large Language Models: An Empirical Study and Beyond }},
	year = {2024},
	volume = {50},
	number = {12},
	ISSN = {1939-3520},
	pages = {3208-3224},
	abstract = { Commit Message Generation (CMG) approaches aim to automatically generate commit messages based on given code diffs, which facilitate collaboration among developers and play a critical role in Open-Source Software (OSS). Very recently, Large Language Models (LLMs) have been applied in diverse code-related tasks owing to their powerful generality. Yet, in the CMG field, few studies systematically explored their effectiveness. This paper conducts the first comprehensive experiment to investigate how far we have been in applying LLM to generate high-quality commit messages and how to go further beyond in this field. Motivated by a pilot analysis, we first construct a multi-lingual high-quality CMG test set following practitioners’ criteria. Afterward, we re-evaluate diverse CMG approaches and make comparisons with recent LLMs. To delve deeper into LLMs’ ability, we further propose four manual metrics following the practice of OSS, including Accuracy, Integrity, Readability, and Applicability for assessment. Results reveal that LLMs have outperformed existing CMG approaches overall, and different LLMs carry different advantages, where GPT-3.5 performs best. To further boost LLMs’ performance in the CMG task, we propose an Efficient Retrieval-based In-Context Learning (ICL) framework, namely ERICommiter, which leverages a two-step filtering to accelerate the retrieval efficiency and introduces semantic/lexical-based retrieval algorithm to construct the ICL examples, thereby guiding the generation of high-quality commit messages with LLMs. Extensive experiments demonstrate the substantial performance improvement of ERICommiter on various LLMs across different programming languages. Meanwhile, ERICommiter also significantly reduces the retrieval time while keeping almost the same performance. Our research contributes to the understanding of LLMs’ capabilities in the CMG field and provides valuable insights for practitioners seeking to leverage these tools in their workflows. },
	keywords = {Codes;Measurement;Manuals;Collaboration;Systematics;Solid modeling;Data models;Python;Large language models;Java},
	doi = {10.1109/TSE.2024.3478317},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3478317},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {dec}
}


@article{10734067,
	author = {Liao, Dianshu and Pan, Shidong and Sun, Xiaoyu and Ren, Xiaoxue and Huang, Qing and Xing, Zhenchang and Jin, Huan and Li, Qinying},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ $\mathbf{A^{3}}$A3-CodGen: A Repository-Level Code Generation Framework for Code Reuse With Local-Aware, Global-Aware, and Third-Party-Library-Aware }},
	year = {2024},
	volume = {50},
	number = {12},
	ISSN = {1939-3520},
	pages = {3369-3384},
	abstract = { LLM-based code generation tools are essential to help developers in the software development process. Existing tools often disconnect with the working context, i.e., the code repository, causing the generated code to be not similar to human developers. In this paper, we propose a novel code generation framework, dubbed $A^{3}$A3-CodGen, to harness information within the code repository to generate code with fewer potential logical errors, code redundancy, and library-induced compatibility issues. We identify three types of representative information for the code repository: local-aware information from the current code file, global-aware information from other code files, and third-party-library information. Results demonstrate that by adopting the $A^{3}$A3-CodGen framework, we successfully extract, fuse, and feed code repository information into the LLM, generating more accurate, efficient, and highly reusable code. The effectiveness of our framework is further underscored by generating code with a higher reuse rate, compared to human developers. This research contributes significantly to the field of code generation, providing developers with a more powerful tool to address the evolving demands in software development in practice. },
	keywords = {Codes;Libraries;Software development management;Data mining;Benchmark testing;Transformers;Software engineering;Context modeling;Chatbots;Australia},
	doi = {10.1109/TSE.2024.3486195},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3486195},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {dec}
}


@inproceedings{10.1145/3597503.3639188,
	author = {Cai, Yuchen and Yadavally, Aashish and Mishra, Abhishek and Montejo, Genesis and Nguyen, Tien},
	title = {Programming Assistant for Exception Handling with CodeBERT},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639188},
	doi = {10.1145/3597503.3639188},
	abstract = {With practical code reuse, the code fragments from developers' forums often migrate to applications. Owing to the incomplete nature of such fragments, they often lack the details on exception handling. The adaptation for exception handling to the codebase is not trivial as developers must learn and memorize what API methods could cause exceptions and what exceptions need to be handled. We propose Neurex, an exception handling recommender that learns from complete code, and accepts a given Java code snippet and recommends 1) if a try-catch block is needed, 2) what statements need to be placed in a try block, and 3) what exception types need to be caught in the catch clause. Inspired by the sequence chunking techniques in natural language processing, we design Neurex via a multi-tasking model with the fine-tuning of the large language model CodeBERT for these three exception handling recommendation tasks. Via the large language model, Neurex can learn the surrounding context, leading to better learning the dependencies among the API elements, and the relations between the statements and the corresponding exception types needed to be handled.Our empirical evaluation shows that Neurex correctly performs all three exception handling recommendation tasks in 71.5\% of the cases with a F1-score of 70.2\%, which is a relative improvement of 166\% over the baseline. It achieves high F1-score from 98.2\%-99.7\% in try-catch block necessity checking (a relative improvement of up to 55.9\% over the baselines). It also correctly decides both the need for try-catch block(s) and the statements to be placed in try blocks with the F1-scores of 74.7\% and 87.1\% at the instance and statement levels, an improvement of 129.1\% and 44.9\% over the baseline, respectively. Our extrinsic evaluation shows that Neurex relatively improves over the baseline by 56.5\% in F1-score for detecting exception-related bugs in incomplete Android code snippets.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {94},
	numpages = {13},
	keywords = {AI4SE, large language models, automated exception handling},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639187,
	author = {Nam, Daye and Macvean, Andrew and Hellendoorn, Vincent and Vasilescu, Bogdan and Myers, Brad},
	title = {Using an LLM to Help With Code Understanding},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639187},
	doi = {10.1145/3597503.3639187},
	abstract = {Understanding code is challenging, especially when working in new and complex development environments. Code comments and documentation can help, but are typically scarce or hard to navigate. Large language models (LLMs) are revolutionizing the process of writing code. Can they do the same for helping understand it? In this study, we provide a first investigation of an LLM-based conversational UI built directly in the IDE that is geared towards code understanding. Our IDE plugin queries OpenAI's GPT-3.5-turbo model with four high-level requests without the user having to write explicit prompts: to explain a highlighted section of code, provide details of API calls used in the code, explain key domain-specific terms, and provide usage examples for an API. The plugin also allows for open-ended prompts, which are automatically contextualized to the LLM with the program being edited. We evaluate this system in a user study with 32 participants, which confirms that using our plugin can aid task completion more than web search. We additionally provide a thorough analysis of the ways developers use, and perceive the usefulness of, our system, among others finding that the usage and benefits differ between students and professionals. We conclude that in-IDE prompt-less interaction with LLMs is a promising future direction for tool builders.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {97},
	numpages = {13},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3663529.3663826,
	author = {Yu, Zhaoyang and Ma, Minghua and Zhang, Chaoyun and Qin, Si and Kang, Yu and Bansal, Chetan and Rajmohan, Saravan and Dang, Yingnong and Pei, Changhua and Pei, Dan and Lin, Qingwei and Zhang, Dongmei},
	title = {MonitorAssistant: Simplifying Cloud Service Monitoring via Large Language Models},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3663826},
	doi = {10.1145/3663529.3663826},
	abstract = {In large-scale cloud service systems, monitoring metric data and conducting anomaly detection is an important way to maintain reliability and stability. However, great disparity exists between academic approaches and industrial practice to anomaly detection. Industry predominantly uses simple, efficient methods due to better interpretability and ease of implementation. In contrast, academically favor deep-learning methods, despite their advanced capabilities, face practical challenges in real-world applications. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
To address these challenges, this paper introduces MonitorAssistant, an end-to-end practical anomaly detection system via Large Language Models. MonitorAssistant automates model configuration recommendation achieving knowledge inheritance and alarm interpretation with guidance-oriented anomaly reports, facilitating a more intuitive engineer-system interaction through natural language. By deploying MonitorAssistant in Microsoft's cloud service system, we validate its efficacy and practicality, marking a significant advancement in the field of practical anomaly detection for large-scale cloud services.},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {38–49},
	numpages = {12},
	keywords = {Anomaly Detection, Cloud Service Monitoring, Large Language Models, Software Reliability},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@inproceedings{10.1145/3663529.3663803,
	author = {Pomian, Dorin and Bellur, Abhiram and Dilhara, Malinda and Kurbatova, Zarina and Bogomolov, Egor and Sokolov, Andrey and Bryksin, Timofey and Dig, Danny},
	title = {EM-Assist: Safe Automated ExtractMethod Refactoring with LLMs},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3663803},
	doi = {10.1145/3663529.3663803},
	abstract = {Excessively long methods, loaded with multiple responsibilities, are challenging to understand, debug, reuse, and maintain. The
 
solution lies in the widely recognized Extract Method refactoring. While the application of this refactoring is supported in modern
 
IDEs, recommending which code fragments to extract has been the topic of many research tools. However, they often struggle to replicate real-world developer practices, resulting in recommendations that do not align with what a human developer would do in real
 
life. To address this issue, we introduce EM-Assist, an IntelliJ IDEA plugin that uses LLMs to generate refactoring suggestions and subsequently validates, enhances, and ranks them. Finally, EM-Assist uses the IntelliJ IDE to apply the user-selected recommendation.
 
In our extensive evaluation of 1,752 real-world refactorings that actually took place in open-source projects, EM-Assist’s recall rate
 
was 53.4\% among its top-5 recommendations, compared to 39.4\% for the previous best-in-class tool that relies solely on static analysis. Moreover, we conducted a usability survey with 18 industrial developers and 94.4\% gave a positive rating.},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {582–586},
	numpages = {5},
	keywords = {Code smells, Java, Kotlin, LLMs, Long Methods, Refactoring},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@article{10697930,
	author = {Pan, Rongqi and Ghaleb, Taher A. and Briand, Lionel C.},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ LTM: Scalable and Black-Box Similarity-Based Test Suite Minimization Based on Language Models }},
	year = {2024},
	volume = {50},
	number = {11},
	ISSN = {1939-3520},
	pages = {3053-3070},
	abstract = { Test suites tend to grow when software evolves, making it often infeasible to execute all test cases with the allocated testing budgets, especially for large software systems. Test suite minimization (TSM) is employed to improve the efficiency of software testing by removing redundant test cases, thus reducing testing time and resources while maintaining the fault detection capability of the test suite. Most existing TSM approaches rely on code coverage (white-box) or model-based features, which are not always available to test engineers. Recent TSM approaches that rely only on test code (black-box) have been proposed, such as ATM and FAST-R. The former yields higher fault detection rates (FDR) while the latter is faster. To address scalability while retaining a high FDR, we propose LTM (Language model-based Test suite Minimization), a novel, scalable, and black-box similarity-based TSM approach based on large language models (LLMs), which is the first application of LLMs in the context of TSM. To support similarity measurement using test method embeddings, we investigate five different pre-trained language models: CodeBERT, GraphCodeBERT, UniXcoder, StarEncoder, and CodeLlama, on which we compute two similarity measures: Cosine Similarity and Euclidean Distance. Our goal is to find similarity measures that are not only computationally more efficient but can also better guide a Genetic Algorithm (GA), which is used to search for optimal minimized test suites, thus reducing the overall search time. Experimental results show that the best configuration of LTM (UniXcoder/Cosine) outperforms ATM in three aspects: (a) achieving a slightly greater saving rate of testing time ($41.72\%$41.72% versus $41.02\%$41.02%, on average); (b) attaining a significantly higher fault detection rate ($0.84$0.84 versus $0.81$0.81, on average); and, most importantly, (c) minimizing test suites nearly five times faster on average, with higher gains for larger test suites and systems, thus achieving much higher scalability. },
	keywords = {Minimization;Codes;Fault detection;Closed box;Scalability;Time measurement;Genetic algorithms;Source coding;Vectors;Unified modeling language},
	doi = {10.1109/TSE.2024.3469582},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3469582},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {nov}
}


@inproceedings{10.1145/3597503.3608136,
	author = {Niu, Changan and Li, Chuanyi and Ng, Vincent and Lo, David and Luo, Bin},
	title = {FAIR: Flow Type-Aware Pre-Training of Compiler Intermediate Representations},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3608136},
	doi = {10.1145/3597503.3608136},
	abstract = {While the majority of existing pre-trained models from code learn source code features such as code tokens and abstract syntax trees, there are some other works that focus on learning from compiler intermediate representations (IRs). Existing IR-based models typically utilize IR features such as instructions, control and data flow graphs (CDFGs), call graphs, etc. However, these methods confuse variable nodes and instruction nodes in a CDFG and fail to distinguish different types of flows, and the neural networks they use fail to capture long-distance dependencies and have over-smoothing and over-squashing problems. To address these weaknesses, we propose FAIR, a Flow type-Aware pre-trained model for IR that involves employing (1) a novel input representation of IR programs; (2) Graph Transformer to address over-smoothing, over-squashing and long-dependencies problems; and (3) five pre-training tasks that we specifically propose to enable FAIR to learn the semantics of IR tokens, flow type information, and the overall representation of IR. Experimental results show that FAIR can achieve state-of-the-art results on four code-related downstream tasks.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {33},
	numpages = {12},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3608140,
	author = {Ding, Yangruibo and Steenhoek, Benjamin and Pei, Kexin and Kaiser, Gail and Le, Wei and Ray, Baishakhi},
	title = {TRACED: Execution-aware Pre-training for Source Code},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3608140},
	doi = {10.1145/3597503.3608140},
	abstract = {Most existing pre-trained language models for source code focus on learning the static code text, typically augmented with static code structures (abstract syntax tree, dependency graphs, etc.). However, program semantics will not be fully exposed before the real execution. Without an understanding of the program execution, statically pre-trained models fail to comprehensively capture the dynamic code properties, such as the branch coverage and the runtime variable values, and they are consequently less effective at code understanding tasks, such as retrieving semantic clones and detecting software vulnerabilities.To close the gap between the static nature of language models and the dynamic characteristics of programs, we introduce TRACED, an execution-aware pre-training strategy for source code. Specifically, we pre-train code language models with a combination of source code, executable inputs, and corresponding execution traces. Our goal is to teach code models the complicated execution logic during the pre-training, enabling the model to statically estimate the dynamic code properties without repeatedly executing code during task-specific fine-tuning.To illustrate the effectiveness of our proposed approach, we fine-tune and evaluate TRACED on three downstream tasks: static execution estimation, clone retrieval, and vulnerability detection. The empirical results show that TRACED relatively improves the statically pre-trained code models by 12.4\% for complete execution path prediction and by 25.2\% for runtime variable value predictions. TRACED also significantly outperforms statically pre-trained models in clone retrieval and vulnerability detection across four public benchmarks.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {36},
	numpages = {12},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639074,
	author = {Yang, Zhou and Zhao, Zhipeng and Wang, Chenyu and Shi, Jieke and Kim, Dongsun and Han, Donggyun and Lo, David},
	title = {Unveiling Memorization in Code Models},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639074},
	doi = {10.1145/3597503.3639074},
	abstract = {The availability of large-scale datasets, advanced architectures, and powerful computational resources have led to effective code models that automate diverse software engineering activities. The datasets usually consist of billions of lines of code from both open-source and private repositories. A code model memorizes and produces source code verbatim, which potentially contains vulnerabilities, sensitive information, or code with strict licenses, leading to potential security and privacy issues.This paper investigates an important problem: to what extent do code models memorize their training data? We conduct an empirical study to explore memorization in large pre-trained code models. Our study highlights that simply extracting 20,000 outputs (each having 512 tokens) from a code model can produce over 40,125 code snippets that are memorized from the training data. To provide a better understanding, we build a taxonomy of memorized contents with 3 categories and 14 subcategories. The results show that the prompts sent to the code models affect the distribution of memorized contents. We identify several key factors of memorization. Specifically, given the same architecture, larger models suffer more from memorization problem. A code model produces more memorization when it is allowed to generate longer outputs. We also find a strong positive correlation between the number of an output's occurrences in the training data and that in the generated outputs, which indicates that a potential way to reduce memorization is to remove duplicates in the training data. We then identify effective metrics that infer whether an output contains memorization accurately. We also make suggestions to deal with memorization.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {72},
	numpages = {13},
	keywords = {open-source software, memorization, code generation},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639091,
	author = {Li, Zongjie and Wang, Chaozheng and Ma, Pingchuan and Liu, Chaowei and Wang, Shuai and Wu, Daoyuan and Gao, Cuiyun and Liu, Yang},
	title = {On Extracting Specialized Code Abilities from Large Language Models: A Feasibility Study},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639091},
	doi = {10.1145/3597503.3639091},
	abstract = {Recent advances in large language models (LLMs) significantly boost their usage in software engineering. However, training a well-performing LLM demands a substantial workforce for data collection and annotation. Moreover, training datasets may be proprietary or partially open, and the process often requires a costly GPU cluster. The intellectual property value of commercial LLMs makes them attractive targets for imitation attacks, but creating an imitation model with comparable parameters still incurs high costs. This motivates us to explore a practical and novel direction: slicing commercial black-box LLMs using medium-sized backbone models.In this paper, we explore the feasibility of launching imitation attacks on LLMs to extract their specialized code abilities, such as "code synthesis" and "code translation." We systematically investigate the effectiveness of launching code ability extraction attacks under different code-related tasks with multiple query schemes, including zero-shot, in-context, and Chain-of-Thought. We also design response checks to refine the outputs, leading to an effective imitation training process. Our results show promising outcomes, demonstrating that with a reasonable number of queries, attackers can train a medium-sized backbone model to replicate specialized code behaviors similar to the target LLMs. We summarize our findings and insights to help researchers better understand the threats posed by imitation attacks, including revealing a practical attack surface for generating adversarial code examples against LLMs.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {74},
	numpages = {13},
	keywords = {large language models, imitation attacks},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639125,
	author = {Zhu, Qihao and Liang, Qingyuan and Sun, Zeyu and Xiong, Yingfei and Zhang, Lu and Cheng, Shengyu},
	title = {GrammarT5: Grammar-Integrated Pretrained Encoder-Decoder Neural Model for Code},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639125},
	doi = {10.1145/3597503.3639125},
	abstract = {Pretrained models for code have exhibited promising performance across various code-related tasks, such as code summarization, code completion, code translation, and bug detection. However, despite their success, the majority of current models still represent code as a token sequence, which may not adequately capture the essence of the underlying code structure.In this work, we propose GrammarT5, a grammar-integrated encoder-decoder pretrained neural model for code. GrammarT5 employs a novel grammar-integrated representation, Tokenized Grammar Rule Sequence (TGRS), for code. TGRS is constructed based on the grammar rule sequence utilized in syntax-guided code generation and integrates syntax information with code tokens within an appropriate input length. Furthermore, we suggest attaching language flags to help GrammarT5 differentiate between grammar rules of various programming languages. Finally, we introduce two novel pretraining tasks---Edge Prediction (EP), and Sub-Tree Prediction (STP) to learn syntactic information.Experiments were conducted on five code-related tasks using eleven datasets, demonstrating that GrammarT5 achieves state-of-the-art (SOTA) performance on most tasks in comparison to models of the same scale. Additionally, the paper illustrates that the proposed pretraining tasks and language flags can enhance GrammarT5 to better capture the syntax and semantics of code.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {76},
	numpages = {13},
	keywords = {neural networks, pretrained model, text tagging},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639182,
	author = {Meem, Fairuz Nawer and Smith, Justin and Johnson, Brittany},
	title = {Exploring Experiences with Automated Program Repair in Practice},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639182},
	doi = {10.1145/3597503.3639182},
	abstract = {Automated program repair, also known as APR, is an approach for automatically repairing software faults. There is a large amount of research on automated program repair, but very little offers in-depth insights into how practitioners think about and employ APR in practice. To learn more about practitioners' perspectives and experiences with current APR tools and techniques, we administered a survey, which received valid responses from 331 software practitioners. We analyzed survey responses to gain insights regarding factors that correlate with APR awareness, experience, and use. We established a strong correlation between APR awareness and tool use and attributes including job position, company size, total coding experience, and preferred language of software practitioners. We also found that practitioners are using other forms of support, such as co-workers and ChatGPT, more frequently than APR tools when fixing software defects. We learned about the drawbacks that practitioners encounter while utilizing existing APR tools and the impact that each drawback has on their practice. Our findings provide implications for research and practice centered on development, adoption, and use of APR.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {86},
	numpages = {11},
	keywords = {automated program repair, software bugs, software tools},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639156,
	author = {Babakol, Timur and Liu, Yu David},
	title = {Tensor-Aware Energy Accounting},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639156},
	doi = {10.1145/3597503.3639156},
	abstract = {With the rapid growth of Artificial Intelligence (AI) applications supported by deep learning (DL), the energy efficiency of these applications has an increasingly large impact on sustainability. We introduce Smaragdine, a new energy accounting system for tensor-based DL programs implemented with TensorFlow. At the heart of Smaragdine is a novel white-box methodology of energy accounting: Smaragdine is aware of the internal structure of the DL program, which we call tensor-aware energy accounting. With Smaragdine, the energy consumption of a DL program can be broken down into units aligned with its logical hierarchical decomposition structure. We apply Smaragdine for understanding the energy behavior of BERT, one of the most widely used language models. Layer-by-layer and tensor-by-tensor, Smaragdine is capable of identifying the highest energy/power-consuming components of BERT. Furthermore, we conduct two case studies on how Smaragdine supports downstream toolchain building, one on the comparative energy impact of hyperparameter tuning of BERT, the other on the energy behavior evolution when BERT evolves to its next generation, ALBERT.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {93},
	numpages = {12},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639142,
	author = {Liu, Zhongxin and Tang, Zhijie and Zhang, Junwei and Xia, Xin and Yang, Xiaohu},
	title = {Pre-training by Predicting Program Dependencies for Vulnerability Analysis Tasks},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639142},
	doi = {10.1145/3597503.3639142},
	abstract = {Vulnerability analysis is crucial for software security. Inspired by the success of pre-trained models on software engineering tasks, this work focuses on using pre-training techniques to enhance the understanding of vulnerable code and boost vulnerability analysis. The code understanding ability of a pre-trained model is highly related to its pre-training objectives. The semantic structure, e.g., control and data dependencies, of code is important for vulnerability analysis. However, existing pre-training objectives either ignore such structure or focus on learning to use it. The feasibility and benefits of learning the knowledge of analyzing semantic structure have not been investigated. To this end, this work proposes two novel pre-training objectives, namely Control Dependency Prediction (CDP) and Data Dependency Prediction (DDP), which aim to predict the statement-level control dependencies and token-level data dependencies, respectively, in a code snippet only based on its source code. During pre-training, CDP and DDP can guide the model to learn the knowledge required for analyzing fine-grained dependencies in code. After pre-training, the pre-trained model can boost the understanding of vulnerable code during fine-tuning and can directly be used to perform dependence analysis for both partial and complete functions. To demonstrate the benefits of our pre-training objectives, we pre-train a Transformer model named PDBERT with CDP and DDP, fine-tune it on three vulnerability analysis tasks, i.e., vulnerability detection, vulnerability classification, and vulnerability assessment, and also evaluate it on program dependence analysis. Experimental results show that PDBERT benefits from CDP and DDP, leading to state-of-the-art performance on the three downstream tasks. Also, PDBERT achieves F1-scores of over 99\% and 94\% for predicting control and data dependencies, respectively, in partial and complete functions.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {151},
	numpages = {13},
	keywords = {source code pre-training, program dependence analysis, vulnerability detection, vulnerability classification, vulnerability assessment},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639075,
	author = {Serafini, Raphael and Otto, Clemens and Horstmann, Stefan Albert and Naiakshina, Alena},
	title = {ChatGPT-Resistant Screening Instrument for Identifying Non-Programmers},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639075},
	doi = {10.1145/3597503.3639075},
	abstract = {To ensure the validity of software engineering and IT security studies with professional programmers, it is essential to identify participants without programming skills. Existing screening questions are efficient, cheating robust, and effectively differentiate programmers from non-programmers. However, the release of ChatGPT raises concerns about their continued effectiveness in identifying non-programmers. In a simulated attack, we showed that Chat-GPT can easily solve existing screening questions. Therefore, we designed new ChatGPT-resistant screening questions using visual concepts and code comprehension tasks. We evaluated 28 screening questions in an online study with 121 participants involving programmers and non-programmers. Our results showed that questions using visualizations of well-known programming concepts performed best in differentiating between programmers and non-programmers. Participants prompted to use ChatGPT struggled to solve the tasks. They considered ChatGPT ineffective and changed their strategy after a few screening questions. In total, we present six ChatGPT-resistant screening questions that effectively identify non-programmers. We provide recommendations on setting up a ChatGPT-resistant screening instrument that takes less than three minutes to complete by excluding 99.47\% of non-programmers while including 94.83\% of programmers.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {181},
	numpages = {13},
	keywords = {chatgpt, programmer screening, developer study, study protection},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639201,
	author = {Choudhuri, Rudrajit and Liu, Dylan and Steinmacher, Igor and Gerosa, Marco and Sarma, Anita},
	title = {How Far Are We? The Triumphs and Trials of Generative AI in Learning Software Engineering},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639201},
	doi = {10.1145/3597503.3639201},
	abstract = {Conversational Generative AI (convo-genAI) is revolutionizing Software Engineering (SE) as engineers and academics embrace this technology in their work. However, there is a gap in understanding the current potential and pitfalls of this technology, specifically in supporting students in SE tasks. In this work, we evaluate through a between-subjects study (N=22) the effectiveness of ChatGPT, a convo-genAI platform, in assisting students in SE tasks. Our study did not find statistical differences in participants' productivity or self-efficacy when using ChatGPT as compared to traditional resources, but we found significantly increased frustration levels. Our study also revealed 5 distinct faults arising from violations of Human-AI interaction guidelines, which led to 7 different (negative) consequences on participants.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {184},
	numpages = {13},
	keywords = {empirical study, software engineering, generative AI, ChatGPT},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639177,
	author = {Wei, Moshi and Harzevili, Nima Shiri and Huang, Yuekai and Yang, Jinqiu and Wang, Junjie and Wang, Song},
	title = {Demystifying and Detecting Misuses of Deep Learning APIs},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639177},
	doi = {10.1145/3597503.3639177},
	abstract = {Deep Learning (DL) libraries have significantly impacted various domains in computer science over the last decade. However, developers often face challenges when using the DL APIs, as the development paradigm of DL applications differs greatly from traditional software development. Existing studies on API misuse mainly focus on traditional software, leaving a gap in understanding API misuse within DL APIs. To address this gap, we present the first comprehensive study of DL API misuse in TensorFlow and PyTorch. Specifically, we first collected a dataset of 4,224 commits from the top 200 most-starred projects using these two libraries and manually identified 891 API misuses. We then investigated the characteristics of these misuses from three perspectives, i.e., types, root causes, and symptoms. We have also conducted an evaluation to assess the effectiveness of the current state-of-the-art API misuse detector on our 891 confirmed API misuses. Our results confirmed that the state-of-the-art API misuse detector is ineffective in detecting DL API misuses. To address the limitations of existing API misuse detection for DL APIs, we propose LLMAPIDet, which leverages Large Language Models (LLMs) for DL API misuse detection and repair. We build LLMAPIDet by prompt-tuning a chain of ChatGPT prompts on 600 out of 891 confirmed API misuses and reserve the rest 291 API misuses as the testing dataset. Our evaluation shows that LLMAPIDet can detect 48 out of the 291 DL API misuses while none of them can be detected by the existing API misuse detector. We further evaluate LLMAPIDet on the latest versions of 10 GitHub projects. The evaluation shows that LLMAPIDet can identify 119 previously unknown API misuses and successfully fix 46 of them.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {201},
	numpages = {12},
	keywords = {API misuse, deep learning APIs, empirical study, detection},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639107,
	author = {Xie, Fuman and Yan, Chuan and Meng, Mark Huasong and Teng, Shaoming and Zhang, Yanjun and Bai, Guangdong},
	title = {Are Your Requests Your True Needs? Checking Excessive Data Collection in VPA App},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639107},
	doi = {10.1145/3597503.3639107},
	abstract = {Virtual personal assistants (VPA) services encompass a large number of third-party applications (or apps) to enrich their functionalities. These apps have been well examined to scrutinize their data collection behaviors against their declared privacy policies. Nonetheless, it is often overlooked that most users tend to ignore privacy policies at the installation time. Dishonest developers thus can exploit this situation by embedding excessive declarations to cover their data collection behaviors during compliance auditing.In this work, we present Pico, a privacy inconsistency detector, which checks the VPA app's privacy compliance by analyzing (in)consistency between data requested and data essential for its functionality. Pico understands the app's functionality topics from its publicly available textual data, and leverages advanced GPT-based language models to address domain-specific challenges. Based on the counterparts with similar functionality, suspicious data collection can be detected through the lens of anomaly detection. We apply Pico to understand the status quo of data-functionality compliance among all 65,195 skills in the Alexa app store. Our study reveals that 21.7\% of the analyzed skills exhibit suspicious data collection, including Top 10 popular Alexa skills that pose threats to 54,116 users. These findings should raise an alert to both developers and users, in the compliance with the purpose limitation principle in data regulations.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {205},
	numpages = {12},
	keywords = {virtual personal assistant, privacy compliance, alexa skills},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3597503.3639585,
	author = {Imran, Mia Mohammad and Chatterjee, Preetha and Damevski, Kostadin},
	title = {Shedding Light on Software Engineering-specific Metaphors and Idioms},
	year = {2024},
	isbn = {9798400702174},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3597503.3639585},
	doi = {10.1145/3597503.3639585},
	abstract = {Use of figurative language, such as metaphors and idioms, is common in our daily-life communications, and it can also be found in Software Engineering (SE) channels, such as comments on GitHub. Automatically interpreting figurative language is a challenging task, even with modern Large Language Models (LLMs), as it often involves subtle nuances. This is particularly true in the SE domain, where figurative language is frequently used to convey technical concepts, often bearing developer affect (e.g., 'spaghetti code). Surprisingly, there is a lack of studies on how figurative language in SE communications impacts the performance of automatic tools that focus on understanding developer communications, e.g., bug prioritization, incivility detection. Furthermore, it is an open question to what extent state-of-the-art LLMs interpret figurative expressions in domain-specific communication such as software engineering. To address this gap, we study the prevalence and impact of figurative language in SE communication channels. This study contributes to understanding the role of figurative language in SE, the potential of LLMs in interpreting them, and its impact on automated SE communication analysis. Our results demonstrate the effectiveness of fine-tuning LLMs with figurative language in SE and its potential impact on automated tasks that involve affect. We found that, among three state-of-the-art LLMs, the best improved fine-tuned versions have an average improvement of 6.66\% on a GitHub emotion classification dataset, 7.07\% on a GitHub incivility classification dataset, and 3.71\% on a Bugzilla bug report prioritization dataset.},
	booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
	articleno = {207},
	numpages = {13},
	location = {Lisbon, Portugal},
	series = {ICSE '24}
}


@inproceedings{10.1145/3663529.3663849,
	author = {Hassan, Ahmed E. and Lin, Dayi and Rajbahadur, Gopi Krishnan and Gallaba, Keheliya and Cogo, Filipe Roseiro and Chen, Boyuan and Zhang, Haoxiang and Thangarajah, Kishanthan and Oliva, Gustavo and Lin, Jiahuei (Justina) and Abdullah, Wali Mohammad and Jiang, Zhen Ming (Jack)},
	title = {Rethinking Software Engineering in the Era of Foundation Models: A Curated Catalogue of Challenges in the Development of Trustworthy FMware},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3663849},
	doi = {10.1145/3663529.3663849},
	abstract = {Foundation models (FMs), such as Large Language Models (LLMs), have revolutionized software development by enabling new use cases and business models. We refer to software built using FMs as FMware. The unique properties of FMware (e.g., prompts, agents and the need for orchestration), coupled with the intrinsic limitations of FMs (e.g., hallucination) lead to a completely new set of software engineering challenges. Based on our industrial experience, we identified ten key SE4FMware challenges that have caused enterprise FMware development to be unproductive, costly, and risky. For each of those challenges, we state the path for innovation that we envision. We hope that the disclosure of the challenges will not only raise awareness but also promote deeper and further discussions, knowledge sharing, and innovative solutions.},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {294–305},
	numpages = {12},
	keywords = {AIware, FMware, Foundation models, Large Language Models},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@inproceedings{10.1145/3663529.3663858,
	author = {Zhang, Dylan and Zhang, Xuchao and Bansal, Chetan and Las-Casas, Pedro and Fonseca, Rodrigo and Rajmohan, Saravan},
	title = {LM-PACE: Confidence Estimation by Large Language Models for Effective Root Causing of Cloud Incidents},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3663858},
	doi = {10.1145/3663529.3663858},
	abstract = {Major cloud providers have employed advanced AI-based solutions like large language models to aid humans in identifying the root causes of cloud incidents. Even though AI-driven assistants are be- coming more common in the process of analyzing root causes, their usefulness in supporting on-call engineers is limited by their unstable accuracy. This limitation arises from the fundamental challenges of the task, the tendency of language model-based methods to produce hallucinate information, and the difficulty in distinguishing these well-disguised hallucinations. To address this challenge, we propose a novel confidence estimation method to assign reliable confidence scores to root cause recommendations, aiding on-call engineers in deciding whether to trust the model’s predictions. We made re- training-free confidence estimation on out-of-domain tasks possible via retrieval augmentation. To elicit better-calibrated confidence es- timates, we adopt a two-stage prompting procedure and a learnable transformation, which reduces the estimated calibration error (ECE) to 31\% of the direct prompting baseline on a dataset comprising over 100,000 incidents from Microsoft. Additionally, we demonstrate that our method is applicable across various root cause prediction models. Our study takes an important move towards reliably and effectively embedding LLMs into cloud incident management systems},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {388–398},
	numpages = {11},
	keywords = {AIOps, Cloud Incident Trouble Shooting, Confidence Estimation, Large Language Model Agents},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@inproceedings{10.1145/3663529.3663779,
	author = {Zheng, Xi and Mok, Aloysius K. and Piskac, Ruzica and Lee, Yong Jae and Krishnamachari, Bhaskar and Zhu, Dakai and Sokolsky, Oleg and Lee, Insup},
	title = {Testing Learning-Enabled Cyber-Physical Systems with Large-Language Models: A Formal Approach},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3663779},
	doi = {10.1145/3663529.3663779},
	abstract = {The integration of machine learning into cyber-physical systems (CPS) promises enhanced efficiency and autonomous capabilities, revolutionizing fields like autonomous vehicles and telemedicine. This evolution necessitates a shift in the software development life cycle, where data and learning are pivotal. Traditional verification and validation methods are inadequate for these AI-driven systems. This study focuses on the challenges in ensuring safety in learning-enabled CPS. It emphasizes the role of testing as a primary method for verification and validation, critiques current methodologies, and advocates for a more rigorous approach to assure formal safety.},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {467–471},
	numpages = {5},
	keywords = {AI-based Systems, LLM-based Testing, automata-learning, model-based testing},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@inproceedings{10.1145/3663529.3663784,
	author = {Jiang, Yu and Liang, Jie and Ma, Fuchen and Chen, Yuanliang and Zhou, Chijin and Shen, Yuheng and Wu, Zhiyong and Fu, Jingzhou and Wang, Mingzhe and Li, Shanshan and Zhang, Quan},
	title = {When Fuzzing Meets LLMs: Challenges and Opportunities},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3663784},
	doi = {10.1145/3663529.3663784},
	abstract = {Fuzzing, a widely-used technique for bug detection, has seen advancements through Large Language Models (LLMs). Despite their potential, LLMs face specific challenges in fuzzing. In this paper, we identified five major challenges of LLM-assisted fuzzing. To support our findings, we revisited the most recent papers from top-tier conferences, confirming that these challenges are widespread. As a remedy, we propose some actionable recommendations to help improve applying LLM in Fuzzing and conduct preliminary evaluations on DBMS fuzzing. The results demonstrate that our recommendations effectively address the identified challenges.},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {492–496},
	numpages = {5},
	keywords = {Fuzzing, Large Language Model},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@inproceedings{10.1145/3663529.3663786,
	author = {Zhang, Quan and Zeng, Binqi and Zhou, Chijin and Go, Gwihwan and Shi, Heyuan and Jiang, Yu},
	title = {Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered Applications},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3663786},
	doi = {10.1145/3663529.3663786},
	abstract = {Presently, with the assistance of advanced LLM application development frameworks, more and more LLM-powered applications can effortlessly augment the LLMs' knowledge with external content using the retrieval augmented generation (RAG) technique. However, these frameworks' designs do not have sufficient consideration of the risk of external content, thereby allowing attackers to undermine the applications developed with these frameworks. In this paper, we reveal a new threat to LLM-powered applications, termed retrieval poisoning, where attackers can guide the application to yield malicious responses during the RAG process. Specifically, through the analysis of LLM application frameworks, attackers can craft documents visually indistinguishable from benign ones. Despite the documents providing correct information, once they are used as reference sources for RAG, the application is misled into generating incorrect responses. Our preliminary experiments indicate that attackers can mislead LLMs with an 88.33\% success rate, and achieve a 66.67\% success rate in the real-world application, demonstrating the potential impact of retrieval poisoning.},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {502–506},
	numpages = {5},
	keywords = {Large Language Models, Retrieval Poisoning Attack},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@inproceedings{10.1145/3663529.3664457,
	author = {Yu, Chihao},
	title = {Unlocking the Full Potential of AI Chatbots: A Guide to Maximizing Your Digital Companions},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3664457},
	doi = {10.1145/3663529.3664457},
	abstract = {Recent advancements in code-generating AI technologies, such as ChatGPT and Cody, are set to significantly transform the programming landscape. Utilizing a qualitative methodology, this study presents recommendations on how programmers should extract information from Cody.},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {680–682},
	numpages = {3},
	keywords = {AI Chatbots, Thematic Analysis},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@inproceedings{10.1145/3663529.3664458,
	author = {Zhang, Yichi},
	title = {Detecting Code Comment Inconsistencies using LLM and Program Analysis},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3664458},
	doi = {10.1145/3663529.3664458},
	abstract = {Code comments are the most important medium for documenting program logic and design.
 
 
 
Nevertheless, as modern software undergoes frequent updates and modifications, maintaining the accuracy and relevance of comments becomes a labor-intensive endeavor. Drawing inspiration from the remarkable performance of Large Language Model (LLM) in comprehending software programs, this paper introduces a program analysis based and LLM-driven methodology for identifying inconsistencies in code comments. Our approach capitalizes on LLMs' ability to interpret natural language descriptions within code comments, enabling the extraction of design constraints. Subsequently, we employ program analysis techniques to accurately identify the implementation of these constraints. We instantiate this methodology using GPT 4.0, focusing on three prevalent types of constraints. In the experiment on 13 open-source projects, our approach identified 160 inconsistencies, and 23 of them have been confirmed and fixed by the developers.},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {683–685},
	numpages = {3},
	keywords = {bug detection, code comment inconsistency, large language model, program analysis},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@inproceedings{10.1145/3663529.3664462,
	author = {Ling, Lin},
	title = {Evaluating Social Bias in Code Generation Models},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3664462},
	doi = {10.1145/3663529.3664462},
	abstract = {The functional correctness of Code Generation Models (CLMs) has been well-studied, but their social bias has not. This study aims to fill this gap by creating an evaluation set for human-centered tasks and empirically assessing social bias in CLMs. We introduce a novel evaluation framework to assess biases in CLM-generated code, using differential testing to determine if the code exhibits biases towards specific demographic groups in social issues. Our core contributions are (1) a dataset for evaluating social problems and (2) a testing framework to quantify CLM fairness in code generation, promoting ethical AI development.},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {695–697},
	numpages = {3},
	keywords = {AI Ethics, Code Generation Models, Social Bias},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@inproceedings{10.1145/3663529.3663820,
	author = {Cogo, Filipe Roseiro and Rajbahadur, Gopi Krishnan and Lin, Dayi and Hassan, Ahmed E.},
	title = {A Tutorial on Software Engineering for FMware},
	year = {2024},
	isbn = {9798400706585},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3663529.3663820},
	doi = {10.1145/3663529.3663820},
	abstract = {Foundation Models (FMs) like GPT-4 have given rise to FMware, FM-powered applications representing a new generation of software that is developed with new roles, assets, and paradigms. FMware has been widely adopted in both software engineering (SE) research (e.g., test generation) and industrial products (e.g., GitHub copilot), despite the numerous challenges introduced by the stochastic nature of FMs. In our tutorial, we will present the latest research and industrial practices in engineering FMware, along with a hands-on session to acquaint attendees with core tools and techniques to build FMware. Our tutorial's perspective is firmly rooted in SE rather than artificial intelligence (AI), ensuring that participants are spared from delving into mathematical and AI-related intricacies unless they are crucial for introducing SE challenges and opportunities.},
	booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
	pages = {710–712},
	numpages = {3},
	keywords = {FMware, Foundation Model, Software engineering for FMware},
	location = {Porto de Galinhas, Brazil},
	series = {FSE 2024}
}


@article{10354028,
	author = {Karmakar, Anjan and Robbes, Romain},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ INSPECT: Intrinsic and Systematic Probing Evaluation for Code Transformers }},
	year = {2024},
	volume = {50},
	number = {02},
	ISSN = {1939-3520},
	pages = {220-238},
	abstract = { Pre-trained models of source code have recently been successfully applied to a wide variety of Software Engineering tasks; they have also seen some practical adoption in practice, e.g. for code completion. Yet, we still know very little about what these pre-trained models learn about source code. In this article, we use probing—simple diagnostic tasks that do not further train the models—to discover to what extent pre-trained models learn about specific aspects of source code. We use an extensible framework to define 15 probing tasks that exercise surface, syntactic, structural and semantic characteristics of source code. We probe 8 pre-trained source code models, as well as a natural language model (BERT) as our baseline. We find that models that incorporate some structural information (such as GraphCodeBERT) have a better representation of source code characteristics. Surprisingly, we find that for some probing tasks, BERT is competitive with the source code models, indicating that there are ample opportunities to improve source-code specific pre-training on the respective code characteristics. We encourage other researchers to evaluate their models with our probing task suite, so that they may peer into the hidden layers of the models and identify what intrinsic code characteristics are encoded. },
	keywords = {Task analysis;Source coding;Probes;Codes;Training;Natural languages;Data models},
	doi = {10.1109/TSE.2023.3341624},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2023.3341624},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {feb}
}


@article{10431665,
	author = {Yang, Zhou and Xu, Bowen and Zhang, Jie M. and Kang, Hong Jin and Shi, Jieke and He, Junda and Lo, David},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ Stealthy Backdoor Attack for Code Models }},
	year = {2024},
	volume = {50},
	number = {04},
	ISSN = {1939-3520},
	pages = {721-741},
	abstract = { Code models, such as CodeBERT and CodeT5, offer general-purpose representations of code and play a vital role in supporting downstream automated software engineering tasks. Most recently, code models were revealed to be vulnerable to backdoor attacks. A code model that is backdoor-attacked can behave normally on clean examples but will produce pre-defined malicious outputs on examples injected with triggers that activate the backdoors. Existing backdoor attacks on code models use unstealthy and easy-to-detect triggers. This paper aims to investigate the vulnerability of code models with stealthy backdoor attacks. To this end, we propose Afraidoor (Adversarial Feature as Adaptive Backdoor). Afraidoor achieves stealthiness by leveraging adversarial perturbations to inject adaptive triggers into different inputs. We apply Afraidoor to three widely adopted code models (CodeBERT, PLBART, and CodeT5) and two downstream tasks (code summarization and method name prediction). We evaluate three widely used defense methods and find that Afraidoor is more unlikely to be detected by the defense methods than by baseline methods. More specifically, when using spectral signature as defense, around 85% of adaptive triggers in Afraidoor bypass the detection in the defense process. By contrast, only less than 12% of the triggers from previous work bypass the defense. When the defense method is not applied, both Afraidoor and baselines have almost perfect attack success rates. However, once a defense is applied, the attack success rates of baselines decrease dramatically, while the success rate of Afraidoor remains high. Our finding exposes security weaknesses in code models under stealthy backdoor attacks and shows that state-of-the-art defense methods cannot provide sufficient protection. We call for more research efforts in understanding security threats to code models and developing more effective countermeasures. },
	keywords = {Codes;Adaptation models;Data models;Task analysis;Security;Predictive models;Grammar},
	doi = {10.1109/TSE.2024.3361661},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3361661},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {apr}
}


@article{10440574,
	author = {Wang, Junjie and Huang, Yuchao and Chen, Chunyang and Liu, Zhe and Wang, Song and Wang, Qing},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ Software Testing With Large Language Models: Survey, Landscape, and Vision }},
	year = {2024},
	volume = {50},
	number = {04},
	ISSN = {1939-3520},
	pages = {911-936},
	abstract = { Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs. This paper provides a comprehensive review of the utilization of LLMs in software testing. It analyzes 102 relevant studies that have used LLMs for software testing, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative. It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs. It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in software testing. },
	keywords = {Software testing;Task analysis;Computational modeling;Codes;Software systems;Natural language processing;Reviews},
	doi = {10.1109/TSE.2024.3368208},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3368208},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {apr}
}


@article{10477672,
	author = {Nader Palacio, David and Velasco, Alejandro and Cooper, Nathan and Rodriguez, Alvaro and Moran, Kevin and Poshyvanyk, Denys},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ Toward a Theory of Causation for Interpreting Neural Code Models }},
	year = {2024},
	volume = {50},
	number = {05},
	ISSN = {1939-3520},
	pages = {1215-1243},
	abstract = { Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of NCMs appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces do${}_{\textbf{code}}$code , a post hoc interpretability method specific to NCMs that is capable of explaining model predictions. do${}_{\textbf{code}}$code is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of do${}_{\textbf{code}}$code are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact of spurious correlations by grounding explanations of model behavior in properties of programming languages. To demonstrate the practical benefit of do${}_{\textbf{code}}$code , we illustrate the insights that our framework can provide by performing a case study on two popular deep learning architectures and ten NCMs. The results of this case study illustrate that our studied NCMs are sensitive to changes in code syntax. All our NCMs, except for the BERT-like model, statistically learn to predict tokens related to blocks of code (e.g., brackets, parenthesis, semicolon) with less confounding bias as compared to other programming language constructs. These insights demonstrate the potential of do${}_{\textbf{code}}$code as a useful method to detect and facilitate the elimination of confounding bias in NCMs. },
	keywords = {Codes;Predictive models;Correlation;Adaptation models;Measurement;Task analysis;Software engineering},
	doi = {10.1109/TSE.2024.3379943},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3379943},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {may}
}


@article{10508627,
	author = {Xian, Zixiang and Huang, Rubing and Towey, Dave and Fang, Chunrong and Chen, Zhenyu},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ TransformCode: A Contrastive Learning Framework for Code Embedding via Subtree Transformation }},
	year = {2024},
	volume = {50},
	number = {06},
	ISSN = {1939-3520},
	pages = {1600-1619},
	abstract = { Artificial intelligence (AI) has revolutionized software engineering (SE) by enhancing software development efficiency. The advent of pre-trained models (PTMs) leveraging transfer learning has significantly advanced AI for SE. However, existing PTMs that operate on individual code tokens suffer from several limitations: They are costly to train and fine-tune; and they rely heavily on labeled data for fine-tuning on task-specific datasets. In this paper, we present TransformCode, a novel framework that learns code embeddings in a contrastive learning manner. Our framework is encoder-agnostic and language-agnostic, which means that it can leverage any encoder model and handle any programming language. We also propose a novel data-augmentation technique called abstract syntax tree (AST) transformation, which applies syntactic and semantic transformations to the original code snippets, to generate more diverse and robust samples for contrastive learning. Our framework has several advantages over existing methods: (1) It is flexible and adaptable, because it can easily be extended to other downstream tasks that require code representation (such as code-clone detection and classification); (2) it is efficient and scalable, because it does not require a large model or a large amount of training data, and it can support any programming language; (3) it is not limited to unsupervised learning, but can also be applied to some supervised learning tasks by incorporating task-specific labels or objectives; and (4) it can also adjust the number of encoder parameters based on computing resources. We evaluate our framework on several code-related tasks, and demonstrate its effectiveness and superiority over the state-of-the-art methods such as SourcererCC, Code2vec, and InferCode. },
	keywords = {Codes;Task analysis;Self-supervised learning;Syntactics;Semantics;Vectors;Training},
	doi = {10.1109/TSE.2024.3393419},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3393419},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {jun}
}


@article{10508714,
	author = {Lin, Yalan and Wan, Chengcheng and Bai, Shuwen and Gu, Xiaodong},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ VarGAN: Adversarial Learning of Variable Semantic Representations }},
	year = {2024},
	volume = {50},
	number = {06},
	ISSN = {1939-3520},
	pages = {1505-1517},
	abstract = { Variable names are of critical importance in code representation learning. However, due to diverse naming conventions, variables often receive arbitrary names, leading to long-tail, out-of-vocabulary (OOV), and other well-known problems. While the Byte-Pair Encoding (BPE) tokenizer has addressed the surface-level recognition of low-frequency tokens, it has not noticed the inadequate training of low-frequency identifiers by code representation models, resulting in an imbalanced distribution of rare and common identifiers. Consequently, code representation models struggle to effectively capture the semantics of low-frequency variable names. In this paper, we propose VarGAN, a novel method for variable name representations. VarGAN strengthens the training of low-frequency variables through adversarial training. Specifically, we regard the code representation model as a generator responsible for producing vectors from source code. Additionally, we employ a discriminator that detects whether the code input to the generator contains low-frequency variables. This adversarial setup regularizes the distribution of rare variables, making them overlap with their corresponding high-frequency counterparts in the vector space. Experimental results demonstrate that VarGAN empowers CodeBERT to generate code vectors that exhibit more uniform distribution for both low- and high-frequency identifiers. There is an improvement of 8% in similarity and relatedness scores compared to VarCLR in the IdBench benchmark. VarGAN is also validated in downstream tasks, where it exhibits enhanced capabilities in capturing token- and code-level semantics. },
	keywords = {Codes;Vectors;Generators;Training;Semantics;Task analysis;Generative adversarial networks},
	doi = {10.1109/TSE.2024.3391730},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3391730},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {jun}
}


@article{10562221,
	author = {Song, Da and Xie, Xuan and Song, Jiayang and Zhu, Derui and Huang, Yuheng and Juefei-Xu, Felix and Ma, Lei},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ LUNA: A Model-Based Universal Analysis Framework for Large Language Models }},
	year = {2024},
	volume = {50},
	number = {07},
	ISSN = {1939-3520},
	pages = {1921-1948},
	abstract = { Over the past decade, Artificial Intelligence (AI) has had great success recently and is being used in a wide range of academic and industrial fields. More recently, Large Language Models (LLMs) have made rapid advancements that have propelled AI to a new level, enabling and empowering even more diverse applications and industrial domains with intelligence, particularly in areas like software engineering and natural language processing. Nevertheless, a number of emerging trustworthiness concerns and issues exhibited in LLMs, e.g., robustness and hallucination, have already recently received much attention, without properly solving which the widespread adoption of LLMs could be greatly hindered in practice. The distinctive characteristics of LLMs, such as the self-attention mechanism, extremely large neural network scale, and autoregressive generation usage contexts, differ from classic AI software based on Convolutional Neural Networks and Recurrent Neural Networks and present new challenges for quality analysis. Up to the present, it still lacks universal and systematic analysis techniques for LLMs despite the urgent industrial demand across diverse domains. Towards bridging such a gap, in this paper, we initiate an early exploratory study and propose a universal analysis framework for LLMs, named LUNA, which is designed to be general and extensible and enables versatile analysis of LLMs from multiple quality perspectives in a human-interpretable manner. In particular, we first leverage the data from desired trustworthiness perspectives to construct an abstract model as an auxiliary analysis asset and proxy, which is empowered by various abstract model construction methods built-in LUNA. To assess the quality of the abstract model, we collect and define a number of evaluation metrics, aiming at both the abstract model level and the semantics level. Then, the semantics, which is the degree of satisfaction of the LLM w.r.t. the trustworthiness perspective, is bound to and enriches the abstract model with semantics, which enables more detailed analysis applications for diverse purposes, e.g., abnormal behavior detection. To better understand the potential usefulness of our analysis framework LUNA, we conduct a large-scale evaluation, the results of which demonstrate that 1) the abstract model has the potential to distinguish normal and abnormal behavior in LLM, 2) LUNA is effective for the real-world analysis of LLMs in practice, and the hyperparameter settings influence the performance, 3) different evaluation metrics are in different correlations with the analysis performance. In order to encourage further studies in the quality assurance of LLMs, we made all of the code and more detailed experimental results data available on the supplementary website of this paper https://sites.google.com/view/llm-luna. },
	keywords = {Hidden Markov models;Analytical models;Measurement;Semantics;Codes;Task analysis;Transformers},
	doi = {10.1109/TSE.2024.3411928},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3411928},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {jul}
}


@article{10645745,
	author = {Paltenghi, Matteo and Pandita, Rahul and Henley, Austin Z. and Ziegler, Albert},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ Follow-Up Attention: An Empirical Study of Developer and Neural Model Code Exploration }},
	year = {2024},
	volume = {50},
	number = {10},
	ISSN = {1939-3520},
	pages = {2568-2582},
	abstract = { Recent neural models of code, such as OpenAI Codex and AlphaCode, have demonstrated remarkable proficiency at code generation due to the underlying attention mechanism. However, it often remains unclear how the models actually process code, and to what extent their reasoning and the way their attention mechanism scans the code matches the patterns of developers. A poor understanding of the model reasoning process limits the way in which current neural models are leveraged today, so far mostly for their raw prediction. To fill this gap, this work studies how the processed attention signal of three open large language models - CodeGen, InCoder and GPT-J - agrees with how developers look at and explore code when each answers the same sensemaking questions about code. Furthermore, we contribute an open-source eye-tracking dataset comprising 92 manually-labeled sessions from 25 developers engaged in sensemaking tasks. We empirically evaluate five heuristics that do not use the attention and ten attention-based post-processing approaches of the attention signal of CodeGen against our ground truth of developers exploring code, including the novel concept of follow-up attention which exhibits the highest agreement between model and human attention. Our follow-up attention method can predict the next line a developer will look at with 47% accuracy. This outperforms the baseline prediction accuracy of 42.3%, which uses the session history of other developers to recommend the next line. These results demonstrate the potential of leveraging the attention signal of pre-trained models for effective code exploration. },
	keywords = {Codes;Task analysis;Gaze tracking;Transformers;Cognition;Attention mechanisms;Visualization},
	doi = {10.1109/TSE.2024.3445338},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3445338},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {oct}
}


@article{10706805,
	author = {Yin, Xin and Ni, Chao and Wang, Shaohua},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ Multitask-Based Evaluation of Open-Source LLM on Software Vulnerability }},
	year = {2024},
	volume = {50},
	number = {11},
	ISSN = {1939-3520},
	pages = {3071-3087},
	abstract = { This paper proposes a pipeline for quantitatively evaluating interactive Large Language Models (LLMs) using publicly available datasets. We carry out an extensive technical evaluation of LLMs using Big-Vul covering four different common software vulnerability tasks. This evaluation assesses the multi-tasking capabilities of LLMs based on this dataset. We find that the existing state-of-the-art approaches and pre-trained Language Models (LMs) are generally superior to LLMs in software vulnerability detection. However, in software vulnerability assessment and location, certain LLMs (e.g., CodeLlama and WizardCoder) have demonstrated superior performance compared to pre-trained LMs, and providing more contextual information can enhance the vulnerability assessment capabilities of LLMs. Moreover, LLMs exhibit strong vulnerability description capabilities, but their tendency to produce excessive output significantly weakens their performance compared to pre-trained LMs. Overall, though LLMs perform well in some aspects, they still need improvement in understanding the subtle differences in code vulnerabilities and the ability to describe vulnerabilities to fully realize their potential. Our evaluation pipeline provides valuable insights into the capabilities of LLMs in handling software vulnerabilities. },
	keywords = {Software;Training;Biological system modeling;Codes;Software quality;Large language models;Source coding;Software systems;Software engineering;Nickel},
	doi = {10.1109/TSE.2024.3470333},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3470333},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {nov}
}


@article{10735776,
	author = {Yang, Zhou and Zhao, Zhipeng and Wang, Chenyu and Shi, Jieke and Kim, Dongsun and Han, DongGyun and Lo, David},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ Gotcha! This Model Uses My Code! Evaluating Membership Leakage Risks in Code Models }},
	year = {2024},
	volume = {50},
	number = {12},
	ISSN = {1939-3520},
	pages = {3290-3306},
	abstract = { Leveraging large-scale datasets from open-source projects and advances in large language models, recent progress has led to sophisticated code models for key software engineering tasks, such as program repair and code completion. These models are trained on data from various sources, including public open-source projects like GitHub and private, confidential code from companies, raising significant privacy concerns. This paper investigates a crucial but unexplored question: What is the risk of membership information leakage in code models? Membership leakage refers to the vulnerability where an attacker can infer whether a specific data point was part of the training dataset. We present Gotcha, a novel membership inference attack method designed for code models, and evaluate its effectiveness on Java-based datasets. Gotcha simultaneously considers three key factors: model input, model output, and ground truth. Our ablation study confirms that each factor significantly enhances attack performance. Our ablation study confirms that each factor significantly enhances attack performance. Our investigation reveals a troubling finding: membership leakage risk is significantly elevated. While previous methods had accuracy close to random guessing, Gotcha achieves high precision, with a true positive rate of 0.95 and a low false positive rate of 0.10. We also demonstrate that the attacker's knowledge of the victim model (e.g., model architecture and pre-training data) affects attack success. Additionally, modifying decoding strategies can help reduce membership leakage risks. This research highlights the urgent need to better understand the privacy vulnerabilities of code models and develop strong countermeasures against these threats. },
	keywords = {Codes;Data models;Training data;Training;Information leakage;Software development management;Data privacy;Privacy;Decoding;Source coding},
	doi = {10.1109/TSE.2024.3482719},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3482719},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {dec}
}


@article{10745266,
	author = {Yu, Xiao and Liu, Lei and Hu, Xing and Keung, Jacky Wai and Liu, Jin and Xia, Xin},
	journal = { IEEE Transactions on Software Engineering },
	title = {{ Fight Fire With Fire: How Much Can We Trust ChatGPT on Source Code-Related Tasks? }},
	year = {2024},
	volume = {50},
	number = {12},
	ISSN = {1939-3520},
	pages = {3435-3453},
	abstract = { With the increasing utilization of large language models such as ChatGPT during software development, it has become crucial to verify the quality of code content it generates. Recent studies proposed utilizing ChatGPT as both a developer and tester for multi-agent collaborative software development. The multi-agent collaboration empowers ChatGPT to produce test reports for its generated code, enabling it to self-verify the code content and fix bugs based on these reports. However, these studies did not assess the effectiveness of the generated test reports in validating the code. Therefore, we conduct a comprehensive empirical investigation to evaluate ChatGPT's self-verification capability in code generation, code completion, and program repair. We request ChatGPT to (1) generate correct code and then self-verify its correctness; (2) complete code without vulnerabilities and then self-verify for the presence of vulnerabilities; and (3) repair buggy code and then self-verify whether the bugs are resolved. Our findings on two code generation datasets, one code completion dataset, and two program repair datasets reveal the following observations: (1) ChatGPT often erroneously predicts its generated incorrect code as correct, its vulnerable completed code as non-vulnerable, and its failed program repairs as successful during its self-verification. (2) The self-contradictory hallucinations in ChatGPT's behavior arise: (a) ChatGPT initially generates code that it believes to be correct but later predicts it to be incorrect; (b) ChatGPT initially generates code completions that it deems secure but later predicts them to be vulnerable; (c) ChatGPT initially outputs code that it considers successfully repaired but later predicts it to be buggy during its self-verification. (3) The self-verification capability of ChatGPT can be enhanced by asking the guiding question, which queries whether ChatGPT agrees with assertions about incorrectly generated or repaired code and vulnerabilities in completed code. (4) Using test reports generated by ChatGPT can identify more vulnerabilities in completed code, but the explanations for incorrectly generated code and failed repairs are mostly inaccurate in the test reports. Based on these findings, we provide implications for further research or development using ChatGPT. },
	keywords = {Codes;Chatbots;Maintenance engineering;Software development management;Software;Computer bugs;Urban areas;Computer science;Accuracy;Source coding},
	doi = {10.1109/TSE.2024.3492204},
	url = {https://doi.ieeecomputersociety.org/10.1109/TSE.2024.3492204},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {dec}
}
